{"cells":[{"cell_type":"markdown","metadata":{"id":"9YehS8enAmDn"},"source":["<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n","\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"__g36CjU8CUi"},"source":["# **Diffusion models**\n","\n","#### **Morning contents/agenda**\n","\n","1. *Why* use diffusion models?\n","\n","2. *What* are diffusion models?\n","\n","3. *How* do they work? ([DDPM](https://arxiv.org/abs/2006.11239) implementation)\n","\n","#### **Afternoon contents/agenda**\n","\n","1. Exercises\n","\n","2. Stable diffusion\n","\n","<br/>\n","\n","---\n","\n","Author: George Strong\n","\n","gs914@ic.ac.uk\n","\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"l5CluZ9CZtgV"},"source":["# 1. *Why* use diffusion models? "]},{"cell_type":"markdown","metadata":{"id":"_Ilh7RBFnnvm"},"source":["#### **Concept and motivation**"]},{"cell_type":"markdown","metadata":{"id":"HJAt134jaL7L"},"source":["Many generative modelling techniques, such as [variational autoencoders](https://arxiv.org/abs/1312.6114) (VAEs) and [generative adversarial networks](https://arxiv.org/abs/1406.2661) (GANs), attempt to go directly from a random latent-space sample to a generated output in one forward pass of a network. This is a *very* difficult thing to do for an artificial neural network. It is also a *very* difficult and unnatural thing for a biological neural network to do. For instance, when an author writes a novel, or an artist creates a painting, they do so by gradually refining it over many iterations. Rarely will an author or artist produce their final creation with one train of thought! This observation is the core inspiration behind diffusion models, a (relatively) new type of deep generative model. \n","\n","<br/>\n","\n","Many of the important limitations of VAEs and GANs can be considered a consequence of the aforementioned difficulty in generating data samples with a single network evaluation. VAEs optimise a surrogate loss that is only an *approximation* of maximum likelihood training. This has practical negative consequences on the quality of the generated outputs. Meanwhile, the adversarial training scheme used to optimise GANs is [notoriously unstable](https://arxiv.org/abs/1606.03498). It is also often unfaithful to the true data distribution ([mode collapse](https://arxiv.org/abs/1611.02163)). In truth, the generator doesn't (directly) care about learning a representative model of the data distribution, so long is it can fool the discriminator.\n","\n","<br/>\n","\n","Diffusion models have the potential to circumvent these issues. They benefit from a stable training scheme, and can generate very high quality samples that are competitive with (and now often surpass) those produced using GANs. This is not to say diffusion models are a perfect silver bullet solution, and they certainly have important limitations of there own.\n","\n","<br/>\n","\n","*“All models are wrong, but some are useful”* - George E. P. Box.\n","\n","<br/>\n","\n","A single pass through a diffusion model will likely produce samples that are *“wrong”* and typically much worse than that produced by a VAE or GAN. However, diffusion models are designed to be *“useful”* over many hundreds or thousands of iterations (and network evaluations). A negative consequence of this is that using diffusion models for inference is considerably more computationally expensive than a VAE or GAN, which only require a single pass through the network.\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"A5kMlxhPmQ9Q"},"source":["#### **The results speak for themselves**"]},{"cell_type":"markdown","metadata":{"id":"23Gs6zlwna6l"},"source":["**Imagen**\n","\n","Developed by Google Research, Brain Team. See paper [here](https://arxiv.org/abs/2205.11487)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoyVhYIcmYQt"},"outputs":[],"source":["from IPython import display\n","display.IFrame('https://imagen.research.google/', width=1000, height=600)"]},{"cell_type":"markdown","metadata":{"id":"pgjB_77Lue9l"},"source":["**DALL$\\cdot$E 2**\n","\n","Developed by OpenAI. See paper [here](https://cdn.openai.com/papers/dall-e-2.pdf)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_nU7rCdiuisf"},"outputs":[],"source":["display.IFrame('https://openai.com/dall-e-2/', width=1000, height=600)"]},{"cell_type":"markdown","metadata":{"id":"8OQGGPpg3C_a"},"source":["**Video Diffusion Models**\n","\n","Developed by Ho et al. 2022. See paper [here](https://arxiv.org/abs/2204.03458)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5ZA0sOr22Wb"},"outputs":[],"source":["display.IFrame('https://video-diffusion.github.io/', width=1000, height=600)"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"UAUdRrgMDOAP"}},{"cell_type":"markdown","metadata":{"id":"3Dx6KzNryPDI"},"source":["# 2. *What* are diffusion models? "]},{"cell_type":"markdown","source":["#### **Energy-based models**"],"metadata":{"id":"4ceMtBYiWoH2"}},{"cell_type":"markdown","source":["An unconditional energy-based model $E_{\\theta}$ over a single independent variable $\\mathbf{x}$ is a model with parameters $\\theta$, that takes a sample $\\mathbf{x}$ and predicts a scalar value $E_{\\theta}(\\mathbf{x})$ termed the energy. If the sample $\\mathbf{x}$ is \"good\" then $E_{\\theta}(\\mathbf{x})$ will be a small number, and vice versa. $E_{\\theta}$ can be parameterised by any desired regression function (e.g. a deep neural network), and it can be transformed into a probability density function using the Boltzmann distribution:\n","\n","<br>\n","\n","$p_{\\theta}(\\mathbf{x}) = \\text{exp}(-E_{\\theta}(\\mathbf{x}))/Z$\n","\n","<br>\n","\n","where $Z$ represents the typically intractable partition function $Z = \\int_{-\\infty}^{\\infty}\\text{exp}(-E_{\\theta}(\\mathbf{x}))\\ \\text{d}\\mathbf{x}$. From this equation, we can see that $E_{\\theta}$ can be interpreted as an unnormalized negative log-probability $-\\text{log}p(\\mathbf{x})$. \n","\n","<br>\n","\n","If we can train the energy-based model $E_{\\theta}$ to provide a well behaved energy landscape, then we will be able to use its gradient $\\nabla_{x}E_{\\theta}(\\mathbf{x})$ to navigate towards low-energy, \"good\" quality samples. Although EBMs are flexible, they are notoriously difficult to train! [Here](https://arxiv.org/abs/2101.03288) is a helpful article if you are curious about energy-based models and how to train them.\n","\n","<br>"],"metadata":{"id":"NlTRqHDEW1SN"}},{"cell_type":"markdown","metadata":{"id":"cP3O3RrY2HV6"},"source":["#### **Origin**"]},{"cell_type":"markdown","metadata":{"id":"c_cNuE8_2Oci"},"source":["The rise of diffusion models began in 2019, with the [paper](https://arxiv.org/abs/1907.05600) titled \"Generative Modeling by Estimating Gradients of the Data Distribution\" by Yang Song and Stefano Ermon. \n","\n","<br>\n","\n","Rather than predicting the log-probability of the data $\\text{log}p(\\mathbf{x})$ (or $-E_{\\theta}(\\mathbf{x})$), they train a network to predict the gradients of the log-data distribution $\\nabla_{\\mathbf{x}}\\text{log}p(\\mathbf{x})$, also known as the score function. These predicted gradients can then be used for gradient-based sampling (e.g. Langevin dynamics) to steer $\\mathbf{x}$ towards higher-density regions and generate accurate samples from the data distribution.\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"ZhAZsDlq8q0_"},"source":["#### **Langevin dynamics**"]},{"cell_type":"markdown","metadata":{"id":"CkLK9T9Y99qU"},"source":["[Langevin dynamics](https://en.wikipedia.org/wiki/Langevin_dynamics) is an iterative sampling method that iteratively updates the sample $\\mathbf{x}_{t}$ according to some desired gradient term, in our case the gradient of the log-data distribution $\\nabla_{\\mathbf{x}_{t}}\\text{log}p(\\mathbf{x}_{t})$, plus a small amount of Gaussian noise $\\mathbf{z}_{t}$. The Langevin update equation for drawing samples from the data distribution is defined as:\n","\n","$\\mathbf{x}_{t+1} \\leftarrow \\mathbf{x}_{t}+\\alpha \\nabla_{\\mathbf{x_{t}}}\\text{log}\\ p(\\mathbf{x}_{t}) + \\mathbf{z}_{t}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FBreHpfc9yhN"},"source":["<br>\n","\n","<img src=\"https://yang-song.net/assets/img/score/langevin.gif\" alt=\"network\" width=\"300\"/>\n","\n","<br>\n"]},{"cell_type":"markdown","metadata":{"id":"gS8zN9SCuDcG"},"source":["#### **Noise perturbation**"]},{"cell_type":"markdown","metadata":{"id":"dgtLnDHU-uu_"},"source":["If we train a network to estimate $\\nabla_{\\mathbf{x}_{t}}\\text{log}p(\\mathbf{x}_{t})$, start with a random Gaussian sample $\\mathbf{x}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$, and apply Langevin dynamics, we \"should\" be able to generate data, right? \n","\n","<br>\n","\n","In practice, this doesn't work because high-dimensional data distributions are almost always concentrated in proportionally small, high-density regions. Outside of these small high-density regions, the gradients of the data distribution are very noisy and practically undefined (because there is almost no data coverage) in these vast areas of the data-space. \n","\n","<br>\n","\n","Another important issue is that high-dimensional data actually resides on low-dimensional latent manifolds that are embedded in high-dimensional space (see the [manifold hypothesis](https://en.wikipedia.org/wiki/Manifold_hypothesis)). If the data is restricted to a low-dimensional manifold, the gradients of the log-data distribution in the high-dimensional data-space will be undefined.\n","\n","<br/>\n","\n","Yang and Ermon's proposed solution is to add random noise to the data, and train a noise-conditioned network to estimate the gradient of the noise-perturbed log-data distribution. If the magnitude of the noise is large, then it is able to populate low-density regions and circumvent the manifold hypothesis, enabling accurate gradient estimates. The noise can then be gradually reduced during the sampling process to generate accurate data samples."]},{"cell_type":"markdown","metadata":{"id":"D51VV3Z_tVF8"},"source":["<br>\n","\n","<img src=\"https://yang-song.net/assets/img/score/ald.gif\" alt=\"network\" width=\"600\"/>\n","\n","<br>\n","\n","<img src=\"https://yang-song.net/assets/img/score/cifar10_large.gif\" alt=\"network\" width=\"400\"/> &nbsp; &nbsp; <img src=\"https://yang-song.net/assets/img/score/celeba_large.gif\" alt=\"network\" width=\"400\"/>\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"3HSx3fbuzg0u"},"source":["Generative models such as this, which comprise a network that predicts the score function $\\nabla_{\\mathbf{x}_{t}}$log$p(\\mathbf{x}_{t})$, are collectively known as score-based generative models (SBGM). For further information on the specifics of how score-based generative models are trained and used, see Yang Song's excellent [blogpost](https://yang-song.net/blog/2021/score/) and [this](https://arxiv.org/abs/2101.03288) tutorial.\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"FRpHaeBovzOG"},"source":["#### **Diffusion models**"]},{"cell_type":"markdown","metadata":{"id":"j4cREAk9y3EF"},"source":["SBGMs were the catalyst that inspired a multitude of different techniques that are collectively referred to as diffusion models. All of which share the common theme of learning how to gradually reverse the process of corrupting a data sample."]},{"cell_type":"markdown","metadata":{"id":"Fy5X0Dss3dfz"},"source":["<br>\n","\n","<img src=\"https://drive.google.com/uc?id=1PsiFARVqpegxvO7qk6bWO85LpP-8KnPd\" width=\"800\"/>\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"uiWBm3tY4oFG"},"source":["As we have discussed, SBGMs tackles the reverse process by learning to predict the score function conditioned at a particular time (or noise level) $t$. Langevin dynamics is then used to step through the reverse diffusion process. \n","\n","<br>\n","\n","Perhaps the most popular type of diffusion model is the denoising diffusion probabilistic model (DDPM). The DDPM tackles the reverse process by learning to predict the noise contained within an image at a particular time (or noise level) $t$. We can then iteratively predict and subtract this noise from our sample, effectively denoising it, from time $t=T$ to $t=0$. \n","\n","<br>"]},{"cell_type":"markdown","source":["<br>\n","\n","---\n","\n"],"metadata":{"id":"SUHfsBe6DGte"}},{"cell_type":"markdown","metadata":{"id":"557XaCD6fmi7"},"source":["# 3. *How* do they work?"]},{"cell_type":"markdown","metadata":{"id":"_j8N7bWyypQD"},"source":["We will answer this question and learn by doing. Our challenge today will be to implement the denoising diffusion probabilistic model directly from the [paper](https://arxiv.org/abs/2006.11239).\n","\n","Let's dive in and see exactly how these models work!"]},{"cell_type":"markdown","source":["## *i*. Setting up our colab environment"],"metadata":{"id":"kX1hMZRDDgMM"}},{"cell_type":"markdown","metadata":{"id":"6-u4LBdn8CUj"},"source":["#### **Mounting your google drive**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6Y5eHvr1BzL"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"markdown","metadata":{"id":"4EyMAaE4NXue"},"source":["#### **Imports**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIXQFfl-NV2s"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","import seaborn as sns\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import transforms"]},{"cell_type":"markdown","metadata":{"id":"iUF5NF_12ILl"},"source":["#### **Using the GPU**"]},{"cell_type":"markdown","source":["Google Colab enables us to obtain free GPU computing resources.  \n","You can switch the runtime of Google Colab from CPU to GPU based via the ```toolbar```:  \n","\n","Commands:\n","```Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU```"],"metadata":{"id":"aNQbDG_OqG4w"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4G2qi-w2NNy"},"outputs":[],"source":["device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","    !nvidia-smi\n","else:\n","    print(\"No GPU available!\")"]},{"cell_type":"markdown","source":["## *ii*. Load and normalize the MNIST dataset"],"metadata":{"id":"1Q7lYu2eERQw"}},{"cell_type":"markdown","source":["We will be implementing and training our DDPM on the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset today. As mentioned in the DDPM paper (**section 3.3 and section 4**), the data was scaled to the range [-1,1], and so we will implement this as a transformation upon loading. "],"metadata":{"id":"WWY1BdXcEmHp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dCo6mB_W8CUm"},"outputs":[],"source":["# define torch transformations\n","img_dim = 32\n","tf = transforms.Compose([transforms.ToTensor(), # convert to a tensor and normalize to [0,1]\n","                         transforms.Resize((img_dim, img_dim)), # resize images to 32x32 for ease-of-use with the U-Net architecture\n","                         transforms.Lambda(lambda t: (t * 2) - 1)]) # scale to the range [-1,1]\n","\n","# download and define MNIST as a torch dataset, using the above transformations\n","dataset = torchvision.datasets.MNIST(\"./\", download=True, train=True, transform=tf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQii25BT8CUn"},"outputs":[],"source":["# define a torch dataloader with a batch size of 64 \n","dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1cOCRloSXiMC"},"outputs":[],"source":["# grab a batch of data and pull out the first 16 images\n","x0 = next(iter(dataloader))[0][:16]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njrD0MeydNvh"},"outputs":[],"source":["# define a plotting function to visualize the batch as a grid image\n","def plot_batch(x):\n","    \"\"\"plot batch x as grid image\"\"\"\n","    \n","    # re-scale and clamp to the range [0,1]\n","    x = torch.clamp((x+1)/2, 0,1)\n","\n","    plt.figure(figsize=(10,10))\n","    plt.imshow(torchvision.utils.make_grid(x, pad_value=0, padding=2).permute(1, 2, 0))\n","    plt.xticks([]); plt.yticks([])\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmyPMtX9b2o0"},"outputs":[],"source":["# visualize the batch\n","plot_batch(x0)"]},{"cell_type":"markdown","source":["## *iii*. Variance schedules\n"],"metadata":{"id":"QLC7FdSTIiIr"}},{"cell_type":"markdown","source":["The forward diffusion process is defined by the repeated addition of Gaussian noise to a data sample, $\\mathbf{x}_{0}$, according to a pre-defined variance schedule $\\beta_{1},\\dots ,\\beta_{T}$. We will adopt the same linear variance schedule from $\\beta_{1}=10^{-4}$ to $\\beta_{T}=0.02$, as defined in **section 4** of the paper.\n","\n","<br>\n","\n","Here, we also pre-calculate other helpful terms that derive from the variance schedule and are required for DDPM training and sampling. In particular, we compute $\\alpha_{t} = 1-\\beta_{t}$ and $\\bar{\\alpha_{t}}=\\prod_{s=1}^{t} \\alpha_{s}$.\n","\n"],"metadata":{"id":"1hEvQzdJJsAP"}},{"cell_type":"code","source":["# set maximum time value\n","T = 1000\n","\n","# define a Linear variance schedule\n","betas = torch.linspace(0.0001, 0.02, T)"],"metadata":{"id":"x1xe8HadPgSm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Exercise 1**\n","\n","Implement the cosine variance schedule suggested by [this](https://arxiv.org/abs/2102.09672) paper. \n","\n","Why might this achieve better performance?\n","\n","<br>"],"metadata":{"id":"_vAAbLOwPojk"}},{"cell_type":"markdown","source":["---\n","\n","<br>\n","\n","#### **Answer 1**"],"metadata":{"id":"8ATDrAImzMwK"}},{"cell_type":"code","source":["### your code goes here\n","### your code goes here\n","### your code goes here\n","### your code goes here\n","### your code goes here\n","### your code goes here"],"metadata":{"id":"DtA9yzkQR3er"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["your written answer goes here"],"metadata":{"id":"rYyffev8wuWg"}},{"cell_type":"markdown","source":["---\n","\n","<br>"],"metadata":{"id":"yNcdWo5Wzem9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hKXPfwdMErB"},"outputs":[],"source":["# pre-calculate other helpful terms that derive from the variance schedule\n","alphas = 1. - betas\n","alphas_cumprod = torch.cumprod(alphas, axis=0)\n","sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n","sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n","sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n","\n","# define an extraction function to handle the discrepancy between batch vs schedule dimensionality\n","def extract(sched, t, device):\n","    \"\"\"extract sched at time t, and expand to batch dimensionality\"\"\"\n","\n","    return sched[t[:,None,None,None]].to(device)"]},{"cell_type":"markdown","source":["## *iv*. Forward diffusion"],"metadata":{"id":"gj5fPDknMKqK"}},{"cell_type":"markdown","source":["Here, we implement the forward diffusion process (defined in **section 2**), where we take a data sample and progressively corrupt it by adding a sequence of Gaussian noise controlled by the variance schedule.\n","\n","<br>\n","\n","A nice property of the forward diffusion is that we can directly sample $\\mathbf{x}_{t}$ at any chosen timestep $t$, without having to run through each individual step from $t=0$ to $t=t$:\n","\n","<br>\n","\n","$\\mathbf{x}_{t} = \\sqrt{\\bar{\\alpha}_{t}}\\ \\mathbf{x}_{0} + \\sqrt{1-\\bar{\\alpha}_{t}}\\ \\mathbf{e}$ \n","\n","<br>\n","\n","where $\\mathbf{e} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})$. "],"metadata":{"id":"rpb8BT36bV4c"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"izPlO-w7HqHz"},"outputs":[],"source":["# let's define a function that implements the forward diffusion process\n","def forward_diffusion(x0, t, e):\n","    \"\"\"run the forward diffusion process on x up to time t using noise e\"\"\"\n","\n","    return extract(sqrt_alphas_cumprod, t, x0.device)*x0 + extract(sqrt_one_minus_alphas_cumprod, t, x0.device)*e"]},{"cell_type":"markdown","source":["Let's test the forward diffusion process out on our current batch $\\mathbf{x}_{0}$. "],"metadata":{"id":"MZfTJ0lGxkV5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDerw2uJHqKY"},"outputs":[],"source":["# sample 16 values for t that are evenly spaced from t=0 to t=T\n","t = torch.linspace(0, T-1, len(x0), dtype=torch.long)\n","\n","# draw a random sample from a normal distribution with unit variance\n","e = torch.randn_like(x0)\n","\n","# calculate the forward diffusion process for a single image from batch x0\n","x_t = forward_diffusion(x0[0], t, e)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVKuRpRgHqND"},"outputs":[],"source":["# display t values and visualize the forward diffusion process\n","print(t)\n","plot_batch(x_t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZH16MNAwxC2"},"outputs":[],"source":["# display histograms of the pixel values throughout the forward diffusion\n","plt.figure(figsize=(15,4))\n","for i in range(len(t)):\n","    plt.subplot(2,len(t)/2,1+i)\n","    plt.hist(x_t[i].flatten(), bins=np.arange(x_t[-1].min(), x_t[-1].max() + 0.5, 0.5), edgecolor='black', linewidth=1.2)\n","    plt.xlim(x_t[-1].min(), x_t[-1].max())\n","    plt.xticks([]); plt.yticks([])\n","    plt.title(f'$t={t[i]}$')\n","    sns.despine()"]},{"cell_type":"markdown","source":["## *v*. Training the model"],"metadata":{"id":"ZzNLdAGu2RMA"}},{"cell_type":"markdown","source":["In the training phase, shown in **algorithm 1**, we draw samples for $t$ uniformly from $1,\\dots,T$ such that each image in the batch $\\mathbf{x}_{0}$ has a corresponding $t$. We then apply the forward process to obtain $\\mathbf{x}_{t}$. "],"metadata":{"id":"wIb279b7DL0o"}},{"cell_type":"markdown","source":["<br>\n","\n","<img src=\"https://drive.google.com/uc?id=1-0JReXBc5m38s90Eea6s1KfzNesLX5NG\" width=\"800\"/>\n","\n","<br>"],"metadata":{"id":"g6VJwbda0VSj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2EFN0V1jMPL"},"outputs":[],"source":["# line 3 of algorithm 1\n","t = torch.randint(0, T, (x0.shape[0],), dtype=torch.long).to(x0.device)"]},{"cell_type":"code","source":["# line 4 of algorithm 1\n","e = torch.randn_like(x0)"],"metadata":{"id":"7Qn6Lglu2G94"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LSFl2zOjOev"},"outputs":[],"source":["# part of line 5 of algorithm 1\n","x_t = forward_diffusion(x0, t, e)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CImNnafIjQeV"},"outputs":[],"source":["# display t values and visualize the forward diffusion process\n","print(t)\n","plot_batch(x_t)"]},{"cell_type":"markdown","source":["At the core of the DDPM method is a type of denoising autoencoder, $\\mathbf{e}_{\\theta}(\\mathbf{x}_{t}, t)$, that takes as inputs a batch of noise corrupted or forward diffused samples $\\mathbf{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\ \\mathbf{x}_{0} + \\sqrt{1-\\bar{\\alpha}_{t}}$, and their corresponding $t$ values, and predicts the noise $\\mathbf{e}$.\n","\n","<br>\n","\n","If such a denoising autoencoder can be trained, then it can be used to reverse the diffusion process by iteratively predicting and subtracting the noise from a random starting sample $\\mathbf{x}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$, effectively denoising it from time $t=T$ to $t=0$. More on this sampling approach (algorithm 2) later... \n","\n","<br>\n","\n","For further reading and insight about the interpretation of diffusion models as autoencoders, as well as the connections between DDPMs and SBGMs, you can check out the article below."],"metadata":{"id":"WTbdNoPuDV8K"}},{"cell_type":"code","source":["display.IFrame('https://benanne.github.io/2022/01/31/diffusion.html', width=1000, height=300)"],"metadata":{"id":"OhPULRUgzU4P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Exercise 2**\n","\n","Compare the SBGM objective given by *equation 5* from the SBGM paper [here](https://arxiv.org/abs/1907.05600), to the DDPM objective given by *equation 14* from the DDPM paper [here](https://arxiv.org/abs/2006.11239).\n","\n","<br>"],"metadata":{"id":"QVsa_n_ZUpWl"}},{"cell_type":"markdown","source":["---\n","\n","<br>\n","\n","#### **Answer 2**"],"metadata":{"id":"_aA3kKFezrnr"}},{"cell_type":"markdown","source":["your written answer goes here\n"],"metadata":{"id":"QFD7ZI3OV99w"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"s37H0hWpzvds"}},{"cell_type":"markdown","source":["#### **Model architecture**"],"metadata":{"id":"KFV2XUoeGmdN"}},{"cell_type":"markdown","source":["A modified [U-Net](https://arxiv.org/abs/1505.04597) architecture is used for the denoising autoencoder $\\mathbf{e}_{\\theta}(\\mathbf{x}_{t}, t)$, as mentioned in **section 4**. Crucially, the U-Net must be conditioned on time $t$, so that it is able to predict the noise $\\mathbf{e}$ from a forward diffused image $\\mathbf{x}_{t}$ at any time from $t=1$ to $t=T$. \n","\n","<br>\n","\n","As discussed in the **section 4**, this is facilitated by using sinusoidal position embeddings to represent $t$, identical to those used within the [Transformer](https://arxiv.org/abs/1706.03762) architecture. The sinusoidal position embedding takes $t$ as an input, and produces an embedding or vector that represents that particular value of $t$ (or noise level)."],"metadata":{"id":"TWgA62Oqp7aG"}},{"cell_type":"code","source":["# define the sinusoidal position embeddings \n","class SinusoidalPositionEmbeddings(nn.Module):\n","    \"\"\"sinusoidal position embedding, https://arxiv.org/abs/1706.03762\"\"\"\n","\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, time):\n","        device = time.device\n","        half_dim = self.dim // 2\n","        embeddings = np.log(10000) / (half_dim - 1)\n","        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n","        embeddings = time[:, None] * embeddings[None, :]\n","        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n","        return embeddings"],"metadata":{"id":"DtpGSbPMnqfg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This embedding is often passed through a small multi-layered perceptron (or fully connected network), and is then added to each convolutional block throughout the U-Net."],"metadata":{"id":"tlUIxF0DoJYe"}},{"cell_type":"markdown","source":["# **Exercise 3**\n","\n","A) Under **appendix B** of the DDPM paper, it states that they use a modified U-Net with [group normalisation](https://arxiv.org/abs/1803.08494). Replace batch normalisation with group normalisation. \n","\n","Why do you think group normalisation might be more suitable than batch normalisation for the DDPM model? \n","\n","<br>\n","\n","B) Under **appendix B** of the DDPM paper, it states that they use [residual](https://arxiv.org/abs/1512.03385) blocks in the downsampling and upsampling paths of the U-Net. Incorporate residual connections into the encoder and decoder blocks.\n","\n","<br>\n","\n","You may also notice that the DDPM model uses the [attention](https://arxiv.org/abs/1706.03762) mechanism used by Transformers. We will not be implementing attention today.\n","\n","<br>"],"metadata":{"id":"4WsozOmjwoJm"}},{"cell_type":"code","source":["# define a convolutional block with time-embedding\n","class ConvBlock(nn.Module):\n","    \"\"\"time-conditioned convolutional block\"\"\"\n","\n","    def __init__(self, in_c, out_c, embed_dim):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n","        self.dense = nn.Linear(embed_dim, out_c) # reshapes the time embedding length to the number of channels\n","        self.bn = nn.BatchNorm2d(out_c)\n","        self.act = nn.SiLU()\n","        \n","    def forward(self, x, t_embed):\n","        x = self.conv(x)\n","        x += self.dense(t_embed)[...,None,None] # add the output of the time embedding dense layer\n","        x = self.bn(x)\n","        x = self.act(x)\n","        return x\n","    \n","# define an encoder block of the U-Net with time-embedding\n","class EncBlock(nn.Module):\n","    \"\"\"time-conditioned U-Net encoder block\"\"\"\n","\n","    def __init__(self, in_c, out_c, embed_dim):\n","        super().__init__()\n","        self.conv_block1 = ConvBlock(in_c, out_c, embed_dim)\n","        self.conv_block2 = ConvBlock(out_c, out_c, embed_dim)\n","        self.pool = nn.MaxPool2d((2, 2))\n","\n","    def forward(self, x, t_embed):\n","        h = self.conv_block1(x, t_embed)\n","        h = self.conv_block2(h, t_embed)\n","        p = self.pool(h)\n","        return h, p\n","\n","# define an decoder block of the U-Net with time-embedding\n","class DecBlock(nn.Module):\n","    \"\"\"time-conditioned U-Net decoder block\"\"\"\n","\n","    def __init__(self, in_c, out_c, embed_dim):\n","        super().__init__()\n","        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n","        self.conv_block1 = ConvBlock(out_c+out_c, out_c, embed_dim)\n","        self.conv_block2 = ConvBlock(out_c, out_c, embed_dim)\n","    \n","    def forward(self, x, s, t_embed):\n","        h = self.up(x)\n","        h = torch.cat([h, s], axis=1) # concatenate x with U-Net skip connection from encoder\n","        h = self.conv_block1(h, t_embed)\n","        h = self.conv_block2(h, t_embed)\n","        return h"],"metadata":{"id":"8Iafd2_9olKr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Putting it all together, we can now define our time-conditioned U-Net!"],"metadata":{"id":"Dpd16e26oonP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWgTvd438CUo"},"outputs":[],"source":["class Unet(nn.Module):\n","    \"\"\"DDPM U-Net, https://arxiv.org/abs/2006.11239 and https://arxiv.org/abs/1505.04597\"\"\"\n","\n","    def __init__(self, n_channels, embed_dim):\n","        super().__init__()\n","        \n","        # time positional embedding MLP\n","        self.embed = nn.Sequential(SinusoidalPositionEmbeddings(embed_dim),\n","                                   nn.Linear(embed_dim, embed_dim), \n","                                   nn.GELU(),\n","                                   nn.Linear(embed_dim, embed_dim)) \n","\n","        # encoder\n","        self.e1 = EncBlock(n_channels, 64, embed_dim)\n","        self.e2 = EncBlock(64, 128, embed_dim)\n","        self.e3 = EncBlock(128, 256, embed_dim)\n","        self.e4 = EncBlock(256, 512, embed_dim)\n","\n","        # bottleneck\n","        self.b1 = ConvBlock(512, 1024, embed_dim)\n","        self.b2 = ConvBlock(1024, 1024, embed_dim)\n","\n","        # decoder\n","        self.d1 = DecBlock(1024, 512, embed_dim)\n","        self.d2 = DecBlock(512, 256, embed_dim)\n","        self.d3 = DecBlock(256, 128, embed_dim)\n","        self.d4 = DecBlock(128, 64, embed_dim)\n","\n","        # output layer\n","        self.output = nn.Conv2d(64, n_channels, kernel_size=1, padding=0)\n","\n","    def forward(self, x, t):\n","        \n","        t_embed = self.embed(t)\n","        \n","        # encoder\n","        s1, x = self.e1(x, t_embed)\n","        s2, x = self.e2(x, t_embed)\n","        s3, x = self.e3(x, t_embed)\n","        s4, x = self.e4(x, t_embed)\n","\n","        # bottleneck\n","        x = self.b1(x, t_embed)\n","        x = self.b2(x, t_embed)\n","\n","        # decoder\n","        x = self.d1(x, s4, t_embed)\n","        x = self.d2(x, s3, t_embed)\n","        x = self.d3(x, s2, t_embed)\n","        x = self.d4(x, s1, t_embed)\n","\n","        # output\n","        output = self.output(x)\n","\n","        return output"]},{"cell_type":"markdown","source":["#### **Algorithm 1**"],"metadata":{"id":"F6MfQ5AUHC3c"}},{"cell_type":"markdown","source":["The DDPM model, or denoising autoencoder, $\\mathbf{e}_{\\theta}(\\mathbf{x}_{t}, t)$, can be trained using **algorithm 1**."],"metadata":{"id":"bUk4M-kZpy36"}},{"cell_type":"markdown","source":["<br>\n","\n","<img src=\"https://drive.google.com/uc?id=1-0JReXBc5m38s90Eea6s1KfzNesLX5NG\" width=\"800\"/>\n","\n","<br>"],"metadata":{"id":"fcReAiVHpwqP"}},{"cell_type":"markdown","source":["In words, the training process can be stated as:\n","\n","1. Randomly select a sample from the training data $\\mathbf{x}_{0}$ (line 2)\n","\n","2. Randomly sample $t$ from a uniform distribution over the range [1,T] (line 3)\n","\n","3. Randomly sample $\\mathbf{e}$ from a zero-mean normal distribution with a variance of 1 (line 4)\n","\n","4. Apply the forward diffusion process on $\\mathbf{x}_{0}$ using $\\mathbf{e}$ and $t$ (line 5)\n","\n","5. Predict the noise by evaluating $\\mathbf{e}_{\\theta}(\\mathbf{x}_{t}, t)$ (line 5)\n","\n","6. Calculate the $L_{2}$ (or MSE) loss between the true noise $\\mathbf{e}$ and the predicted noise $\\mathbf{e}_{\\theta}(\\mathbf{x}_{t}, t)$ (line 5)\n","\n","7. Take a gradient descent (or Adam) step with respect to the model parameters $\\theta$ (line 5)\n","\n","8. Repeat steps 1 to 7 until converged (lines 1 and 6)\n","\n","These steps are performed batch-wise in practice."],"metadata":{"id":"wddZfzLduYiV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3SQMTwt_iBxF"},"outputs":[],"source":["# define the model and send to the GPU\n","model = Unet(1, img_dim*4).to(device)\n","\n","# specify the Adam optimizer with a learning rate of 2e-4 (see appendix B)\n","opt = torch.optim.Adam(model.parameters(), lr=2e-4)\n","\n","# set the criterion to be the mse loss \n","criterion = F.mse_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CV1VFKMrHqSM"},"outputs":[],"source":["# ensure model is in training mode\n","model.train()\n","\n","# set number of epochs and loop over them (line 1)\n","n_epoch = 5\n","for epoch in range(n_epoch):\n","    \n","    # loop over each batch using tqdm to report progress and loss stats (line 1 and 2)\n","    pbar = tqdm(dataloader)\n","    for x0, _ in pbar: \n","\n","        # zero the gradients\n","        opt.zero_grad()\n","\n","        # send the batch to the GPU\n","        x0 = x0.to(device)\n","\n","        # sample t from a uniform distribution (line 3)\n","        t = torch.randint(0, T, (x0.shape[0],), dtype=torch.long).to(device)\n","\n","        # sample e from a normal distribution (line 4)\n","        e = torch.randn_like(x0)\n","\n","        # run the forward diffusion process to add noise to x0 (line 5)\n","        x_t = forward_diffusion(x0, t, e)\n","\n","        # calculate the loss between the predicted noise and the true noise (line 5)\n","        loss = criterion(model(x_t, t), e)\n","        \n","        # backpropagation to obtain gradients w.r.t model parameters (line 5)\n","        loss.backward()\n","\n","        # report current loss using tqdm\n","        pbar.set_description(f\"loss: {loss.item():.4f}\")\n","\n","        # take an optimisation step\n","        opt.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0OmGnJkBf96"},"outputs":[],"source":["# save the model\n","torch.save(model.state_dict(), './gdrive/MyDrive/ddpm_opt.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pRLoGl5Sy-i"},"outputs":[],"source":["# load the model\n","model.load_state_dict(torch.load('./gdrive/MyDrive/ddpm_opt.pt'))"]},{"cell_type":"markdown","source":["## *v*. Reverse diffusion (sampling)"],"metadata":{"id":"HlS4qNSl3RLr"}},{"cell_type":"markdown","source":["Here we implement the reverse diffusion process, where we use our denoising autoencoder $\\mathbf{e}_{\\theta}(\\mathbf{x}_{t}, t)$, to progressively undo (or denoise) a random sample from a normal distribution $\\mathbf{x}_{T}$. This is achieved through **algorithm 2**. "],"metadata":{"id":"hhmyhvHi56ha"}},{"cell_type":"markdown","source":["<br>\n","\n","<img src=\"https://drive.google.com/uc?id=1-0JReXBc5m38s90Eea6s1KfzNesLX5NG\" width=\"800\"/>\n","\n","<br>"],"metadata":{"id":"-eUsL7GJ3aeF"}},{"cell_type":"markdown","source":["In words, the sampling process can be stated as:\n","\n","1. Randomly sample $\\mathbf{x}_{T}$ from a zero-mean normal distribution with a variance of 1 (line 1)\n","\n","2. If $t>1$, randomly sample $\\mathbf{z}$ from a zero-mean normal distribution with a variance of 1, otherwise set $\\mathbf{z}=\\mathbf{0}$ (line 3)\n","\n","3. Calculate the updated sample $\\mathbf{x}_{t-1}$ by subtracting the predicted noise $\\mathbf{e}_{\\theta}(\\mathbf{x}_{t}, t)$ from $\\mathbf{x}_{t}$ (line 4) \n","\n","4. Apply $\\mathbf{z}$, scaled by $\\sigma_{t}$, to the sample (line 4)\n","\n","5. Repeat steps 2 to 4 for $t=T,\\dots,1$ (lines 2 and 5)\n","\n","Scaling factors derived from the variance schedule are also used in line 4. These naturally fall out of the maths (the second term of the variational bound on the negative log likelihood), but they don't add any meaningful intuition. \n","\n","<br>\n","\n","We will also set $\\sigma_{t} = \\sqrt{\\beta_{t}}$, as discussed in **section 3.2**. This means that, as $t$ approaches $0$, and **algorithm 2** converges towards the final generative sample, the variance $\\beta_{t}$ will approach its minimum, and so the noise that is added in line 4 will be small. "],"metadata":{"id":"yQ3E6NiDBg54"}},{"cell_type":"markdown","source":["# **Exercise 4**\n","\n","Compare line 4 of **algorithm 2** to the Langevin dynamics update equation. \n","\n","<br>"],"metadata":{"id":"SAv84sr9ChqZ"}},{"cell_type":"markdown","source":["---\n","\n","<br>\n","\n","#### **Answer 4**"],"metadata":{"id":"IgzPkOYjDG50"}},{"cell_type":"markdown","source":["your written answer goes here"],"metadata":{"id":"cmQQylRIDMMn"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"5OhaJK-PEqEf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IH2azKXpM3M5"},"outputs":[],"source":["# ensure model is in evaluation mode\n","model.eval()\n","\n","# randomly sample an xT batch from a noral distribution (line 1)\n","x = torch.randn_like(next(iter(dataloader))[0])\n","\n","# send to the GPU and only consider first 16 of the batch to ease the computational burden\n","x = x.to(device)[:16]\n","\n","# container to store sample throughout the reverse diffusion\n","reverse_diffusion = torch.empty((T, x.shape[0], x.shape[1], x.shape[2], x.shape[3])).to(x.device)\n","\n","# context management to ensure gradients are not tracked for any torch tensors or parameters \n","with torch.no_grad():\n","    \n","    # loop over time from T to 0 (lines 2 and 5)\n","    for t in reversed(range(0,T)):\n","        \n","        # sample z from a normal distribution if condition met (line 3)\n","        if t > 0:\n","            z = torch.randn_like(x)\n","        else:\n","            z = 0\n","\n","        # visualize samples x every 100 iterations\n","        if t % 50 == 0:\n","            print(f'samples at t={t}')\n","            plot_batch(x.cpu())\n","        \n","        # convert t to a tensor, send to the GPU, and expand its first dimension to be equal to batch size\n","        t = torch.tensor(t).to(device).expand(x.shape[0])\n","\n","        # denoise x (line 4)\n","        x = extract(sqrt_recip_alphas, t, x.device) * (x - ((1.-extract(alphas, t, x.device))/(extract(sqrt_one_minus_alphas_cumprod, t, x.device)))*model(x, t))\n","\n","        # add z (line 4)\n","        x += torch.sqrt(extract(betas, t, x.device))*z\n","\n","        # clipping to ensure the sample values remain in the range -1 to 1\n","        x = torch.clamp(x, -1, 1)\n","\n","        # store the current samples x\n","        reverse_diffusion[t] = x"]},{"cell_type":"code","source":["# visualize the diffusion process for a single sample\n","plot_batch(reverse_diffusion[::66,0].flip(0).cpu())"],"metadata":{"id":"3gz87vn3LOnC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Congratulations, you have implemented and trained a DDPM diffusion model! "],"metadata":{"id":"lRa84NB-N0ND"}},{"cell_type":"markdown","source":["# 4. Final thoughts"],"metadata":{"id":"NBDzZbRfCXqQ"}},{"cell_type":"markdown","source":["Research on diffusion modelling is moving fast. A few months ago, [this](https://arxiv.org/abs/2208.09392) paper demonstrated that *any* image transformation that \"degrades\" the  quality of a sample can be used to build a diffusion model. This includes transformations that **do not contain any randomness whatsoever**. \n","\n","<br>\n","\n","This finding represents a significant challenge to our current theoretical understanding of diffusion models, e.g. the theoretical justifications behind DDPMs and SBGMs! According to the semi-official PyTorch DDPM [implementation](https://github.com/lucidrains/denoising-diffusion-pytorch), it \"turns out none of the technicalities really matters at all\" 💀....\n","\n","<br>"],"metadata":{"id":"xiUm2f4WCZ05"}},{"cell_type":"markdown","source":["# 5. Stable diffusion demo"],"metadata":{"id":"VI1ot19YExW-"}},{"cell_type":"markdown","source":["[Stable diffusion](https://github.com/CompVis/stable-diffusion) is an open-source pre-trained text-to-image model based on the latent diffusion model presented in [this](https://arxiv.org/abs/2112.10752) paper."],"metadata":{"id":"MhA1FXvwC6_H"}},{"cell_type":"markdown","source":["---\n","\n","<br>\n","<center>\n","⚠ \n","\n","<br>\n","\n","Do not burn through all of your Google colab compute units! \n","\n","⚠ \n","\n","<br>\n","\n","---"],"metadata":{"id":"SkuRhfYmE2fc"}},{"cell_type":"markdown","source":["#### **Configuration**"],"metadata":{"id":"YTCxARn2DuJG"}},{"cell_type":"code","source":["# install necessary libraries\n","!pip install --quiet --upgrade diffusers transformers scipy mediapy accelerate"],"metadata":{"id":"mc_Z-ef3Zb9o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 1.** You will need a hugging face account to be able to access and download the stable diffusion model weights.\n","\n","<br>\n","\n","**Step 2.** Once you have an account, you will need to generate a token to login. Execute the following cell and enter your token.\n","\n","<br>"],"metadata":{"id":"oHTjlPgtGT1K"}},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"id":"L2v8aWJJIwpu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 3.** Visit the [card](https://huggingface.co/CompVis/stable-diffusion-v1-4) for the stable diffusion v1-4 model we will be using today, read license and tick the check box if you agree. \n","\n","<br>"],"metadata":{"id":"P26nZFnRHKjn"}},{"cell_type":"markdown","source":["#### **Imports**"],"metadata":{"id":"GYRsTJnKGzz6"}},{"cell_type":"code","source":["from diffusers import PNDMScheduler\n","from diffusers import StableDiffusionPipeline\n","import mediapy as media"],"metadata":{"id":"DQyHWXlJGzNG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Variance schedule**\n","\n","There are multiple proposed variance schedules that can be used with pre-trained diffusion models such as Stable diffusion. We will use the [PNDM](https://arxiv.org/abs/2202.09778) scheduler here, but there are many other suitable alternatives. "],"metadata":{"id":"-2eliWMIEQ_F"}},{"cell_type":"code","source":["# define the PNDM scheduler\n","scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", skip_prk_steps=True)"],"metadata":{"id":"0KQKMazRbhIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load the pre-trained stable diffusion model pipeline\n","pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", scheduler=scheduler, \n","                                               torch_dtype=torch.float16, revision=\"fp16\", use_auth_token=True)\n","\n","# send pipeline to the GPU\n","pipe.to(\"cuda\") "],"metadata":{"id":"Fu-FxkgdZ_u2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define a text prompt\n","prompt = \"\"\n","\n","# generate multiple images from the same text prompt\n","num_images = 4\n","prompts = [prompt] * num_images\n","\n","# run the reverse diffusion\n","images = pipe(prompts, guidance_scale=8, num_inference_steps=50).images"],"metadata":{"id":"YDF4jEBvaAXI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["media.show_images(images, height=400)"],"metadata":{"id":"L_tv6Zp-bSJi"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["__g36CjU8CUi","l5CluZ9CZtgV","_Ilh7RBFnnvm","A5kMlxhPmQ9Q","3Dx6KzNryPDI","4ceMtBYiWoH2","cP3O3RrY2HV6","ZhAZsDlq8q0_","gS8zN9SCuDcG","FRpHaeBovzOG","557XaCD6fmi7","kX1hMZRDDgMM","6-u4LBdn8CUj","4EyMAaE4NXue","iUF5NF_12ILl","1Q7lYu2eERQw","QLC7FdSTIiIr","_vAAbLOwPojk","8ATDrAImzMwK","gj5fPDknMKqK","ZzNLdAGu2RMA","QVsa_n_ZUpWl","_aA3kKFezrnr","KFV2XUoeGmdN","4WsozOmjwoJm","F6MfQ5AUHC3c","HlS4qNSl3RLr","SAv84sr9ChqZ","IgzPkOYjDG50","NBDzZbRfCXqQ","VI1ot19YExW-","YTCxARn2DuJG","GYRsTJnKGzz6","-2eliWMIEQ_F"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}