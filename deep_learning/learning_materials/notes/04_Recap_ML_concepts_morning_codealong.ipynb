{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNu3iIodc/VZpqynyTMaG/I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n","\n","\n","---\n"],"metadata":{"id":"9YehS8enAmDn"}},{"cell_type":"markdown","source":["# **Recap of ML/DL basic concepts**\n","\n","\n","#### **Morning contents/agenda**\n","\n","1. Supervised VS unsupervised learning\n","\n","2. Parameters and hyperparameters of a network\n","\n","  2.1 Activation functions \n","\n","  2.2 Losses\n","\n","3. **Training is an optimisation problem**: gradient descent and backpropagation\n","\n","4. Batch, mini-batch, and stochastic gradient descent\n","\n","5. Bias and variance, and regularisers\n","\n","#### **Learning outcomes**\n","\n","1. Understand the difference between parameters of hyperparameters of a network\n","\n","2. Understand how networks are trained using gradient descent and backpropagation\n","\n","3. Understand how batch size works and the effect of regularisers in the training process\n","\n","\n","<br>\n","\n","#### **Afternoon contents/agenda**\n","\n","1. Half-moon classifier \n","\n","2. L2 regularisation on MNIST\n","\n","#### **Learning outcomes**\n","\n","1. Be able to build a simple classifier for simple datasets\n","\n","2. Understand the effect of explicit regularisation during training\n","\n","<br/>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"ZKS9BL1zphFL"}},{"cell_type":"code","source":["!pip install pycm livelossplot\n","%pylab inline\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from livelossplot import PlotLosses\n","from pycm import *\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","from torchsummary import summary"],"metadata":{"id":"_kYki018eTFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"],"metadata":{"id":"LI8sNA9feT3H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Today we will recap some of the concepts you have already seen in the ML module"],"metadata":{"id":"Z-BLv6L2_LaF"}},{"cell_type":"markdown","source":["## 1. Supervised VS unsupervised learning\n","\n","<img src=\"https://drive.google.com/uc?id=1Lp6offWwF8IGv6I-WulLQ5xPJDkiucW7\" width=\"1000\"/>\n","\n","Definitions from the book Deep Learning (Goodfellow et al, 2016):\n","\n","<img src=\"https://drive.google.com/uc?id=1av7AhfBq_-0QdWPlZToT19Ku1aKMY9wt\" width=\"1000\"/>\n","\n","There are other types of learning (reinforced, self-supervised, and semi-supervised, we will cover some of them on the last week of the module).\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","<br/>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"2RAfufbgYAjF"}},{"cell_type":"markdown","source":["## 2. Parameters and hyperparameters of a network\n","\n","We use deep learning to perform tasks that usually rely on large amounts of data. Such tasks, or problems, are then formulated as optimisation problems involving neural networks.\n","\n","Let's say that we have some data and a task that needs to be done. We want to use a solution based on training a network, but a few questions arise:\n","\n","1. What does training a network mean and how do we do it?\n","2. What is the network 'design'?\n","3. How do we choose the best 'design' for our problem?  \n","\n","Let's look at each of those one by one:\n","\n","<br>\n","\n","##### **1. What does training a network mean and how do we do it?**\n","\n","Training a network means to find the optimal parameters of the network according to some measure that we define, usually using a **loss** function. The network parameters are the $\\theta$ parameters we saw on the first day:\n","\n","<img src=\"https://drive.google.com/uc?id=1u8u915wCBPKuKURiQJWHle_d0ci_EVt0\" width=\"800\"/>\n","\n","Network parameters are also referred to as weights and biases ($w$ and $b$ respectively), where the **weights** are the parameters that operate on the data inputs (our outputs of the previous layer), and the **biases** are an extra parameter we add to provide the network with extra flexibility:\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1__yM95UJrKTtxtJQyR1HAtFFcwVmovOg\" width=\"300\"/></center>\n","\n","<br>\n","\n","To train a network is to find the parameters, which are (unsurprisingly) called **trainable parameters**, that better accomplish the task at hand. We will discuss the mechanism to measure this 'accomplishment' when we talk about loss functions further down.\n","\n","\n","##### **2. What is the network 'design'?**\n","\n","The design/shape/form of a network is known as its architecture. The architecture of the network is mostly controled by the network **hyperparameters**.\n","\n","'Definition' from the book [Deep Learning](https://www.deeplearningbook.org/):\n","\n","*Most machine learning algorithms have hyperparameters, settings that we can use to control the algorithm’s behavior. The values of hyperparameters are not adapted by the learning algorithm itself (though we can design a nested learning procedure in which one learning algorithm learns the best hyperparameters for another learning algorithm).*\n","\n","Examples of hyperparameters are:\n","\n","- learning rate\n","- batch size\n","- loss type\n","- number of layers\n","- types of layers\n","- and a very long etc.\n","\n","In essence, ***network settings that are not trainable***.\n","\n","<br>\n","\n","##### **3. How do we choose the best *hyperparameters* for our problem?**\n","\n","If they are not trainable, how do we find their optimal values then?\n","\n","There are several methods to search for the best model hyperparameters. They all rely on exploring the effect these hyperparameters have on the training performance. The two most common strategies are grid and random search ([image source](https://medium.com/@cjl2fv/an-intro-to-hyper-parameter-optimization-using-grid-search-and-random-search-d73b9834ca0a)):\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://miro.medium.com/max/1400/0*yDmmJmvRowl0cSN8.webp\" width=\"400\"/></p><p align = \"center\">\n","\n","<br>\n","\n","A good video by Andrew Ng on the topic [here](https://www.youtube.com/watch?v=AXDByU3D1hA)\n","\n","<br>\n","\n","In fact, this need to optimise hyperparameters is the reason why we split our data in three sets: training, validation, and testing. The table below summarises how each of these three datasets are used during a basic deep learning workflow:\n","\n","<br>\n","\n","| Dataset      | Used for training?| Used to find hyperparameters? | Good to assess Generalisation* |\n","| -----------    | :---: | :----: | :---: |\n","| Training set   | Yes      |  Partly | No  |\n","| Validation set | No       |  Yes | Partly  |\n","| Testing set    | No       |  No  | Yes |\n","\n","<br>\n","\n","\\*Generalisation error (from Goodfellow et al, 2017):\n","\n","*The trained model must perform well on new, previously unseen data, not just those on which the model was trained. The ability to perform well on previously unseen data – or Test Data - is called Generalisation.*\n","\n","<br>\n","\n","#### **Putting it all together**\n","\n","Finally, a somewhat simplified workflow to design, train, and test a network would be something like this. For each set of hyperaparemeters we want to optimise:\n","\n","1. Train each neural network (corresponding to each hyperparameter combination) using the training set.\n","2. Test performance on the validation set.\n","3. Pick the hyperparameters that give the best performance on the validation set.\n","4. Possibly retrain the network with optimal hyperparameters but now using both training and validation sets.\n","5. Test the network using the test set.\n","\n","***The Test Set is the final measure of performance but must never be used in the Training.***\n","\n","<br>\n","\n","\n","In the next two sections we will cover two special types of '***hyperparameters***'.\n","\n","<br/>\n"],"metadata":{"id":"Gw1L_8b8YAjH"}},{"cell_type":"markdown","source":["### 2.1 Activation functions\n","\n","Without activation functions, neural networks would just be a composition of linear operations. We introduce activation functions to introduce non-linearities:\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1NwA80ENDZvuJNAp_Zv8UURivfbGRT-Ax\" width=\"800\"/></center>\n","\n","But why do we want to introduce non-linearities, and how do we do it?\n","\n","This blog by Christopher Olah has a very nice visual explanation of it:"],"metadata":{"id":"39etFchsYAjJ"}},{"cell_type":"code","source":["%%html\n","<iframe src=\"https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\" width=\"1200\" height=\"700\"></iframe>"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":725},"executionInfo":{"status":"ok","timestamp":1668614398077,"user_tz":0,"elapsed":8,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"}},"outputId":"4850aea3-6a09-485b-ac16-a66bf7fade0c","id":"4lfoWR8AYAjJ"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<iframe src=\"https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\" width=\"1200\" height=\"700\"></iframe>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["There are many different types of activation functions. Historically, the three most commonly used are:\n","\n","- tanh\n","- sigmoid\n","- ReLU (Rectified Linear Unit)\n","\n","\n","<img src=\"https://miro.medium.com/max/1190/1*f9erByySVjTjohfFdNkJYQ.jpeg\" width=\"210\"/>\n","\n","<img src=\"https://miro.medium.com/max/4800/1*XxxiA0jJvPrHEJHD4z893g.png\" width=\"400\"/>\n","\n","But over time, many more have been developed:\n","\n","<center><img src=\"https://drive.google.com/uc?id=1KvLxqibUIQmP6tgjrV_YJdm949n5CY3q\" width=\"800\"/></center>\n","\n","A special case of activation functions are those involving outputs from several neurons in each layer:\n","\n","<center><img src=\"https://drive.google.com/uc?id=1USS9LY9VuVMeDqIwbLR5nrRchaE8ldHR\" width=\"400\"/></center>\n","\n","[link to wikipedia entry](https://en.wikipedia.org/wiki/Activation_function) about activation functions.\n","\n","\n","Fun version of the activation functions ([link](https://sefiks.com/2020/02/02/dance-moves-of-deep-learning-activation-functions/)):\n","\n","<center><img src=\"https://i0.wp.com/sefiks.com/wp-content/uploads/2020/02/sample-activation-functions-square.png?ssl=1\" width=\"400\"/></center>\n","\n","To reiterate, activation functions act as non-linear operators that provide models with additional flexibility.\n","\n","Play with different activations provided by PyTorch given a random tensor to see the effect they have. You can find them in the [documentation](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity):"],"metadata":{"id":"qrCBL4oLYAjK"}},{"cell_type":"code","source":["# simple example of the effects of activation functions:\n","m = nn.Sigmoid()                      #### change this activation function to see what happens\n","input = torch.randn(25, 25)*100\n","output = m(input)\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2)\n","fig.tight_layout(pad=5.0)\n","plt.rcParams[\"figure.figsize\"] = (10,10)\n","\n","im1 = ax1.imshow(input.detach().numpy(), cmap='seismic')#, vmin=-100, vmax=100)  # play with the colorscales to highlight the \n","im2 = ax2.imshow(output.detach().numpy(), cmap='seismic')#, vmin=-100, vmax=100) # effects ofthe different activations\n","plt.colorbar(im1, ax=ax1, fraction=0.04)\n","plt.colorbar(im2, ax=ax2, fraction=0.04)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"status":"ok","timestamp":1669543673139,"user_tz":0,"elapsed":1458,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"}},"outputId":"a8e49049-2ddc-4bd6-d9a5-bd885e030742","id":"nR5ssDArYAjK"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.colorbar.Colorbar at 0x7fa1cb748690>"]},"metadata":{},"execution_count":4},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x720 with 4 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["### 2.2 Loss functions\n","\n","Training a network is an optimisation procedure, and therefore it relies on being able to quantify somehow the performance of models (networks in DL). To do so, we define a quantity that distills the performance of the model to a single quantity, called the objective function. In deep learning, we often use a measure of 'error' for this, and we try to minimise it, hence the name loss function. This quantity has other names, most of them summarised in the following definition from the Deep Learning book (Goodfellow et al, 2016):\n","\n","*The function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function.*\n","\n","<br>\n","\n","The two most common loss functions are:\n","\n","- Mean squared error (MSE):  \n","\n","$$ L_{MSE} = \\frac{1}{N} \\sum_i^N ( y_i - h_i(x) )^2$$\n","\n","where $h_i(x)$ is an element of the output of the network, $y_i$ and element of what we define as the target, and $N$ is the total number of elements for this data sample. For example, $h_i(x)$ and $y_i$ could be pixels in an image, and $N$ the total number of pixels in this image.\n","\n","<br>\n","\n","- Cross entropy(CE): $$ \n","\n","$$ L_{CE} = \\sum_j^M y_j \\log(h_j(x)) $$\n","\n","where $h_j(x)$ is the probability (calculated using softmax) of the sample to belong to class $j$, $y_j$ is the real probability of the sample belonging to this class (either 1 or 0), and $M$ is the total number of classes we are using in the classification. For example, in the network we implemented on the first day, we had 10 classes (digits 0-9), and every time we ran a sample (image of a handwritten digit) through our network, we had an output vector with 10 values corresponding to the probability of the sample to belong to each class.\n","\n","<br>\n","\n","It is trivial to see that if we only have two classes, we can use the binary cross entropy, which only requires the true probability to belong to one of the classes:\n","\n","$$L_{BCE} = -y \\log(h(x)) - (1 - y)\\log(1-h(x))$$\n","\n","which looks like:\n","\n","<center><img src=\"https://drive.google.com/uc?id=1szARToXpU0uOEq0g4_HdpbUHQHFK9VOB\" width=\"400\"/></center>\n","\n","\n","<br>\n","\n","The loss in binary classification tasks, then, has this form:\n","\n","<center><img src=\"https://drive.google.com/uc?id=1NrdAcxR0CqpVJfAN_VmpgZsZM1Vq0TQj\" width=\"400\"/></center>\n","\n","\n","\n","<br>\n","\n","#### **Clarifications and links to activations for the cross-entropy loss:**\n","\n","The last layer of a binary classifier that uses a BCE loss is equivalent to logistic regression:\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1__yM95UJrKTtxtJQyR1HAtFFcwVmovOg\" width=\"300\"/></center>\n","\n","<br>\n","\n","where the activation of the last layer is a sigmoid function:\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1nhqUVA3MeXKl435F5HFaGOJtRfZornHC\" width=\"600\"/></center>\n","\n","<br>\n","\n","which also defines the decision boundary:\n","\n","<center><img src=\"https://drive.google.com/uc?id=1LVhaQrkOL6xCYiaOJ0xIaackmfhoAGRo\" width=\"700\"/></center>\n","\n","<br>\n","\n","Do you remember https://playground.tensorflow.org/? There we could see either the value of the output of the last layer discretised (ie, the decision boundary), or the values of the last layer of the classifier (which only had one neuron and used a BCE loss):\n","\n","\n","\n"],"metadata":{"id":"FiCvlghqYAjL"}},{"cell_type":"code","source":["%%html\n","<iframe src=\"https://playground.tensorflow.org/\" width=\"1200\" height=\"700\"></iframe>"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":745},"id":"87kQJ5dRL6l2","executionInfo":{"status":"ok","timestamp":1669839548530,"user_tz":0,"elapsed":8,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"}},"outputId":"c9526569-5a41-4b79-d20d-ecd6a15cacfb"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe src=\"https://playground.tensorflow.org/\" width=\"1200\" height=\"700\"></iframe>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["But for the case of multiple classes, we can't use sigmoid. \n","\n","**Why not?**\n","\n","<br>\n","\n","Because when we have only two classes, we can calculate the probability of the second class given the probability of the first, as they both have to add up to one. And if we use a sigmoid activation in the last layer, since its value is restricted between zero and one, we can interpret it as a probability.\n","\n","But this is no longer true for more than 2 classes, as reflected in the expression of the cross-entropy loss we saw above (where $h_j(x)$ is now $p_j$):\n","\n","$$ L_{CE} = \\sum_j^M y_j \\log(p_j) $$\n","\n","\n","<center><img src=\"https://drive.google.com/uc?id=1VOzMIXk6TRf3AiyEv0fT1S6fK6B_UpFj\" width=\"700\"/></center>\n","\n","\n","And now we have an output that can represent the probability of belonging to one of many multiple classes.\n","\n","**Why is it a probability now?** Think about what all the numbers add up to.\n","\n","The Cross Entropy implementation in `PyTorch` ([`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html))does a bit more than advertised, as it performs a `Softmax` operation for us. If we don't want `PyTorch` to do this for us, we can use the Negative-Log-Likelihood-Loss ([`NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)), because as we will see in the probability lecture, the cross entropy is the same as the negative of the log-likelihood.\n","\n","In addition, `PyTorch`'s `CrossEntropyLoss` does a `LogSoftmax`, not a Softmax, and the reason for doing this is that taking the log of the softmax values does not change the positions of the solution(s) of our problem (as log it is a monotonic function), but scales the values to improve numerical stability.\n","\n","<br>\n","\n","You can find a complete list of losses provided by PyTorch [here](https://pytorch.org/docs/stable/nn.html#loss-functions)\n","\n","<br>\n","\n","\n","\n","\n","<br>\n","\n","---\n","\n","<br>\n"],"metadata":{"id":"vuigJkHGL3gR"}},{"cell_type":"markdown","source":["## 3. **Training is an optimisation problem**: gradient descent and backpropagation\n","\n","#### **Gradient descent**\n","\n","When we train a network we are finding the values of the model parameters (weights and biases) that minimise the loss function. The loss function defines a hypersurface in a space that has as many dimensions as model parameters.\n","\n","That means that each combination of parameters correspond to a unique value of the loss, and we want to make this value as small as possible.\n","\n","For example, if we had a very simple network with only two parameters, the loss (or solution space, as it is also often called) could look something like this:\n","\n","<center><img src=\"https://drive.google.com/uc?id=1ttGRRIYmDwjBJOBMYQJlHMm0kJ6igqN2\" width=\"400\"/></center>\n"],"metadata":{"id":"ZWhuwteuYAjN"}},{"cell_type":"code","source":["%%html\n","<iframe src=\"https://losslandscape.com/explorer\" width=\"1200\" height=\"700\"></iframe>"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":725},"executionInfo":{"status":"ok","timestamp":1668701358111,"user_tz":0,"elapsed":214,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"}},"outputId":"af97c371-739d-437d-c204-cb988c1f6e9c","id":"KvhUNio8YAjO"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe src=\"https://losslandscape.com/explorer\" width=\"1200\" height=\"700\"></iframe>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["By looking at the plot above, one may imagine that a simple solution would be to densely sample sufficient combinations of parameters, and then pick the lowest value you find. While this, in theory, would work, such brute force approach is completely infeasible in even the smallest networks which have at least thousands of parameters because it would take too long to sample this solutions space properly.\n","\n","Instead, we use a **local optimisation** technique called **gradient descent**.\n","\n","<center><img src=\"https://drive.google.com/uc?id=1ZVczYMBAwXkYDprAUmbkmcdNMIivuJ3M\" width=\"800\"/></center>\n","\n","where, for simplicity, in this example we assume that the 'network' is a simple multiplication by the value $\\theta$.\n","\n","\\[**NOTE**: it is also common to us $J$ for the **loss** (or **cost** function)\\]\n","\n","In essence, gradient descent is a method that provides us with the information of the local curvature of the loss, and then we just update our model by changing the parameters in the steepest descent direction provided by the gradient expression. As the loss has a complex shape, this process needs to be iterated, as illustrated in the interactive web shown above.\n","\n","\n","In the figure above, we only have one parameter $\\theta$, but in practice, our networks have many more parameters than that. Additionally, they have dependencies betwen them. \n","\n","<br>\n","\n","#### **Backpropagation**\n","<center><img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2022/02/DS-Guide-to-Gradient-Descent_Pic5.gif\" width=\"400\"/></center>\n","\n","\n","The question is then, how do we use gradient descent to train our networks? \n","\n","The answer is **backpropagation**: a combination of gradient descent and the derivative chain rule.\n","\n","Let's illustrate it with a very simple step-by-step example:\n","\n","<br>\n","\n","| Step 1 |\n"," :---: \n","| <center><img src=\"https://drive.google.com/uc?id=1Gy1qztT343rsaU0PjS0QwF1nhia5wq7-\" width=\"800\"/></center> |\n","\n","<br>\n","\n","Here is worth noting that $a_4$ depends on all the model parameters $w_i,b_i$.\n","\n","<br>\n","\n","| Step 2 |\n"," :---: \n","| <center><img src=\"https://drive.google.com/uc?id=1MAw3PQNVmLkl_TsZ1RlDnDi5lCI_7516\" width=\"800\"/></center> |\n","\n","<br>\n","\n","where here we add a $\\frac{1}{2}$ in front of the loss to simplify calculations:\n","\n","$$\\require{cancel}$$\n","$$\\frac{\\partial C}{\\partial a_4} = \\frac{1}{\\cancel{2}} \\cancel{2} (a_4 - y)$$\n","\n","and then we continue with the chain rule: \n","\n","<br>\n","\n","| Step 3 |\n"," :---: \n","| <center><img src=\"https://drive.google.com/uc?id=11jEYeMTjHPeNwHtU0ayCrr_0FmdFCt9Z\" width=\"800\"/></center> |\n","\n","<br>\n","\n","And the same principle can be applied in networks with more neurons per layer, of course:\n","\n","<center><img src=\"https://drive.google.com/uc?id=1DHzIAkMAyTpq1i2YgaR6D6kfunsaaFWZ\" width=\"600\"/></center>\n","\n","[Here](https://drive.google.com/file/d/1OF9mdC1pUXScfFxdmjucU3IjBIbburTD/view?usp=share_link) you can find a video recorded by Oscar Bates (who will deliver the lecture on VAEs next week). In the video, he calculates by hand all the steps necessary to calculate gradients for all parameters for a simple network using backprop.\n","\n","<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"gCBbd-EFYAjP"}},{"cell_type":"markdown","source":["## **4**. Batch, mini-batch, and stochastic gradient descent\n","\n","So far we have analysed what happens when we put a single sample through a network, and how to use the loss to measure how well a network performs a given task.\n","\n","In reality, our datasets are composed of hundreds, thousands, or even millions of samples. And now we should ask ourselves the question: how do we use all the data we have to train a network?\n","\n","There are three different strategies to combine the data we have. In each of those, one **iteration** (one update of the model) consists of:\n","\n","- **Batch gradient descent**: we use all the data samples (**batch**) we have, and for each we calculate a gradient. Once we have calculated all the gradients corresponding to all the data values, we combine them (average, sum, etc), and we update the model parameters using a particular learning rate. \n","\n","- **Mini-batch gradient descent**: we use subsets of the data (**mini-batches**) to compute the gradients corresponding to each sample in the subset, then combine them (again, average, sum, etc), and update the model parameters.\n","\n","- **Stochastic gradient descent**: we use only one sample of the data to calculate a gradient (hence the name **stochastic**), and with this gradient we update the model before moving on the the next data sample.\n","\n","When we are training networks, we normally define how many **epochs** we want to train. Every time we pass all the samples available in our dataset through our network for training, we have performed an epoch. The table below gives an examples of iterations and epochs for a dataset with 1000 samples and with a mini-batch size of 100:\n","\n","<br>\n","\n","|    | batch | mini-batch | stochastic |\n","| -- | --: | --: | --: |\n","| iterations |  1  | 10 | 1000 |\n","| epochs     |  1  |  1 |    1 |\n","\n","<br>\n","\n","Visually ([gif source](https://medium.com/analytics-vidhya/gradient-descent-optimization-techniques-4316419c5b74)):\n","<center><img src=\"https://miro.medium.com/max/1400/1*m4taSeNUZzTiEr-aSv1c3w.gif\" width=\"600\"/></center>\n","\n","\n","**NOTE**:\n","`PyTorch` calls its mini-batch optimiser SGC (for stochastic gradient descent), which can be confusing. It is a nomenclature convention they use which is different from the one used here.\n","\n","<br>\n","\n","PyTorch provides different optimisers we can use.\n","\n","Two of the most common optimisers are **SGD with momentum** and **Adam**, and during this module we will use them quite a lot, but it is a good exercise to experiment with other algorithms."],"metadata":{"id":"EpG030cdYAjQ"}},{"cell_type":"code","source":[" %%html\n","<iframe src=\"https://pytorch.org/docs/stable/optim.html\" width=\"1200\" height=\"700\"></iframe>"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":725},"executionInfo":{"status":"ok","timestamp":1668781504902,"user_tz":0,"elapsed":222,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"}},"outputId":"7233b652-f01b-4741-8110-2782f14eef9d","id":"rsKU0H4MYAjQ"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe src=\"https://pytorch.org/docs/stable/optim.html\" width=\"1200\" height=\"700\"></iframe>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["even though, at the end of the day:\n","\n","<center><img src=\"https://miro.medium.com/max/1000/1*kqPSj6ylpvMf26rAiX7m6g.webp\" width=\"700\"/></center>\n","\n","\n","<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"d7Ra8ZjXYAjQ"}},{"cell_type":"markdown","source":["### *Exercise*\n","\n","We will now revisit the network we implemented on the first day, but we will add a few modifications:\n","\n","1. Add one extra layer to the network.\n","2. Add biases.\n","3. Change the activation functions\n","4. Change the batch size (from pure SGD to full batch)\n","5. Modify the learning rate and observe what happens to the loss and accuracy evolution curves\n","6. Change the random seed\n","\n","Also, discuss and try to justify which modifications make sense in this context."],"metadata":{"id":"aGW0V_66GBhe"}},{"cell_type":"code","source":["# create a simple network\n","class simpleFFN(nn.Module):\n","  def __init__(self):\n","    super(simpleFFN, self).__init__()\n","    self.hidden_1 = nn.Linear(784, 200, bias=False)\n","    self.hidden_2 = nn.Linear(200, 50, bias=False)              ### add bias terms\n","    ## add a new activation layer\n","    self.output = nn.Linear(50, 10, bias=False)\n","    self.activation = ### modify the activation function\n","    \n","  def forward(self, X):\n","    z1 = self.hidden_1(X)\n","    a1 = self.activation(z1)\n","    z2 = self.hidden_2(a1)\n","    a2 = self.activation(z2)           ### modify the forward pass to include the new layer\n","    z3 = self.output(a2)\n","    a3 = self.activation(z3)\n","    return a3\n","\n","# test that it runs  \n","x = torch.randn((1, 1, 784))\n","model = simpleFFN()\n","y = model(x)\n","print(y)\n","print(model)"],"metadata":{"id":"WNlT1-zQ05ox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# download MNIST\n","mnist_train = MNIST(\"./\", download=True, train=True)\n","mnist_test = MNIST(\"./\", download=True, train=False)"],"metadata":{"id":"u9m0Kg3QEzxI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split the data\n","shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split(mnist_train.train_data, mnist_train.train_labels) \n","indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]\n","\n","# define an standardisation function\n","def apply_standardization(X): \n","  X /= 255.\n","  X -= 0.1307\n","  X /= 0.3081\n","  return X\n","\n","# standardise the data\n","X_train, y_train = apply_standardization(mnist_train.train_data[indices[0]].float()), mnist_train.train_labels[indices[0]]\n","X_val, y_val = apply_standardization(mnist_train.train_data[indices[1]].float()), mnist_train.train_labels[indices[1]]\n","X_test, y_test =  apply_standardization(mnist_test.test_data.float()), mnist_test.test_labels"],"metadata":{"id":"HYqjkRTPE4tO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the TensorDatasets containing mnist_train, mnist_validate, and mnist_test\n","mnist_train = TensorDataset(X_train, y_train.long())\n","mnist_validate = TensorDataset(X_val, y_val.long())\n","mnist_test = TensorDataset(X_test, y_test.long())"],"metadata":{"id":"l39A5Ml1FCft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# implement training and validation functions\n","\n","def train(model, optimizer, criterion, data_loader):\n","    model.train()                         # the model is in the training mode so the parameters(weights)to be optimised will be updated\n","    train_loss, train_accuracy = 0, 0     # initialise loss and accuracy to 0 for training\n","    for X, y in data_loader:              # iterate over the mini-batches defined in the data loader\n","        X, y = X.to(device), y.to(device) # send data to the device (GPU in our case)\n","        optimizer.zero_grad()             # resetting optimiser info\n","        a2 = model(X.view(-1, 28*28))     # forward pass\n","        loss = criterion(a2, y)           # compute loss\n","        loss.backward()                   # backpropagation to calculate the gradients\n","        train_loss += loss*X.size(0)      # # add it up for different mini-batches and undo loss normalisation\n","        y_pred = F.log_softmax(a2, dim=1).max(1)[1]  # get predictions\n","        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0) # compute accuracy\n","        optimizer.step()                  # perform a step of gradient descent\n","        \n","    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)  # here we can average over the whole dataset\n","\n","\n","def validate(model, criterion, data_loader):      # does not need optimiser\n","    model.eval()                                  # model is set to evaluation mode so no dropout or any other funny stuff here\n","    validation_loss, validation_accuracy = 0., 0. # initialise loss and accuracy to 0 for training\n","    for X, y in data_loader:                      # iterate over the mini-batches defined in the data loader\n","        with torch.no_grad():                     # deactivates autograd engine\n","            X, y = X.to(device), y.to(device)     # send data to the device (GPU in our case)\n","            a2 = model(X.view(-1, 28*28))         # forward pass\n","            loss = criterion(a2, y)               # evaluate loss\n","            validation_loss += loss*X.size(0)     # add it up for different mini-batches and undo loss normalisation\n","            y_pred = F.log_softmax(a2, dim=1).max(1)[1]  # get predictions\n","            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0) # compute accuracy\n","            \n","    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)  # here we can average over the whole dataset"],"metadata":{"id":"KbWyTwASFDPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set hyperparameters\n","\n","seed = ### change the random seed (what happens if we change it??)\n","lr = ### change the learning rate\n","momentum = 0.9\n","batch_size = ### change the batch size\n","test_batch_size = 1000\n","n_epochs = 30\n","\n","set_seed(seed)\n","model = simpleFFN().to(device)                                              # instantiate model and send it to the GPU\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)   # instantiate the optimizer\n","criterion = nn.CrossEntropyLoss() "],"metadata":{"id":"ovYwQ3ZBFOsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create DataLoaders\n","\n","train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0) ## num_workers=0 means that the main process will retrieve the data.\n","validation_loader = DataLoader(mnist_validate, batch_size=test_batch_size, shuffle=False, num_workers=0)\n","test_loader = DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False, num_workers=0)"],"metadata":{"id":"Db_8ocXSFaQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train for the number of epochs specified\n","\n","set_seed(seed)\n","liveloss = PlotLosses()    # plots evolution of loss and accuracy\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy.item()\n","    \n","    validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n","    logs['val_' + 'log loss'] = validation_loss.item()\n","    logs['val_' + 'accuracy'] = validation_accuracy.item()\n","    \n","    liveloss.update(logs)\n","    liveloss.draw()\n","    print(validation_loss.item())"],"metadata":{"id":"NPO7na2IFkNp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","---\n","\n","<br>\n"],"metadata":{"id":"4pEeg1_rKzIO"}},{"cell_type":"code","source":["from torchsummary import summary\n","summary(model,(1,28*28))"],"metadata":{"id":"MVwraLS_Lxa_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.size()"],"metadata":{"id":"muWldd_jM2o-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Bias and variance, and regularisers:\n","\n","#### **Bias and variance**:\n","\n","<center><img src=\"https://miro.medium.com/max/1400/1*LMExgwNg1emIEaN88TaU0g.png\" width=\"600\"/></center>\n","\n","**Bias** (or underfitting) and **variance** (or overfitting) analogy in polynomial fitting:\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1SyqF66YZRxX5xgOcZ1aC2T-7uFhuTTGP\" width=\"800\"/></center>\n","\n","<br>\n","\n","The capacity of the model is closely related to the model complexity. Increasing the capacity of a model increases how expressive a model can be, or in other words, how much variation in the data it can accomodate. The capacity of the model has to be adapted to correctly capture the distribution of the data in the dataset. \n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1BeJ0cZlrBAvy5Ep_sD0hebwqvVLGZj1s\" width=\"800\"/></p><p align = \"center\">\n","<i>image from the book Deep learning (Goodfellow et. al 2016)</i>\n","</p>\n","\n","<br>\n","\n","We have already seen a case of bias (or underfitting) in the first training we did on Monday:\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1Tcu2e0HBnlvqQpHVNx4HAoMbRdtiqUYR\" width=\"600\"/></p><p align = \"center\">\n","<i>the model does not have enough capacity to predict the correct digits</i>\n","</p>\n","\n","<br>\n","\n","and from the exercise we did this morning, we can see a clear case of variance (overfitting):\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1inwH-1UvYFh9IVYYXiigV783rIJkoplu\" width=\"600\"/></p><p align = \"center\">\n","<i>the model does not have enough capacity to predict the correct digits</i>\n","</p>\n","\n","<br>\n","\n","The natural solution to avoid underfitting is to increase the capacity of our models, but to avoid overfitting is less straightforward. Sometimes we want to keep the expressivity of the model, but avoid overfitting to the training set.\n","\n","A common strategy to solve this overfitting problem is to add **regularisation**, which aims at reducing variance (overfitting) without increasing bias (underfitting).\n","\n","Regularisers can be:\n","\n","- **explicit regularisers**: add terms to the loss definition\n","\n","or\n","\n","- **implicit regularisers**: adopt strategies that mitigate overfitting by any other means, for example dropout, or data augmentation.\n","\n","<br>\n","\n","#### **Explicit regularisers**\n","\n","There is a wide range of explicit regularisers we could use (some people even use networks as added terms to the loss). But the two most common ones are:\n","\n","-$L_1$:\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=15IP5FCi9yC6XdhIDPpDa1Wd8BydKoJxI\" width=\"400\"/></p><p align = \"center\">\n","</p>\n","\n","<br>\n","\n","-$L_2$ \n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=14dU7hmRz8fa0ivei4vD5U_Tfq1kHH0SQ\" width=\"400\"/></p><p align = \"center\">\n","</p>\n","\n","Where in both cases the parameter $\\lambda$ controls the strength of the regularisation. Is this a hyperparameter? And how do we choose the optimal one?\n","\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1z293J_DJxc6Dv5CXCSiU7It4mzwFWoZ0\" width=\"600\"/></p><p align = \"center\">\n","</p>\n","\n","We try different values of $\\lambda$ and select the best one.\n","\n","<br>\n","\n","#### **Implicit regularisers**\n","\n","We will cover three examples of implicit regularisers in this section: dropout, data augmentation, and batch normalisation. We focus on these three because they are routinely used in deep learning, and they have proven to be robust solutions in a wide variety of problems.\n","\n","\n","#### **- Dropout**\n","\n","\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=12zFVeSJXHtwMecwaifMSrWjTpTN4UFOj\" width=\"800\"/></p><p align = \"center\">\n","image from <i>Dropout: A Simple Way to Prevent Neural Networks from Overfitting (Srivastava, Hinton, et al 2014)</i>\n","</p>\n","\n","<br>\n","\n","<p><img src=\"https://drive.google.com/uc?id=1A9MkG4xqt7ag7j3RZaU2sfsCGU4RUBjn\" width=\"1000\"/></p><p>\n","</p>\n","\n","<br>\n","\n","Dropout is implemented by defining a parameter $p$ that defines the probability of a neuron in a layer to be dropped, and it is only applied during training. During inference (when we are using a trained model), we don't apply dropout, but instead scale the outputs of the neurons by a factor $p$ to keep the magnitude of the data consistent as it goes through the network.\n","\n","Remember the `model.train()` and `model.eval()` we saw with Debbie? Here it would activate or deactive the dropout in the `nn.Module` we define.\n","\n","A video by Hinton explaining it in more detail [here](https://www.youtube.com/watch?v=kAwF--GJ-ek)\n","\n","Finally, dropout can also be interpreted as a form of [ensemble learning](https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/).\n","\n","<br>\n","\n","#### **- Data augmentation**\n","\n","Data augmentation is a strategy to increase our dataset by generating new samples based on the original dataset. These modifications should not change the ideal response of the network to them. Common strategies to augment image datasets are:\n","- rotate\n","- translate\n","- zoom\n","- crop\n","- add noise\n","- change brightness level\n","\n","<br>\n","\n","<p><img src=\"https://drive.google.com/uc?id=18PnNn5wWkA8kRaJmJOa64dBiBFOqpfHt\" width=\"800\"/></p><p>\n","</p>\n","\n","<br>\n","\n","#### **- Batch normalisation**\n","\n","\n","\n","Batch normalisation is based on normalising the outputs of the neurons (normally applied after activations, but could be applied before too). We have already seen that it is desirable to standardise the inputs, and batch normalisation extends this concept to the layers of the model.\n","\n","This normalisation is controled by parameters that are also updated during training.\n","\n","There are different interpretations of what batch normalisation does. Some people interpret it as a means to reduce the internal covariate shift (see [original paper](https://arxiv.org/abs/1502.03167) for details), but more recent developments seem to point that it acts as a regulariser that smooths the objective function (and therefore make it better suited to gradient descent minimisation methods).\n","\n","***internal covariate shift***: *changes in the distribution of data as it navigates through the layers of the networks have.*\n","\n","In summary, batch normalisation:\n","\n","- Normalise(standardise) values at each layer of the network, instead of just on input layer.\n","- Accelerates training by stabilizing the layer changes through the iterations.\n","- Provides some regularization, like dropout, but not as strong (open topic of discussion).\n","- Batch normalisation should really be called batch standardization.\n","It reduces the probability that vanishing or exploding gradients occur.\n","\n","[A good video by Andrew Ng](https://www.youtube.com/watch?v=tNIpEZLv_eg) on this topic\n","\n"],"metadata":{"id":"MMBI_ljFKpCi"}}]}