{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqAa5LwIn1iv9S0OKaJLj4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n","\n","\n","---\n"],"metadata":{"id":"OfDZgR2DATTm"}},{"cell_type":"markdown","source":["# **Deep Reinforcement Learning**\n","\n","\n","#### **Contents/agenda**\n","\n","1. What is Reinforcement Learning?\n","2. What is Deep RL?\n","3. How is RL different from Supervised and Unsupervised Learning?\n","4. Types of (Deep) RL algorithms.\n","5. Implementing a Deep RL algorithm.\n","\n","\n","\n","#### **Learning outcomes**\n","\n","1. Understand the basic concepts used in RL papers and algorithms\n","2. Understand the different components of RL problems.\n","3. Understand the differences between RL and other types of learning.\n","4. Understand how Deep Learning could have a role in RL.\n","5. Learn how to implement a  simple Deep RL algorithm.\n","\n","\n","<br/>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"npGKuw3yAeMz"}},{"cell_type":"code","source":["%matplotlib notebook\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.datasets\n","import torchvision.transforms as transforms\n","from torchsummary import summary\n","\n","import numpy as np\n","from PIL import Image\n","from pathlib import Path\n","from collections import deque\n","import matplotlib.pyplot as plt\n","import random, time, datetime, os, copy\n","from IPython.display import display, clear_output\n","from IPython.html.widgets import interactive\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8lfARp6HAn8i","executionInfo":{"status":"ok","timestamp":1670867193037,"user_tz":0,"elapsed":1990,"user":{"displayName":"Carlos Cueto Mondéjar","userId":"10605816413378977833"}},"outputId":"129ce809-5237-4985-a5ab-7ded1f5c0c09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n","  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"]}]},{"cell_type":"code","source":["def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  # uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v5aix1fzAx2q","executionInfo":{"status":"ok","timestamp":1670867193511,"user_tz":0,"elapsed":483,"user":{"displayName":"Carlos Cueto Mondéjar","userId":"10605816413378977833"}},"outputId":"9ccb1065-c2b5-4a47-dd86-b853e886a765"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cuda installed! Running on GPU!\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rV02rtTFqj5O","executionInfo":{"status":"ok","timestamp":1670867196501,"user_tz":0,"elapsed":2992,"user":{"displayName":"Carlos Cueto Mondéjar","userId":"10605816413378977833"}},"outputId":"5316f444-d706-47a3-b0d2-79f539cb50b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## What is Reinforcement Leaning?\n","\n","Reinforcement Learning (RL) is a much larger field than Deep RL, so let's start by reviewing some what RL is and some of its core concepts. This will allow you to better undersand Deep RL and literature on the topic.\n","\n","Our goal in artificial intelligence is, arguably, to allow machines to find solutions to problems themselves, so that we only need to specify the problem setting or the goal. This requires autonomous learning of decision-making processes in order to achieve these goals. That is, we need to develop systems that are able to learn to make good sequences of decisions (usually under uncertainty). \n","\n","RL is the study of agents that can learn by trial an error how to make good seqeunces of decisions. It involves:\n","\n","- optimisation - learning to make decisions that have optimal consequences;\n","- delayed consequences - the agent might not know if it made a good decision until much later;\n","- exploration - the agent learns about the world by making decisions within the it and, in the process, altering the world with its decisions;\n","- generalisation - the agent needs to learn to make decisions that can generalise beyond the experience lived.\n","\n","\n","### RL concepts\n","\n","<img src=\"https://drive.google.com/uc?id=1pVsXYC39_Xbn9_PZ4a4vmq6AVu21_L--\" width=\"700\"/>\n","\n","The main characters in our RL worldview are the **agent** and the **environment**. The environment is the world in which the agent lives. The agent interacts with the enviroment through actions $A_t$ and receives information about the environment through observations $O_t$ and through (scalar) rewards $R_t$. \n","\n","We usually divide the interaction process between agent and environment in a series of discrete steps, with each step denoted by the subscript $t$ in our notation. \n","\n","This does **not** mean that what we see here is an RNN: this is an abstract representation of the problem that we are trying to pose, and not of the algorithm or model that we use to solve the problem.\n","\n","Although it is usual to have discrete action sequences, there are extensions of RL that work with with continuous interactions. We will not look at these in this class.\n","\n","\n","#### **The state**\n","\n","<img src=\"https://drive.google.com/uc?id=1L1Ajt1EfXZoMXiVnVogxlkecwyw7ffDn\" width=\"700\"/>\n","\n","There are two states that we talk about in RL. The **state of the environment**, which might or might not be known to the agent, and which might be partially or completely observable by the agent. The **state of the agent**, which it keeps as a way to understand its situation within the world. Both the environment and agent states could be deterministic or stochastic.\n","\n","Now, what should the state of the agent contain in order for it to keep track of its situation? To answer that, we first need to understand what is the **history**. The history is the full sequence of observations, actions and rewards:\n","\n","$$\\mathcal{H}_t = \\{O_0, A_0, R_1, O_1, A_1, R_2, ..., O_{t-1}, A_{t-1}, R_t, O_t\\}$$\n","\n","The state of the agent is only a function of the history. Depending on the amount of information that the observations provide to the agent, we also need to consider specific cases: full observability, where the observation is the full environment state (for example, if we have an agent that plays chess or go); partial observability, where the observation is only a limited view of the environment state (for example, if our agent is a robot with a camera or a poker-playing agent that can only see its own cards).\n","\n","So, how much information should the state of the agent contain? Generally in RL, we assume that our problem is a **Markov decision process (MDP)**, which allows us to more easily reason about our problems and pose algorithms to solve them by limiting the amount of information stored in the agent state. A decision process is MDP if,\n","\n","$$p(r, s | S_t, A_t) = p(r, s | \\mathcal{H}_t, A_t)$$\n","\n","That is, if the probability of a reward $r$ and subsequent state $s$ depends only on the current state and action, and not on the entire history. Or, what is the same, that the state contains all relevant information that we need to know from the history. This is not to say that the state contains everything, only that adding more information to it does not help. This is very useful because the history might be very large (in terms of computer memory), whereas the state might be much smaller. \n","\n","The trivial example of MDP is one in which $S_t = \\mathcal{H}_t$, but obviously that is not very useful. Instead, the state is normally some form of compression of the history. Can you think of a way in which you could create such a compression in practice?\n","\n","In partially observable environments, we can extend the concept of MDP to partialy observable Markov decision processes (POMDP), which we are not going to look at rigorously.\n","\n","<img src=\"https://drive.google.com/uc?id=1JXapof376b0ZSq5HYczN7AL1DWSANN0B\" width=\"700\"/>\n","\n","#### **The policy**\n","\n","<img src=\"https://drive.google.com/uc?id=1VN-bnqtFdq5cvrlncMv2n3RWn70P53Zh\" width=\"700\"/>\n","\n","A mapping from states to actions is called a **policy**, and it defines the behaviour of the agent. Policies can be deterministic:\n","\n","$$A_t = \\mu(S_t)$$\n","\n","and they can be stochastic:\n","\n","$$A_t \\backsim \\pi(\\cdot | S_t)$$\n","\n","\n","#### **The value**\n","\n","RL is based on the reward hypothesis, which states that: \n","\n",">*Any goal can be formalised as the outcome of maximising a cumulative reward.*\n","\n","Think about it - can you find a counter-example for this? \n","\n","In order to pose RL problems and create algorithms to solve them, we will have figure out what is the most appropriate reward function so that maximising it allows the agent to reach the expected goal.\n","\n","Some examples of problems and their associated reward functions:\n","\n","- Fly a felicopter - air time, inverse distance, ...\n","- Manage an investent portfolio - gains, gains minus risk, ...\n","- Make a robot walk - distance, speed, lack of accidents, ...\n","- Play a game - win, maximise score, ...\n","\n","The **goal**, or more commonly the **return** in RL lingo, is then the sum of all (discounted) future instantatenous rewards:\n","\n","$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$$\n","\n","for the discount factor $\\gamma \\in [0, 1]$, which trades the importance of immediate vs long-term rewards. Actions cannot influence the past, only the future, so the goodness of an action can only be measured by looking at its impact in future rewards.\n","\n","We cannot always observe the return, and instead we look at its expectation given that we are currently at a given state, which we call the **value function** or **state value function**:\n","\n","$$V(s) = \\mathbb{E}[G_t | S_t = s] =\\\\ \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | S_t = s]$$\n","\n","If we condition the value also on the actions we can also write the **state-action value function**:\n","\n","$$Q(s, a) = \\mathbb{E}[G_t | S_t = s, A_t = a] =\\\\ \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | S_t = s, A_t = a]$$\n","\n","Any goal can then be achieved by maximising the value, which depends on the states and actions taken. There is no supervised feedback, though, no one is saying whether actions are correct or incorrect, only whether they lead to maximising the reward.\n","\n","Given a certain policy, the dependency of these value functions on the policy is usually expressed as:\n","\n","$$V_\\pi(s) = \\mathbb{E}[G_t | S_t = s, \\pi] =\\\\ \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | S_t = s, \\pi]$$\n","\n","&nbsp;\n","\n","$$Q_\\pi(s, a) = \\mathbb{E}[G_t | S_t = s, A_t = a] =\\\\ \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | S_t = s, A_t = a, \\pi]$$\n","\n","&nbsp;\n","\n","which explicitly says that these are the value functions given that all future actions are selected given some policy $\\pi$.\n","\n","\n","#### **Bellman equations**\n","\n","The value functions can be expressed in a resursive form, leading to the **Bellman equations**:\n","\n","$$V_\\pi(s) = \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | S_t = s, \\pi] =\\\\\n","\\mathbb{E}[R_{t+1} + \\gamma V_\\pi(S_{t+1}) | S_t = s, \\pi]$$\n","\n","&nbsp;\n","\n","$$Q_\\pi(s, a) = \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | S_t = s, A_t = a, \\pi] =\\\\\n","\\mathbb{E}[R_{t+1} + \\gamma Q_\\pi(s_{t+1}, a_{t+1}) | S_t = s, A_t = a, \\pi]$$\n","\n","&nbsp;\n","\n","Similar equations can be written for the optimal values:\n","\n","$$V^*(s) = \\max_a \\mathbb{E}[R_{t+1} + \\gamma V^*(S_{t+1}) | S_t = s]$$\n","\n","&nbsp;\n","\n","$$Q^*(s, a) = \\max_a \\mathbb{E}[R_{t+1} + \\gamma Q^*(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]$$\n","\n","&nbsp;\n","\n","which do not depend on the policy.\n","\n","These equations are the basis for most RL algorithms.\n","\n","\n","#### **The learning in Reinforcement Learning**\n","\n","We have seen the different components of an RL problem and how we can measure the idoneity of action sequences, but we haven't looked yet at what the agent will actually be learning when we pose our RL algorithms.\n","\n","When developing RL algorithms, there are a number of different components that an agent can learn in order to choose optimal actions. These usually are:\n","\n","- The policy\n","- The state value function\n","- The action-state value function\n","- A model of the state of the environment\n","- A model of how the agent state will evolve\n","\n","Depending on the one or multiple components that our agent learns, we will obtain different types of RL algorithms.\n","\n","\n","## What is Deep Reinforcement Learning?\n","\n","In this course, we have seen how deep learning is extremely useful tool to encode, decompose, transform data in order to reason about it and extract useful features from it. Deep RL then emerges from using these deep neural networks in order to represent, learn, and inform the different components of our agent within the framework of RL. \n","\n","Why could this be a good idea? \n","\n","It allows us to use neural network's characteristics, thanks for the universal function approximation theorem, to create more complex and sophisticated agents that can better reason about very high-dimensional spaces that are closer to real life.\n","\n","For example, let's say we have a robot agent that has uses a camera to observe the environment and that the camera produces RGB images of size 100x200. What would be the space of possible inputs of our RL agent if it only used one image at a time? $(256^{100x200})^3$\n","\n","The idea of using neural networks in RL is not new. It was originally proposed in the 1990s [1], but never gained much traction. It wasn't until the mid-2010s that DeepMind showed that it was possible to use neural networks successfully to solve RL problems with very high-dimensional outputs by training networks to play Atari games with, in many cases, better performance than humans [2]."],"metadata":{"id":"BWqYBG47A0hZ"}},{"cell_type":"code","source":["%%html\n","<iframe src=\"https://www.youtube.com/embed/Q70ulPJW3Gk\" width=\"600\", height=\"400\"></iframe>"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"4Q2bbOhGD14f","executionInfo":{"status":"ok","timestamp":1670867196502,"user_tz":0,"elapsed":5,"user":{"displayName":"Carlos Cueto Mondéjar","userId":"10605816413378977833"}},"outputId":"b9379d05-536f-4903-ef7d-36cdb70cbcf4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe src=\"https://www.youtube.com/embed/Q70ulPJW3Gk\" width=\"600\", height=\"400\"></iframe>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["Since then, there have been incredible advances in Deep RL, including the famous Alpha* family (AlphaGo, AlphaZero, AlphaTensor). \n","\n","<img src=\"https://drive.google.com/uc?id=13eqryeyFtLn9B-8vM7J7u6pdHc3pnwm_\" width=\"500\"/>\n","\n","\n","## How is RL different from Supervised and Unsupervised Learning?\n","\n","We could think of RL as learning by experience vs other forms of learning in which the model learns by example.\n","\n","We can also understand these differences by looking at the characteristics of RL, which we introduced before:\n","\n","- optimisation;\n","- delayed consequences;\n","- exploration;\n","- generalisation.\n","\n","And we can compare that with those of supervised:\n","\n","- optimisation;\n","- generalisation;\n","- learning by example, but with correct labels provided;\n","\n","and unsupervised learning:\n","\n","- optimisation;\n","- generalisation;\n","- learning by example, but with no labels from the world;\n","\n","In neither of them there exists exploration, while consequences are immediate instead of delayed, example by example.\n","\n","\n","## Types of (Deep) RL algorithms\n","\n","<img src=\"https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg\" width=\"700\"/>"],"metadata":{"id":"CjH4-3ayENbK"}},{"cell_type":"markdown","source":["## Implementing a Deep RL algorithm\n","\n","<img src=\"https://drive.google.com/uc?id=1Lc2ZuEMNvKyj7NW55Yi6z8S-qOwK7WIb\" width=\"300\"/>\n","\n","In this practical, we are going to train a Deep RL model to play Mario! Our implementation is inspired by the one found in the [PyTorch documentation.](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html)\n","\n","We will use a network to learn a model of the optimal Q state-action value function. The rationale for doing this is that, if we know the value of taking a certain action at a certain state, it will be possible to choose optimal actions based on this model.\n","\n","\n","### OpenAI Gym\n","\n","We will get access to a predefined environment for our agent to play Mario by using [OpenAI's gym](https://github.com/openai/gym), an open-source Python library that provides a standard API to communicate between RL agents and a set of standard envinments, including many games."],"metadata":{"id":"8vlI3j3r2lr0"}},{"cell_type":"code","source":["%%bash\n","pip install gym-super-mario-bros==7.4.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rlHU6RQp2lB2","executionInfo":{"status":"ok","timestamp":1670867199184,"user_tz":0,"elapsed":2686,"user":{"displayName":"Carlos Cueto Mondéjar","userId":"10605816413378977833"}},"outputId":"92a24973-0577-4312-f1de-b6f48c4ed6fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym-super-mario-bros==7.4.0 in /usr/local/lib/python3.8/dist-packages (7.4.0)\n","Requirement already satisfied: nes-py>=8.1.4 in /usr/local/lib/python3.8/dist-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n","Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.64.1)\n","Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n","Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.25.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.21.6)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.13.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.11.0)\n"]}]},{"cell_type":"code","source":["import gym\n","from gym.spaces import Box\n","from gym.wrappers import FrameStack\n","\n","# NES Emulator for OpenAI Gym\n","from nes_py.wrappers import JoypadSpace\n","\n","# Super Mario environment for OpenAI Gym\n","import gym_super_mario_bros"],"metadata":{"id":"wxHSrPM54vnM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### The environment\n","\n","Let's start by initialising our environment:"],"metadata":{"id":"Fk1pHlFu6lAL"}},{"cell_type":"code","source":["# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n","if gym.__version__ < '0.26':\n","    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n","else:\n","    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ah7kfdZn6oyv","executionInfo":{"status":"ok","timestamp":1670867199185,"user_tz":0,"elapsed":14,"user":{"displayName":"Carlos Cueto Mondéjar","userId":"10605816413378977833"}},"outputId":"e8c23239-345b-4eff-b018-7a7b06b30f6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n","  logger.warn(\n"]}]},{"cell_type":"markdown","source":["In this setting, our agent will interact with the environment through a series of predetermined valid actions. For this example, we will limit the valid actions to `walk right` and `jump right`."],"metadata":{"id":"NDkeGYTz5jNo"}},{"cell_type":"code","source":["# Limit the action-space to\n","#   0. walk right\n","#   1. jump right\n","env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5zM6SKCr6fNq","executionInfo":{"status":"ok","timestamp":1670867199186,"user_tz":0,"elapsed":14,"user":{"displayName":"Carlos Cueto Mondéjar","userId":"10605816413378977833"}},"outputId":"00f482ce-8da9-45cf-8c41-635e64d291d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"markdown","source":["\n","In turn, the Mario environment will offer the agent the opportunity to observe its state by looking at the instantenous game screen, an RGB image of size `[3, 240, 256]`.\n","\n","The environment will also provide some other information to the agent, such as coins obtained, lives remaining, time spent, whether the character is small or large, etc."],"metadata":{"id":"wyFOtb0R60Rx"}},{"cell_type":"code","source":["# reset the environment\n","# test taking a step\n","# print information returned by the environment"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKKAyKfC61O2","executionInfo":{"status":"ok","timestamp":1670867199186,"user_tz":0,"elapsed":11,"user":{"displayName":"Carlos Cueto Mondéjar","userId":"10605816413378977833"}},"outputId":"053102ad-d813-4a07-9d13-44d097c3866b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(240, 256, 3),\n"," 0.0,\n"," False,\n"," {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  logger.deprecation(\n"]}]},{"cell_type":"markdown","source":["Because the information that our agent needs to extract from the screen does not depend, for the most part, on its colour, we will need to preprocess these to make them grayscale. We will also reduce the size of this data stream by downsampling the images and by skipping some of the frames.\n","\n","OpenAI's gym has a number of wrappers that allow us to do this easily:"],"metadata":{"id":"MmkwNnDt7Rc4"}},{"cell_type":"code","source":["class SkipFrame(gym.Wrapper):\n","\n","    def __init__(self, env, skip):\n","        super().__init__(env)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        total_reward = 0.0\n","\n","        for i in range(self._skip):\n","            # Accumulate reward and repeat the same action\n","            # for skipped frames\n","            obs, reward, done, trunk, info = self.env.step(action)\n","            total_reward += reward\n","            if done:\n","                break\n","\n","        return obs, total_reward, done, trunk, info\n","\n","\n","class GrayScaleObservation(gym.ObservationWrapper):\n","\n","    def __init__(self, env):\n","        super().__init__(env)\n","        obs_shape = self.observation_space.shape[:2]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def permute_orientation(self, observation):\n","        # permute [H, W, C] array to [C, H, W] tensor\n","        observation = np.transpose(observation, (2, 0, 1))\n","        observation = torch.tensor(observation.copy(), dtype=torch.float)\n","\n","        return observation\n","\n","    def observation(self, observation):\n","        observation = self.permute_orientation(observation)\n","        transform = transforms.Grayscale()\n","        observation = transform(observation)\n","\n","        return observation\n","\n","\n","class ResizeObservation(gym.ObservationWrapper):\n","\n","    def __init__(self, env, shape):\n","        super().__init__(env)\n","        if isinstance(shape, int):\n","            self.shape = (shape, shape)\n","        else:\n","            self.shape = tuple(shape)\n","\n","        obs_shape = self.shape + self.observation_space.shape[2:]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def observation(self, observation):\n","        t = transforms.Compose(\n","            [transforms.Resize(self.shape), transforms.Normalize(0, 255)]\n","        )\n","        observation = t(observation).squeeze(0)\n","\n","        return observation\n","\n","env = SkipFrame(env, skip=4)\n","env = GrayScaleObservation(env)\n","env = ResizeObservation(env, shape=84)"],"metadata":{"id":"_dWP0onf7JTx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will also use a `FrameStack` wraper to accumulate multiple consecutive frames before giving them to our agent. This will allow it to understand patterns such as direction of movement."],"metadata":{"id":"DQykPDIY8F8U"}},{"cell_type":"code","source":["if gym.__version__ < '0.26':\n","    env = FrameStack(env, num_stack=4, new_step_api=True)\n","else:\n","    env = FrameStack(env, num_stack=4)"],"metadata":{"id":"2koD4ZA08EYJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### The agent\n","\n","We have created an environment to interact with and, now, we need to create an agent that can interact with this environment and learn from it.\n","\n","At the end of this exercise, we will encapsulate the agent in a class but, before we do that, let's introduce some key practical aspects of training Deep RL networks and some of the code that we will use to implement them.\n","\n","#### **Exploration vs exploitation**\n","\n","Our agent is going to learn a model of the Q value function, based on which it will choose optimal actions at any given state. \n","\n","Given a model of the value function, the optimal action should be that which maximises the expected return. However, in order to obtain a representative and complete model of the value function, the agent should also explore the range of possible actions at any given state in order to construct a better model.\n","\n","Therefore, the agent needs to balance between *exploration* ang *exploitation*. When an agent explores, it gets more accurate estimates of action-values. And when it exploits, it might get more reward. It cannot, however, choose to do both simultaneously, which is also called the exploration-exploitation dilemma.\n","\n","We solve this dilemma by using what is called an **$\\epsilon$-greedy policy**, a simple method to balance exploration and exploitation. Under this policy, the agent will choose between exploration and exploitation randomly with a probability $\\epsilon$. Ideally, we want to balance $\\epsilon$ so that the agent explores more at the beginning of our training and gradually exploits more as it progresses."],"metadata":{"id":"dqL0u0kJ9KB1"}},{"cell_type":"code","source":["exploration_rate = 1\n","exploration_rate_decay = 0.99999975\n","exploration_rate_min = 0.1\n","curr_step = 0\n","\n","def act(state):\n","    # randomly choose whether to explore or exploit, given the exploration rate\n","    # EXPLORE\n","    # randomly choose an action\n","\n","    # EXPLOIT\n","    # use the network to predict the Q value for each action\n","    # select the action with the maximum Q value\n","\n","    # decrease exploration_rate\n","\n","    # increment step\n","\n","    return action_idx"],"metadata":{"id":"N8ZsqybrB_2e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### **Experience replay**\n","\n","The success of the original DQN network can be partly attributed to the introduction of experience replay. When the agent performs an action, it will store the results of that action into a memory table. \n","\n","During training, instead of only looking at the current game, the agent will pull a random element from its memory and replay it, learning (possibly multiple times) from it.\n","\n","This makes sure previous experience is not lost and overwritten throughout the training process. It also improves convergence by making the learning examples more `i.i.d.`, which is a key assumption when training neural networks.\n"],"metadata":{"id":"-P050F1KCBNM"}},{"cell_type":"code","source":["memory = deque(maxlen=100000)\n","batch_size = 32\n","\n","def cache(state, next_state, action, reward, done):\n","    def first_if_tuple(x):\n","        return x[0] if isinstance(x, tuple) else x\n","\n","    state = first_if_tuple(state).__array__()\n","    next_state = first_if_tuple(next_state).__array__()\n","\n","    # copy all cached elements into new tensors\n","\n","    # append to the memory\n","\n","def recall():\n","    # randomly sample as many memories as the size of our batch\n","    # stack the memories\n","\n","    return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"],"metadata":{"id":"2Fm0sXdS9CJo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### **DDQN algorithm**\n","\n","The DDQN algorithm is an improvement over the original Atari-playing DQN. \n","\n","Both the original DQN and DDQN use a network to learn a Q function, using the Bellman equation as a basis to formulate a loss. If we recall the Bellman equation:\n","\n","$$Q_\\pi(s, a) = \\mathbb{E}[R_{t+1} + \\gamma Q_\\pi(s_{t+1}, a_{t+1}) | S_t = s, A_t = a, \\pi]$$\n","\n","we can use it to write the following loss function:\n","\n","$$L = || (R_{t+1} + \\gamma \\max_{A_{t+1}} Q(S_{t+1}, A_{t+1})) - Q(S_t, A_t) ||_{p}$$\n","\n","where the left side $R_{t+1} + \\gamma \\max_{A_{t+1}} Q(S_{t+1}, A_{t+1})$ is called the target and the right side $Q(S_t, A_t)$ is called the prediction.\n","\n","Computing this loss, however, poses some problems: the Q function being learned is present both in the target and prediction. This can make the training unstable. The DDQN algorithm solves this by introducing a second Q network, $Q_{target}$, that is only update network with the weights of the primary Q network, $Q_{prediction}$ every n number of timesteps.\n","\n","The loss function then becomes:\n","\n","$$L = || (R_{t+1} + \\gamma \\max_{A_{t+1}} Q_{target}(S_{t+1}, arg\\max_a Q_{prediction}(S_{t+1}, A_{t+1}))) - Q_{prediction}(S_t, A_t) ||_{p}$$"],"metadata":{"id":"48p93ns_CFy4"}},{"cell_type":"code","source":["class MarioNet(nn.Module):\n","\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        c, h, w = input_dim\n","\n","        # create the Q_prediction network\n","\n","        # create the Q_target network\n","\n","        # Q_target parameters are frozen\n","\n","    def forward(self, input, model):\n","        # select network appropriately"],"metadata":{"id":"000lUxYFCGKv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **The loss function**\n","\n","To calculate the loss function, we need to calculate the prediction and target portions of it.\n","\n","The prediction will be given by our estimated optimal Q funcion for a given state $S_t$:\n","\n","$$L_{prediction} = Q_{prediction}(S_t, A_t)$$\n","\n","The target will be obtained by summing the current reward and the estimated function given the next state and action:\n","\n","$$L_{target} = R_{t+1} + \\gamma Q_{target}(S_{t+1}, A_{t+1})$$\n","\n","where $A_{t+1} = arg\\max_{a} Q_{prediction}(S_{t+1}, A_{t+1})$ because the next action is not known at this stage."],"metadata":{"id":"q7joFeqpHlLt"}},{"cell_type":"code","source":["gamma = 0.9\n","\n","def l_prediction(state, action):\n","    # Q_prediction(s,a)\n","\n","    return current_Q\n","\n","# remember, no update should be applied to Q_target,\n","# so we need to deactivate grads\n","@torch.no_grad()\n","def l_target(reward, next_state, done):\n","    # predict the next action that will be taken\n","\n","    # calculate the associated target Q value\n","\n","    # calculate l_target\n"],"metadata":{"id":"fHWN4PJcJa-O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will also need to consider how to update $Q_{prediction}$ and $Q_{target}$:"],"metadata":{"id":"GaV_ENtPJz9X"}},{"cell_type":"code","source":["def update_Q_prediction(l_estimate, l_target):\n","    # calculate the difference betweeen both parts of the loss\n","\n","    # backpropagate\n","\n","    # update params\n","\n","    return loss.item()\n","\n","def sync_Q_target():\n","    # sync weights between two networks\n"],"metadata":{"id":"7h1TqvAjJyRu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's put all of it together! This will be our agent class `Mario`:"],"metadata":{"id":"0hVvZabgopaI"}},{"cell_type":"code","source":["class Mario:\n","\n","    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.memory = deque(maxlen=10000)\n","        self.batch_size = 32\n","\n","        self.exploration_rate = 1\n","        self.exploration_rate_decay = 0.99999975\n","        self.exploration_rate_min = 0.1\n","        self.gamma = 0.9\n","\n","        self.curr_step = 0\n","        self.burnin = 1e2  # min. experiences before training\n","        self.learn_every = 3   # no. of experiences between updates to Q_prediction\n","        self.sync_every = 1e4   # no. of experiences between Q_target & Q_prediction sync\n","\n","        self.save_every = 5e5   # no. of experiences between saving Mario Net\n","        self.save_dir = save_dir\n","\n","        self.use_cuda = torch.cuda.is_available()\n","\n","        # Mario's DNN to predict the most optimal action\n","        self.net = MarioNet(self.state_dim, self.action_dim).float()\n","        self.net = self.net.to(device=device)\n","\n","        if checkpoint:\n","            self.load(checkpoint)\n","\n","        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)  # use Adam optimiser\n","        self.loss_fn = torch.nn.SmoothL1Loss()  # use L1 loss\n","\n","    def act(self, state):\n","        # EXPLORE\n","        if np.random.rand() < self.exploration_rate:\n","            action_idx = np.random.randint(self.action_dim)\n","\n","        # EXPLOIT\n","        else:\n","            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n","            state = torch.tensor(state, device=device).unsqueeze(0)\n","            action_values = self.net(state, model='prediction')\n","            action_idx = torch.argmax(action_values, axis=1).item()\n","\n","        # decrease exploration_rate\n","        self.exploration_rate *= self.exploration_rate_decay\n","        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n","\n","        # increment step\n","        self.curr_step += 1\n","        return action_idx\n","\n","    def cache(self, state, next_state, action, reward, done):\n","        def first_if_tuple(x):\n","            return x[0] if isinstance(x, tuple) else x\n","\n","        state = first_if_tuple(state).__array__()\n","        next_state = first_if_tuple(next_state).__array__()\n","\n","        state = torch.tensor(state, device=device)\n","        next_state = torch.tensor(next_state, device=device)\n","        action = torch.tensor([action], device=device)\n","        reward = torch.tensor([reward], device=device)\n","        done = torch.tensor([done], device=device)\n","\n","        self.memory.append( (state, next_state, action, reward, done,) )\n","\n","    def recall(self):\n","        batch = random.sample(self.memory, self.batch_size)\n","        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n","\n","        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n","\n","    def l_estimate(self, state, action):\n","        current_Q = self.net(state, model='prediction')[np.arange(0, self.batch_size), action] # Q_prediction(s,a)\n","\n","        return current_Q\n","\n","    @torch.no_grad()\n","    def l_target(self, reward, next_state, done):\n","        next_state_Q = self.net(next_state, model='prediction')\n","        best_action = torch.argmax(next_state_Q, axis=1)\n","        next_Q = self.net(next_state, model='target')[np.arange(0, self.batch_size), best_action]\n","\n","        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n","\n","    def update_Q_prediction(self, l_estimate, l_target) :\n","        loss = self.loss_fn(l_estimate, l_target)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","\n","        self.optimizer.step()\n","\n","        return loss.item()\n","\n","    def sync_Q_target(self):\n","        self.net.target.load_state_dict(self.net.prediction.state_dict())\n","\n","    def learn(self):\n","        # sync networks if needed\n","        if self.curr_step % self.sync_every == 0:\n","            self.sync_Q_target()\n","\n","        # save checkpoint if needed\n","        if self.curr_step % self.save_every == 0:\n","            self.save()\n","\n","        # only start learning when we have enough initial experiences stored\n","        if self.curr_step < self.burnin:\n","            return None, None\n","\n","        # only learn every n steps\n","        if self.curr_step % self.learn_every != 0:\n","            return None, None\n","\n","        # Sample from memory\n","\n","        # Get L Estimate\n","\n","        # Get L Target\n","\n","        # Backpropagate loss through Q_prediction\n","\n","\n","    def save(self):\n","        save_path = self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n","        torch.save(\n","            dict(\n","                model=self.net.state_dict(),\n","                exploration_rate=self.exploration_rate\n","            ),\n","            save_path\n","        )\n","        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n","\n","    def load(self, load_path):\n","        if not load_path.exists():\n","            raise ValueError(f\"{load_path} does not exist\")\n","\n","        ckp = torch.load(load_path, map_location=('cuda' if self.use_cuda else 'cpu'))\n","        exploration_rate = ckp.get('exploration_rate')\n","        state_dict = ckp.get('model')\n","\n","        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n","        self.net.load_state_dict(state_dict)\n","        self.exploration_rate = exploration_rate"],"metadata":{"id":"vMmCGn47p-ou"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training\n","\n","Before we train, let's create a utility function to log the results of our training."],"metadata":{"id":"rD07RQVAp_Gr"}},{"cell_type":"code","source":["class MetricLogger:\n","\n","    def __init__(self, save_dir):\n","        self.save_log = save_dir / \"log\"\n","        with open(self.save_log, \"w\") as f:\n","            f.write(\n","                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n","                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n","                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n","            )\n","        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n","        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n","        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n","        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n","\n","        # History metrics\n","        self.ep_rewards = []\n","        self.ep_lengths = []\n","        self.ep_avg_losses = []\n","        self.ep_avg_qs = []\n","\n","        # Moving averages, added for every call to record()\n","        self.moving_avg_ep_rewards = []\n","        self.moving_avg_ep_lengths = []\n","        self.moving_avg_ep_avg_losses = []\n","        self.moving_avg_ep_avg_qs = []\n","\n","        # Current episode metric\n","        self.init_episode()\n","\n","        # Timing\n","        self.record_time = time.time()\n","\n","    def log_step(self, reward, loss, q):\n","        self.curr_ep_reward += reward\n","        self.curr_ep_length += 1\n","        if loss:\n","            self.curr_ep_loss += loss\n","            self.curr_ep_q += q\n","            self.curr_ep_loss_length += 1\n","\n","    def log_episode(self):\n","        \"Mark end of episode\"\n","        self.ep_rewards.append(self.curr_ep_reward)\n","        self.ep_lengths.append(self.curr_ep_length)\n","        if self.curr_ep_loss_length == 0:\n","            ep_avg_loss = 0\n","            ep_avg_q = 0\n","        else:\n","            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n","            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n","        self.ep_avg_losses.append(ep_avg_loss)\n","        self.ep_avg_qs.append(ep_avg_q)\n","\n","        self.init_episode()\n","\n","    def init_episode(self):\n","        self.curr_ep_reward = 0.0\n","        self.curr_ep_length = 0\n","        self.curr_ep_loss = 0.0\n","        self.curr_ep_q = 0.0\n","        self.curr_ep_loss_length = 0\n","\n","    def record(self, episode, epsilon, step):\n","        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n","        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n","        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n","        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n","        self.moving_avg_ep_rewards.append(mean_ep_reward)\n","        self.moving_avg_ep_lengths.append(mean_ep_length)\n","        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n","        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n","\n","        last_record_time = self.record_time\n","        self.record_time = time.time()\n","        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n","\n","        print(\n","            f\"Episode {episode} - \"\n","            f\"Step {step} - \"\n","            f\"Epsilon {epsilon} - \"\n","            f\"Mean Reward {mean_ep_reward} - \"\n","            f\"Mean Length {mean_ep_length} - \"\n","            f\"Mean Loss {mean_ep_loss} - \"\n","            f\"Mean Q Value {mean_ep_q} - \"\n","            f\"Time Delta {time_since_last_record} - \"\n","            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n","        )\n","\n","        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n","            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n","            plt.savefig(getattr(self, f\"{metric}_plot\"))\n","            plt.clf()"],"metadata":{"id":"nPiZ4XTWosPj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And let's see how our agent performs after running for 10 episodes:\n"],"metadata":{"id":"9MjhqznSqAby"}},{"cell_type":"code","source":["save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n","save_dir.mkdir(parents=True)\n","\n","mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n","\n","logger = MetricLogger(save_dir)\n","\n","episodes = 20\n","for e in range(episodes):\n","    state = env.reset()\n","\n","    # Play the game!\n","    while True:\n","        # Run agent on the state\n","\n","        # Agent performs action\n","\n","        # Remember\n","\n","        # Learn\n","\n","        # Logging\n","\n","        # Update state\n","\n","        # Check if end of game\n","        if done or info[\"flag_get\"]:\n","            break\n","\n","    logger.log_episode()\n","    logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"id":"547_jSUKqAvR","executionInfo":{"status":"ok","timestamp":1670867279885,"user_tz":0,"elapsed":80706,"user":{"displayName":"Carlos Cueto Mondéjar","userId":"10605816413378977833"}},"outputId":"d1c6279a-51ff-4e2e-e3ad-d42e77d68b96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 0 - Step 40 - Epsilon 0.9999900000487484 - Mean Reward 231.0 - Mean Length 40.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.349 - Time 2022-12-12T17:46:40\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["/* Put everything inside the global mpl namespace */\n","window.mpl = {};\n","\n","\n","mpl.get_websocket_type = function() {\n","    if (typeof(WebSocket) !== 'undefined') {\n","        return WebSocket;\n","    } else if (typeof(MozWebSocket) !== 'undefined') {\n","        return MozWebSocket;\n","    } else {\n","        alert('Your browser does not have WebSocket support. ' +\n","              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n","              'Firefox 4 and 5 are also supported but you ' +\n","              'have to enable WebSockets in about:config.');\n","    };\n","}\n","\n","mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n","    this.id = figure_id;\n","\n","    this.ws = websocket;\n","\n","    this.supports_binary = (this.ws.binaryType != undefined);\n","\n","    if (!this.supports_binary) {\n","        var warnings = document.getElementById(\"mpl-warnings\");\n","        if (warnings) {\n","            warnings.style.display = 'block';\n","            warnings.textContent = (\n","                \"This browser does not support binary websocket messages. \" +\n","                    \"Performance may be slow.\");\n","        }\n","    }\n","\n","    this.imageObj = new Image();\n","\n","    this.context = undefined;\n","    this.message = undefined;\n","    this.canvas = undefined;\n","    this.rubberband_canvas = undefined;\n","    this.rubberband_context = undefined;\n","    this.format_dropdown = undefined;\n","\n","    this.image_mode = 'full';\n","\n","    this.root = $('<div/>');\n","    this._root_extra_style(this.root)\n","    this.root.attr('style', 'display: inline-block');\n","\n","    $(parent_element).append(this.root);\n","\n","    this._init_header(this);\n","    this._init_canvas(this);\n","    this._init_toolbar(this);\n","\n","    var fig = this;\n","\n","    this.waiting = false;\n","\n","    this.ws.onopen =  function () {\n","            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n","            fig.send_message(\"send_image_mode\", {});\n","            if (mpl.ratio != 1) {\n","                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n","            }\n","            fig.send_message(\"refresh\", {});\n","        }\n","\n","    this.imageObj.onload = function() {\n","            if (fig.image_mode == 'full') {\n","                // Full images could contain transparency (where diff images\n","                // almost always do), so we need to clear the canvas so that\n","                // there is no ghosting.\n","                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n","            }\n","            fig.context.drawImage(fig.imageObj, 0, 0);\n","        };\n","\n","    this.imageObj.onunload = function() {\n","        fig.ws.close();\n","    }\n","\n","    this.ws.onmessage = this._make_on_message_function(this);\n","\n","    this.ondownload = ondownload;\n","}\n","\n","mpl.figure.prototype._init_header = function() {\n","    var titlebar = $(\n","        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n","        'ui-helper-clearfix\"/>');\n","    var titletext = $(\n","        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n","        'text-align: center; padding: 3px;\"/>');\n","    titlebar.append(titletext)\n","    this.root.append(titlebar);\n","    this.header = titletext[0];\n","}\n","\n","\n","\n","mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n","\n","}\n","\n","\n","mpl.figure.prototype._root_extra_style = function(canvas_div) {\n","\n","}\n","\n","mpl.figure.prototype._init_canvas = function() {\n","    var fig = this;\n","\n","    var canvas_div = $('<div/>');\n","\n","    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n","\n","    function canvas_keyboard_event(event) {\n","        return fig.key_event(event, event['data']);\n","    }\n","\n","    canvas_div.keydown('key_press', canvas_keyboard_event);\n","    canvas_div.keyup('key_release', canvas_keyboard_event);\n","    this.canvas_div = canvas_div\n","    this._canvas_extra_style(canvas_div)\n","    this.root.append(canvas_div);\n","\n","    var canvas = $('<canvas/>');\n","    canvas.addClass('mpl-canvas');\n","    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n","\n","    this.canvas = canvas[0];\n","    this.context = canvas[0].getContext(\"2d\");\n","\n","    var backingStore = this.context.backingStorePixelRatio ||\n","\tthis.context.webkitBackingStorePixelRatio ||\n","\tthis.context.mozBackingStorePixelRatio ||\n","\tthis.context.msBackingStorePixelRatio ||\n","\tthis.context.oBackingStorePixelRatio ||\n","\tthis.context.backingStorePixelRatio || 1;\n","\n","    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n","\n","    var rubberband = $('<canvas/>');\n","    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n","\n","    var pass_mouse_events = true;\n","\n","    canvas_div.resizable({\n","        start: function(event, ui) {\n","            pass_mouse_events = false;\n","        },\n","        resize: function(event, ui) {\n","            fig.request_resize(ui.size.width, ui.size.height);\n","        },\n","        stop: function(event, ui) {\n","            pass_mouse_events = true;\n","            fig.request_resize(ui.size.width, ui.size.height);\n","        },\n","    });\n","\n","    function mouse_event_fn(event) {\n","        if (pass_mouse_events)\n","            return fig.mouse_event(event, event['data']);\n","    }\n","\n","    rubberband.mousedown('button_press', mouse_event_fn);\n","    rubberband.mouseup('button_release', mouse_event_fn);\n","    // Throttle sequential mouse events to 1 every 20ms.\n","    rubberband.mousemove('motion_notify', mouse_event_fn);\n","\n","    rubberband.mouseenter('figure_enter', mouse_event_fn);\n","    rubberband.mouseleave('figure_leave', mouse_event_fn);\n","\n","    canvas_div.on(\"wheel\", function (event) {\n","        event = event.originalEvent;\n","        event['data'] = 'scroll'\n","        if (event.deltaY < 0) {\n","            event.step = 1;\n","        } else {\n","            event.step = -1;\n","        }\n","        mouse_event_fn(event);\n","    });\n","\n","    canvas_div.append(canvas);\n","    canvas_div.append(rubberband);\n","\n","    this.rubberband = rubberband;\n","    this.rubberband_canvas = rubberband[0];\n","    this.rubberband_context = rubberband[0].getContext(\"2d\");\n","    this.rubberband_context.strokeStyle = \"#000000\";\n","\n","    this._resize_canvas = function(width, height) {\n","        // Keep the size of the canvas, canvas container, and rubber band\n","        // canvas in synch.\n","        canvas_div.css('width', width)\n","        canvas_div.css('height', height)\n","\n","        canvas.attr('width', width * mpl.ratio);\n","        canvas.attr('height', height * mpl.ratio);\n","        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n","\n","        rubberband.attr('width', width);\n","        rubberband.attr('height', height);\n","    }\n","\n","    // Set the figure to an initial 600x600px, this will subsequently be updated\n","    // upon first draw.\n","    this._resize_canvas(600, 600);\n","\n","    // Disable right mouse context menu.\n","    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n","        return false;\n","    });\n","\n","    function set_focus () {\n","        canvas.focus();\n","        canvas_div.focus();\n","    }\n","\n","    window.setTimeout(set_focus, 100);\n","}\n","\n","mpl.figure.prototype._init_toolbar = function() {\n","    var fig = this;\n","\n","    var nav_element = $('<div/>');\n","    nav_element.attr('style', 'width: 100%');\n","    this.root.append(nav_element);\n","\n","    // Define a callback function for later on.\n","    function toolbar_event(event) {\n","        return fig.toolbar_button_onclick(event['data']);\n","    }\n","    function toolbar_mouse_event(event) {\n","        return fig.toolbar_button_onmouseover(event['data']);\n","    }\n","\n","    for(var toolbar_ind in mpl.toolbar_items) {\n","        var name = mpl.toolbar_items[toolbar_ind][0];\n","        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n","        var image = mpl.toolbar_items[toolbar_ind][2];\n","        var method_name = mpl.toolbar_items[toolbar_ind][3];\n","\n","        if (!name) {\n","            // put a spacer in here.\n","            continue;\n","        }\n","        var button = $('<button/>');\n","        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n","                        'ui-button-icon-only');\n","        button.attr('role', 'button');\n","        button.attr('aria-disabled', 'false');\n","        button.click(method_name, toolbar_event);\n","        button.mouseover(tooltip, toolbar_mouse_event);\n","\n","        var icon_img = $('<span/>');\n","        icon_img.addClass('ui-button-icon-primary ui-icon');\n","        icon_img.addClass(image);\n","        icon_img.addClass('ui-corner-all');\n","\n","        var tooltip_span = $('<span/>');\n","        tooltip_span.addClass('ui-button-text');\n","        tooltip_span.html(tooltip);\n","\n","        button.append(icon_img);\n","        button.append(tooltip_span);\n","\n","        nav_element.append(button);\n","    }\n","\n","    var fmt_picker_span = $('<span/>');\n","\n","    var fmt_picker = $('<select/>');\n","    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n","    fmt_picker_span.append(fmt_picker);\n","    nav_element.append(fmt_picker_span);\n","    this.format_dropdown = fmt_picker[0];\n","\n","    for (var ind in mpl.extensions) {\n","        var fmt = mpl.extensions[ind];\n","        var option = $(\n","            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n","        fmt_picker.append(option);\n","    }\n","\n","    // Add hover states to the ui-buttons\n","    $( \".ui-button\" ).hover(\n","        function() { $(this).addClass(\"ui-state-hover\");},\n","        function() { $(this).removeClass(\"ui-state-hover\");}\n","    );\n","\n","    var status_bar = $('<span class=\"mpl-message\"/>');\n","    nav_element.append(status_bar);\n","    this.message = status_bar[0];\n","}\n","\n","mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n","    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n","    // which will in turn request a refresh of the image.\n","    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n","}\n","\n","mpl.figure.prototype.send_message = function(type, properties) {\n","    properties['type'] = type;\n","    properties['figure_id'] = this.id;\n","    this.ws.send(JSON.stringify(properties));\n","}\n","\n","mpl.figure.prototype.send_draw_message = function() {\n","    if (!this.waiting) {\n","        this.waiting = true;\n","        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n","    }\n","}\n","\n","\n","mpl.figure.prototype.handle_save = function(fig, msg) {\n","    var format_dropdown = fig.format_dropdown;\n","    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n","    fig.ondownload(fig, format);\n","}\n","\n","\n","mpl.figure.prototype.handle_resize = function(fig, msg) {\n","    var size = msg['size'];\n","    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n","        fig._resize_canvas(size[0], size[1]);\n","        fig.send_message(\"refresh\", {});\n","    };\n","}\n","\n","mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n","    var x0 = msg['x0'] / mpl.ratio;\n","    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n","    var x1 = msg['x1'] / mpl.ratio;\n","    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n","    x0 = Math.floor(x0) + 0.5;\n","    y0 = Math.floor(y0) + 0.5;\n","    x1 = Math.floor(x1) + 0.5;\n","    y1 = Math.floor(y1) + 0.5;\n","    var min_x = Math.min(x0, x1);\n","    var min_y = Math.min(y0, y1);\n","    var width = Math.abs(x1 - x0);\n","    var height = Math.abs(y1 - y0);\n","\n","    fig.rubberband_context.clearRect(\n","        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n","\n","    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n","}\n","\n","mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n","    // Updates the figure title.\n","    fig.header.textContent = msg['label'];\n","}\n","\n","mpl.figure.prototype.handle_cursor = function(fig, msg) {\n","    var cursor = msg['cursor'];\n","    switch(cursor)\n","    {\n","    case 0:\n","        cursor = 'pointer';\n","        break;\n","    case 1:\n","        cursor = 'default';\n","        break;\n","    case 2:\n","        cursor = 'crosshair';\n","        break;\n","    case 3:\n","        cursor = 'move';\n","        break;\n","    }\n","    fig.rubberband_canvas.style.cursor = cursor;\n","}\n","\n","mpl.figure.prototype.handle_message = function(fig, msg) {\n","    fig.message.textContent = msg['message'];\n","}\n","\n","mpl.figure.prototype.handle_draw = function(fig, msg) {\n","    // Request the server to send over a new figure.\n","    fig.send_draw_message();\n","}\n","\n","mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n","    fig.image_mode = msg['mode'];\n","}\n","\n","mpl.figure.prototype.updated_canvas_event = function() {\n","    // Called whenever the canvas gets updated.\n","    this.send_message(\"ack\", {});\n","}\n","\n","// A function to construct a web socket function for onmessage handling.\n","// Called in the figure constructor.\n","mpl.figure.prototype._make_on_message_function = function(fig) {\n","    return function socket_on_message(evt) {\n","        if (evt.data instanceof Blob) {\n","            /* FIXME: We get \"Resource interpreted as Image but\n","             * transferred with MIME type text/plain:\" errors on\n","             * Chrome.  But how to set the MIME type?  It doesn't seem\n","             * to be part of the websocket stream */\n","            evt.data.type = \"image/png\";\n","\n","            /* Free the memory for the previous frames */\n","            if (fig.imageObj.src) {\n","                (window.URL || window.webkitURL).revokeObjectURL(\n","                    fig.imageObj.src);\n","            }\n","\n","            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n","                evt.data);\n","            fig.updated_canvas_event();\n","            fig.waiting = false;\n","            return;\n","        }\n","        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n","            fig.imageObj.src = evt.data;\n","            fig.updated_canvas_event();\n","            fig.waiting = false;\n","            return;\n","        }\n","\n","        var msg = JSON.parse(evt.data);\n","        var msg_type = msg['type'];\n","\n","        // Call the  \"handle_{type}\" callback, which takes\n","        // the figure and JSON message as its only arguments.\n","        try {\n","            var callback = fig[\"handle_\" + msg_type];\n","        } catch (e) {\n","            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n","            return;\n","        }\n","\n","        if (callback) {\n","            try {\n","                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n","                callback(fig, msg);\n","            } catch (e) {\n","                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n","            }\n","        }\n","    };\n","}\n","\n","// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n","mpl.findpos = function(e) {\n","    //this section is from http://www.quirksmode.org/js/events_properties.html\n","    var targ;\n","    if (!e)\n","        e = window.event;\n","    if (e.target)\n","        targ = e.target;\n","    else if (e.srcElement)\n","        targ = e.srcElement;\n","    if (targ.nodeType == 3) // defeat Safari bug\n","        targ = targ.parentNode;\n","\n","    // jQuery normalizes the pageX and pageY\n","    // pageX,Y are the mouse positions relative to the document\n","    // offset() returns the position of the element relative to the document\n","    var x = e.pageX - $(targ).offset().left;\n","    var y = e.pageY - $(targ).offset().top;\n","\n","    return {\"x\": x, \"y\": y};\n","};\n","\n","/*\n"," * return a copy of an object with only non-object keys\n"," * we need this to avoid circular references\n"," * http://stackoverflow.com/a/24161582/3208463\n"," */\n","function simpleKeys (original) {\n","  return Object.keys(original).reduce(function (obj, key) {\n","    if (typeof original[key] !== 'object')\n","        obj[key] = original[key]\n","    return obj;\n","  }, {});\n","}\n","\n","mpl.figure.prototype.mouse_event = function(event, name) {\n","    var canvas_pos = mpl.findpos(event)\n","\n","    if (name === 'button_press')\n","    {\n","        this.canvas.focus();\n","        this.canvas_div.focus();\n","    }\n","\n","    var x = canvas_pos.x * mpl.ratio;\n","    var y = canvas_pos.y * mpl.ratio;\n","\n","    this.send_message(name, {x: x, y: y, button: event.button,\n","                             step: event.step,\n","                             guiEvent: simpleKeys(event)});\n","\n","    /* This prevents the web browser from automatically changing to\n","     * the text insertion cursor when the button is pressed.  We want\n","     * to control all of the cursor setting manually through the\n","     * 'cursor' event from matplotlib */\n","    event.preventDefault();\n","    return false;\n","}\n","\n","mpl.figure.prototype._key_event_extra = function(event, name) {\n","    // Handle any extra behaviour associated with a key event\n","}\n","\n","mpl.figure.prototype.key_event = function(event, name) {\n","\n","    // Prevent repeat events\n","    if (name == 'key_press')\n","    {\n","        if (event.which === this._key)\n","            return;\n","        else\n","            this._key = event.which;\n","    }\n","    if (name == 'key_release')\n","        this._key = null;\n","\n","    var value = '';\n","    if (event.ctrlKey && event.which != 17)\n","        value += \"ctrl+\";\n","    if (event.altKey && event.which != 18)\n","        value += \"alt+\";\n","    if (event.shiftKey && event.which != 16)\n","        value += \"shift+\";\n","\n","    value += 'k';\n","    value += event.which.toString();\n","\n","    this._key_event_extra(event, name);\n","\n","    this.send_message(name, {key: value,\n","                             guiEvent: simpleKeys(event)});\n","    return false;\n","}\n","\n","mpl.figure.prototype.toolbar_button_onclick = function(name) {\n","    if (name == 'download') {\n","        this.handle_save(this, null);\n","    } else {\n","        this.send_message(\"toolbar_button\", {name: name});\n","    }\n","};\n","\n","mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n","    this.message.textContent = tooltip;\n","};\n","mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n","\n","mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n","\n","mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n","    // Create a \"websocket\"-like object which calls the given IPython comm\n","    // object with the appropriate methods. Currently this is a non binary\n","    // socket, so there is still some room for performance tuning.\n","    var ws = {};\n","\n","    ws.close = function() {\n","        comm.close()\n","    };\n","    ws.send = function(m) {\n","        //console.log('sending', m);\n","        comm.send(m);\n","    };\n","    // Register the callback with on_msg.\n","    comm.on_msg(function(msg) {\n","        //console.log('receiving', msg['content']['data'], msg);\n","        // Pass the mpl event to the overridden (by mpl) onmessage function.\n","        ws.onmessage(msg['content']['data'])\n","    });\n","    return ws;\n","}\n","\n","mpl.mpl_figure_comm = function(comm, msg) {\n","    // This is the function which gets called when the mpl process\n","    // starts-up an IPython Comm through the \"matplotlib\" channel.\n","\n","    var id = msg.content.data.id;\n","    // Get hold of the div created by the display call when the Comm\n","    // socket was opened in Python.\n","    var element = $(\"#\" + id);\n","    var ws_proxy = comm_websocket_adapter(comm)\n","\n","    function ondownload(figure, format) {\n","        window.open(figure.imageObj.src);\n","    }\n","\n","    var fig = new mpl.figure(id, ws_proxy,\n","                           ondownload,\n","                           element.get(0));\n","\n","    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n","    // web socket which is closed, not our websocket->open comm proxy.\n","    ws_proxy.onopen();\n","\n","    fig.parent_element = element.get(0);\n","    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n","    if (!fig.cell_info) {\n","        console.error(\"Failed to find cell for figure\", id, fig);\n","        return;\n","    }\n","\n","    var output_index = fig.cell_info[2]\n","    var cell = fig.cell_info[0];\n","\n","};\n","\n","mpl.figure.prototype.handle_close = function(fig, msg) {\n","    var width = fig.canvas.width/mpl.ratio\n","    fig.root.unbind('remove')\n","\n","    // Update the output cell to use the data from the current canvas.\n","    fig.push_to_output();\n","    var dataURL = fig.canvas.toDataURL();\n","    // Re-enable the keyboard manager in IPython - without this line, in FF,\n","    // the notebook keyboard shortcuts fail.\n","    IPython.keyboard_manager.enable()\n","    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n","    fig.close_ws(fig, msg);\n","}\n","\n","mpl.figure.prototype.close_ws = function(fig, msg){\n","    fig.send_message('closing', msg);\n","    // fig.ws.close()\n","}\n","\n","mpl.figure.prototype.push_to_output = function(remove_interactive) {\n","    // Turn the data on the canvas into data in the output cell.\n","    var width = this.canvas.width/mpl.ratio\n","    var dataURL = this.canvas.toDataURL();\n","    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n","}\n","\n","mpl.figure.prototype.updated_canvas_event = function() {\n","    // Tell IPython that the notebook contents must change.\n","    IPython.notebook.set_dirty(true);\n","    this.send_message(\"ack\", {});\n","    var fig = this;\n","    // Wait a second, then push the new image to the DOM so\n","    // that it is saved nicely (might be nice to debounce this).\n","    setTimeout(function () { fig.push_to_output() }, 1000);\n","}\n","\n","mpl.figure.prototype._init_toolbar = function() {\n","    var fig = this;\n","\n","    var nav_element = $('<div/>');\n","    nav_element.attr('style', 'width: 100%');\n","    this.root.append(nav_element);\n","\n","    // Define a callback function for later on.\n","    function toolbar_event(event) {\n","        return fig.toolbar_button_onclick(event['data']);\n","    }\n","    function toolbar_mouse_event(event) {\n","        return fig.toolbar_button_onmouseover(event['data']);\n","    }\n","\n","    for(var toolbar_ind in mpl.toolbar_items){\n","        var name = mpl.toolbar_items[toolbar_ind][0];\n","        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n","        var image = mpl.toolbar_items[toolbar_ind][2];\n","        var method_name = mpl.toolbar_items[toolbar_ind][3];\n","\n","        if (!name) { continue; };\n","\n","        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n","        button.click(method_name, toolbar_event);\n","        button.mouseover(tooltip, toolbar_mouse_event);\n","        nav_element.append(button);\n","    }\n","\n","    // Add the status bar.\n","    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n","    nav_element.append(status_bar);\n","    this.message = status_bar[0];\n","\n","    // Add the close button to the window.\n","    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n","    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n","    button.click(function (evt) { fig.handle_close(fig, {}); } );\n","    button.mouseover('Stop Interaction', toolbar_mouse_event);\n","    buttongrp.append(button);\n","    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n","    titlebar.prepend(buttongrp);\n","}\n","\n","mpl.figure.prototype._root_extra_style = function(el){\n","    var fig = this\n","    el.on(\"remove\", function(){\n","\tfig.close_ws(fig, {});\n","    });\n","}\n","\n","mpl.figure.prototype._canvas_extra_style = function(el){\n","    // this is important to make the div 'focusable\n","    el.attr('tabindex', 0)\n","    // reach out to IPython and tell the keyboard manager to turn it's self\n","    // off when our div gets focus\n","\n","    // location in version 3\n","    if (IPython.notebook.keyboard_manager) {\n","        IPython.notebook.keyboard_manager.register_events(el);\n","    }\n","    else {\n","        // location in version 2\n","        IPython.keyboard_manager.register_events(el);\n","    }\n","\n","}\n","\n","mpl.figure.prototype._key_event_extra = function(event, name) {\n","    var manager = IPython.notebook.keyboard_manager;\n","    if (!manager)\n","        manager = IPython.keyboard_manager;\n","\n","    // Check for shift+enter\n","    if (event.shiftKey && event.which == 13) {\n","        this.canvas_div.blur();\n","        // select the cell after this one\n","        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n","        IPython.notebook.select(index + 1);\n","    }\n","}\n","\n","mpl.figure.prototype.handle_save = function(fig, msg) {\n","    fig.ondownload(fig, null);\n","}\n","\n","\n","mpl.find_output_cell = function(html_output) {\n","    // Return the cell and output element which can be found *uniquely* in the notebook.\n","    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n","    // IPython event is triggered only after the cells have been serialised, which for\n","    // our purposes (turning an active figure into a static one), is too late.\n","    var cells = IPython.notebook.get_cells();\n","    var ncells = cells.length;\n","    for (var i=0; i<ncells; i++) {\n","        var cell = cells[i];\n","        if (cell.cell_type === 'code'){\n","            for (var j=0; j<cell.output_area.outputs.length; j++) {\n","                var data = cell.output_area.outputs[j];\n","                if (data.data) {\n","                    // IPython >= 3 moved mimebundle to data attribute of output\n","                    data = data.data;\n","                }\n","                if (data['text/html'] == html_output) {\n","                    return [cell, data, j];\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","// Register the function which deals with the matplotlib target/channel.\n","// The kernel may be null if the page has been refreshed.\n","if (IPython.notebook.kernel != null) {\n","    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n","}\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div id='1a7a2eba-bbaf-438e-bbb1-a9123b12a3ac'></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Episode 1 - Step 269 - Epsilon 0.9999327522528155 - Mean Reward 631.0 - Mean Length 134.5 - Mean Loss 1.176 - Mean Q Value 2.682 - Time Delta 4.719 - Time 2022-12-12T17:46:45\n","Episode 2 - Step 1359 - Epsilon 0.9996603076659931 - Mean Reward 618.0 - Mean Length 453.0 - Mean Loss 1.362 - Mean Q Value 2.648 - Time Delta 20.705 - Time 2022-12-12T17:47:05\n","Episode 3 - Step 1623 - Epsilon 0.9995943322546434 - Mean Reward 717.75 - Mean Length 405.75 - Mean Loss 1.172 - Mean Q Value 2.467 - Time Delta 3.18 - Time 2022-12-12T17:47:09\n","Episode 4 - Step 2109 - Epsilon 0.9994728889059106 - Mean Reward 771.2 - Mean Length 421.8 - Mean Loss 1.041 - Mean Q Value 2.355 - Time Delta 5.619 - Time 2022-12-12T17:47:14\n","Episode 5 - Step 2423 - Epsilon 0.999394433353734 - Mean Reward 859.167 - Mean Length 403.833 - Mean Loss 0.961 - Mean Q Value 2.289 - Time Delta 3.702 - Time 2022-12-12T17:47:18\n","Episode 6 - Step 2463 - Epsilon 0.9993844394581194 - Mean Reward 769.429 - Mean Length 351.857 - Mean Loss 0.914 - Mean Q Value 2.295 - Time Delta 0.666 - Time 2022-12-12T17:47:19\n","Episode 7 - Step 3004 - Epsilon 0.9992492818360093 - Mean Reward 828.125 - Mean Length 375.5 - Mean Loss 0.862 - Mean Q Value 2.288 - Time Delta 6.098 - Time 2022-12-12T17:47:25\n","Episode 8 - Step 3213 - Epsilon 0.9991970724184832 - Mean Reward 825.222 - Mean Length 357.0 - Mean Loss 0.816 - Mean Q Value 2.288 - Time Delta 2.57 - Time 2022-12-12T17:47:27\n","Episode 9 - Step 3256 - Epsilon 0.9991863311063457 - Mean Reward 765.3 - Mean Length 325.6 - Mean Loss 0.77 - Mean Q Value 2.284 - Time Delta 0.812 - Time 2022-12-12T17:47:28\n","Episode 10 - Step 3300 - Epsilon 0.9991753401157786 - Mean Reward 716.0 - Mean Length 300.0 - Mean Loss 0.735 - Mean Q Value 2.278 - Time Delta 0.7 - Time 2022-12-12T17:47:29\n","Episode 11 - Step 3457 - Epsilon 0.9991361232484073 - Mean Reward 709.333 - Mean Length 288.083 - Mean Loss 0.712 - Mean Q Value 2.302 - Time Delta 1.967 - Time 2022-12-12T17:47:31\n","Episode 12 - Step 3569 - Epsilon 0.9991081478251134 - Mean Reward 702.154 - Mean Length 274.538 - Mean Loss 0.69 - Mean Q Value 2.318 - Time Delta 1.475 - Time 2022-12-12T17:47:32\n","Episode 13 - Step 4035 - Epsilon 0.9989917584911373 - Mean Reward 722.643 - Mean Length 288.214 - Mean Loss 0.67 - Mean Q Value 2.35 - Time Delta 5.309 - Time 2022-12-12T17:47:38\n","Episode 14 - Step 4340 - Epsilon 0.9989155882640469 - Mean Reward 726.6 - Mean Length 289.333 - Mean Loss 0.652 - Mean Q Value 2.373 - Time Delta 3.539 - Time 2022-12-12T17:47:41\n","Episode 15 - Step 4777 - Epsilon 0.998806462683467 - Mean Reward 743.0 - Mean Length 298.562 - Mean Loss 0.635 - Mean Q Value 2.391 - Time Delta 5.976 - Time 2022-12-12T17:47:47\n","Episode 16 - Step 4879 - Epsilon 0.9987809934402153 - Mean Reward 735.647 - Mean Length 287.0 - Mean Loss 0.619 - Mean Q Value 2.404 - Time Delta 1.712 - Time 2022-12-12T17:47:49\n","Episode 17 - Step 5032 - Epsilon 0.9987427907930659 - Mean Reward 736.611 - Mean Length 279.556 - Mean Loss 0.605 - Mean Q Value 2.424 - Time Delta 1.948 - Time 2022-12-12T17:47:51\n","Episode 18 - Step 5359 - Epsilon 0.9986611468969416 - Mean Reward 734.789 - Mean Length 282.053 - Mean Loss 0.594 - Mean Q Value 2.438 - Time Delta 3.815 - Time 2022-12-12T17:47:55\n","Episode 19 - Step 5650 - Epsilon 0.998588496932088 - Mean Reward 763.3 - Mean Length 282.5 - Mean Loss 0.583 - Mean Q Value 2.449 - Time Delta 3.463 - Time 2022-12-12T17:47:58\n"]}]},{"cell_type":"markdown","source":["### Replay\n","\n","Let's test our model's ability to play Mario:"],"metadata":{"id":"PTt6079I4Hhk"}},{"cell_type":"code","source":["%matplotlib inline\n","\n","# make sure there is no exploration when replaying\n","\n","episodes = 100\n","\n","fig,ax = plt.subplots(1,1)\n","hdisplay = display(\"\", display_id=True)\n","fig, ax = plt.subplots(1, 1)\n","\n","def plot(_env, _img):\n","    _img.set_data(_env.render(mode='rgb_array'))\n","    time.sleep(1e-3)\n","    hdisplay.update(fig)\n","\n","for e in range(episodes):\n","\n","    state = env.reset()\n","    img = ax.imshow(env.render(mode='rgb_array'))\n","    plt.title('episode - %d' % e)\n","\n","    while True:\n","        plot(env, img)\n","\n","        action = mario.act(state)\n","\n","        next_state, reward, done, trunc, info = env.step(action)\n","        state = next_state\n","\n","        if done or info['flag_get']:\n","            break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"_Gqmnqku4HwM","outputId":"ad90ff54-c6b6-48d2-8d6d-fc873e4bd366"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["## Further resources\n","\n","These are some very good courses on RL that touch on Deep RL as well:\n","\n","- [Stanford's RL course](http://web.stanford.edu/class/cs234/index.html)\n","- [UCL+DeepMind's RL course](https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver)\n","\n","OpenAI also has a very interesting [project](https://spinningup.openai.com/en/latest/user/introduction.html) in which they implement many RL algorithms using PyTorch and TensorFlow.\n","\n","Most of this notebook has been generated based on these resources.\n","\n","## References\n","\n","1. Gerald Tesauro, \"TD-Gammon, a self-teaching backgammon program, achieves master-level play\", 1994 - [https://dl.acm.org/doi/10.1162/neco.1994.6.2.215](https://dl.acm.org/doi/10.1162/neco.1994.6.2.215)\n","2. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, \"Playing Atari with Deep Reinforcement Learning\", 2013 - [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)"],"metadata":{"id":"HxSsBZ-Bzoot"}},{"cell_type":"code","source":[],"metadata":{"id":"_tqzMr13APcG"},"execution_count":null,"outputs":[]}]}