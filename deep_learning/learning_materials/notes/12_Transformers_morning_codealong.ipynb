{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHhK3ENp4uXi"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RltgTYFzqsdj"
   },
   "source": [
    "### **REMINDER**: Friday's coursework 2 will be in the classroom (1.51 or 1.47 depending if you are an ACSE, EDSML, or GEMS student)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8ewdbcdOTpd"
   },
   "source": [
    "# **Transformers**\n",
    "\n",
    "#### **Morning & afternoon contents/agenda**\n",
    "\n",
    "1. What is a Transformer?\n",
    "\n",
    "2. Applications and impact\n",
    "\n",
    "3. Dissecting and implementing a Transformer:\n",
    "  - Embeddings\n",
    "  - Positional Encoding\n",
    "  - Self-attention mechanism & multi-head attention\n",
    "  - Dimensions, parallelisation and residual connections\n",
    "  - Masked multi-head attention and other decoder differences\n",
    "  - Regularisation and optimiser\n",
    "\n",
    "4. Visualisation of attention maps\n",
    "\n",
    "5. `torch.nn.Transformer`\n",
    "\n",
    "\\[**NOTE**: Vision Transformers (ViT) and multi-modal Transformers will be covered on Thursday\\]\n",
    "\n",
    "\\\\\n",
    "\n",
    "#### **Learning outcomes**\n",
    "\n",
    "1. Understand the transformer architecture based on the self-attention mechanism\n",
    "\n",
    "2. Develop an intuition of what self-attention does by visualising attention maps\n",
    "\n",
    "3. Implement the main components of a transformer in PyTorch\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uPSzUFSbhqd"
   },
   "source": [
    "# WARNING\n",
    "\n",
    "Transformers are difficult to understand in their entirety, and they require multiple reviews of their architecture and internal processing elements. I do not expect you to walk away from today's lecture with a perfect understanding of how they work to the the last detail.\n",
    "\n",
    "\\\\\n",
    "\n",
    "But I do expect you to understand:\n",
    "- **What** is the self-attention mechanism and how does it work.\n",
    "- **What** are the embeddings, the positional encodings, and skip-connections used in a transformer.\n",
    "- **How** a forward pass through a transformer digests data to produce an output (for our particular implementation).\n",
    "\n",
    "\\\\\n",
    "\n",
    "I will **not** assess you or expect to fully grasp the following concepts:\n",
    "- **Why** self-attention, positional encodings, skip-connections, label smoothing and other tricks make transformers work so well.\n",
    "- **How** to train a transformer\n",
    "- **How** are transformers used in chatGPT, BERT, DALL·E 2, and other advanced networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjHjj8PoOw3n"
   },
   "source": [
    "## 1. What is a transformer?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1RHmk_SZSC08IGxQbuzUC7i5gWarGC-Yz\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "A transformer is a deep-learning model that uses a mechanism called self-attention to balance the importance of different parts of the input.\n",
    "\n",
    "\n",
    "### VAEs, GANs, transformers, and many other architectures, aim to understand and perhaps mimic how humans learn. Transformers try to capture the concept of humans not learning sequentally, but accessing memories when they are relevant for our learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AG9QcmuoPPU5"
   },
   "source": [
    "### How does it work?\n",
    "\n",
    "A transformer, like any other network will take an input and produce an output. The way the output is generated differs significantly from other architectures we have seen in previous lectures. It uses a **self-attention mechanism** to focus on different parts of the input data, but it does not require sequential inputs (like RNNs or LSTMs).\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=14NBRyF8CZ0J_6F2ELo0wUesBN8I6gLZv\" width=\"1000\"/>\n",
    "\n",
    "The inputs and outputs can be anything you want, they do not have to be text strings\n",
    "\n",
    "\\\\\n",
    "\n",
    "\\\\\n",
    "\n",
    "### History\n",
    "\n",
    "Transformers were introduced by a team working at Google Brain. They published a paper called [*Attention is all you need*](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "\\\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1653002556238,
     "user": {
      "displayName": "Lluis Guasch",
      "userId": "08850588352998994016"
     },
     "user_tz": -60
    },
    "id": "cDMKEAUlh9iH",
    "outputId": "e9666344-6d37-431b-c3f3-c0ed0d73e482"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://arxiv.org/pdf/1706.03762.pdf\" width=\"1000\" height=\"800\">\n",
       "</iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://arxiv.org/pdf/1706.03762.pdf\" width=\"1000\" height=\"800\">\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U24jA4VwQh1n"
   },
   "source": [
    "Visual representation of the **self-attention mechanism** when using a transformer to generate text:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1422/0*ODlgeguKzjyzjuuJ.gif\" width=\"700\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "and compared with a visual representation of how a **CNN-based NLP model** consumes and propagates input data:\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/NzOntsH9hZRjMaKVfuEeyyRKZ9RVxBzahp3VqkxEjcq6c9IrOKYhGuOQFkOpV3WPphaLNNTscng4tN2HdZW12-dMJgOzoJ_X0MB1loWIYKGn4mcl0Hgn-hrMJ6g4TpI-vFLqiACz\" width=\"700\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "and we have also seen that RNNs, LSTMs and GRUs consume input data sequentally and try to keep some record of what they have seen in **hidden-** or **cell-state vectors**.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*n-IgHZM5baBUjq0T7RYDBw.gif\" width=\"700\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "### The main contribution of the Transformer architecture is that it dispenses with the recurrent cells and/or convolution layers by using self-attention.\n",
    "\n",
    "\n",
    "One important difference between sequential methods and Transformers is that Transformers can paralellise inputs on training, so that they do not require sequential data processing.\n",
    "\n",
    "This results in an increase in computational efficiency, and also it means that they can see **'into the future'** (even though in the gif above we are only using the decoder part of a pre-trained transformer, and we can't really see this there).\n",
    "\n",
    "\n",
    "\\\\\n",
    "\n",
    "---\n",
    "\n",
    "\\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFkToiAGgC-P"
   },
   "source": [
    "## 2. **Applications and Impact**\n",
    "\n",
    "Transformers have revolutionised deep learning, and perhaps are one the biggest break-throughs in the field in the last years. They were originally created for NLP applications and have excelled at a performing a variety of NLP tasks:\n",
    "- translation\n",
    "- text generation\n",
    "- text comprehension\n",
    "- etc\n",
    "\n",
    "\\\\\n",
    "\n",
    "The most prominent example nowadays is perhaps the recently released **chatGPT** (We will try it on Thursday).\n",
    "\n",
    "#### [chatGPT](https://chat.openai.com/chat)\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://openai.com/content/images/size/w1400/2022/11/ChatGPT.jpg\" width=\"200\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "But they can also be used in other tasks such as computer vision (like video understanding, image classification, etc), as well as a wide range of other applications, like fraud detection.\n",
    "\n",
    "\\\\\n",
    "\n",
    "### High-impact Transformer models:\n",
    "\n",
    "\n",
    "#### [GATO](https://www.deepmind.com/publications/a-generalist-agent)\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://assets-global.website-files.com/621e749a546b7592125f38ed/627d13d743dc353a184da8d4_data_sequences.png\" width=\"500\"/>\n",
    "\n",
    "- I haven't even had time to see what this does properly.\n",
    "- Multi-modal, multi-task, multi-embodiment generalist policy.\n",
    "- The same network with **the same weights** can play Atari, caption images, chat, stack blocks with a real robot arm and much more.\n",
    "- Decides based on its context whether to output text, joint torques, button presses, or other tokens.\n",
    "- It uses Transformers by tokenising and embedding the different data input types appropriately (I think).\n",
    "\n",
    "\\\\\n",
    "\n",
    "#### [AlphaFold](https://alphafold.ebi.ac.uk/)\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://alphafold.ebi.ac.uk/assets/img/Q8I3H7_1.png\" width=\"300\"/>\n",
    "\n",
    "- Predicts protein's 3D structure from its amino-acid sequence\n",
    "- Beats all previous methods on this particular task by a large margin\n",
    "- The first version (AlphaFold 1) has 21 million parameters\n",
    "\n",
    "\\\\\n",
    "\n",
    "#### [GPT-3](https://openai.com/blog/gpt-3-apps/)\n",
    "\n",
    "#### ***they do not seem to have a logo anymore...***\n",
    "\n",
    "\n",
    "- Language model that produces high-quality text (https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3)\n",
    "- It has 175 billion parameters (350GB memory required)\n",
    "- It would take 1024 A100 GPUs (me and my colleagues have spent almost £200k to buy just 8 of them) working at full capacity for 34 days.\n",
    "- Estimated cost of training: $10-20 milion (that's compute cost only!)\n",
    "\n",
    "\\\\\n",
    "\n",
    "#### [DALL·E 2](https://openai.com/dall-e-2/)\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://cdn.openai.com/dall-e-2/demos/text2im/soup/portal/digital_art/0.jpg\" width=\"300\"/>\n",
    "\n",
    "\n",
    "- Uses a combination of CLIP (Contrastive Language-Image Pre-training) and Transformers to generate images from text. It is a bit more complicated than that really.\n",
    "- Has 3.5 billion parameters (DALL·E had 12 billion, so it is better and more efficient).\n",
    "\n",
    "\\\\\n",
    "\n",
    "#### [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n",
    "**B**idirectional **E**ncoder **R**epresentations from **T**ransformers\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://miro.medium.com/max/1220/1*PD-PyafBI-GUXT45MVwefQ.png\" width=\"300\"/>\n",
    "\n",
    "- Language modelling and next sequence prediction\n",
    "- Adopted by Google's search engine\n",
    "- 110 million parameters\n",
    "\n",
    "#### [Perceiver](https://arxiv.org/abs/2103.03206)\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://miro.medium.com/max/1400/1*hVHk3wJY5FZ6zg6zvBKxJA.png\" width=\"300\"/>\n",
    "\n",
    "- Perceivers are a Transformer-based architecture that accept any inputs without any modality-specific elements.\n",
    "- Multi-tasking capabilities: can output different modalities and perform a wide range of tasks.\n",
    "\n",
    "\\\\\n",
    "\n",
    "### **Applications in my research**:\n",
    "\n",
    "My group and I are exploring ways to apply this Transformer (and other state-of-the-art) ideas to solve scientific problems related to PDEs, image reconstruction, new Loss functions and generative models for DL, etc.\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://drive.google.com/uc?id=194uJja4G6L3ghvcvuk89OHg8_hdY_atr\" width=\"1000\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "---\n",
    "\n",
    "\\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmAfKK0S1hcx"
   },
   "source": [
    "## 3. **Dissecting Transformers**\n",
    "\n",
    "A high-level analysis reveals that Transformers are composed by an encoder and a decoder:\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=12R5_hjYmB-phlqcV7rdfr2qbZlzCtOOC\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "Both encoder and decoder are actually a combination of a series of encoding and decoding layers respectively (6 in the [original paper](https://arxiv.org/pdf/1706.03762.pdf)).\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1dhTxerpmzGbeToBDuubU3bQdabSWc7OJ\" width=\"1000\"/> | <img src=\"https://drive.google.com/uc?export=view&id=1-fBv1WcbADzeuw13BH4HbD9Z-C2GkMA5\" width=\"1000\"/>\n",
    "-|-\n",
    "\n",
    "\n",
    "\\\\\n",
    "\n",
    "Each encoder/decoder is composed of a multi-head attention layer (sometimes masked in the decoder) followed by a fully-connected layer (feed-forward), plus a some skip connections.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1qujnfANe7-qJSO9Oyw5T82jDirGmQIHf\" width=\"500\"/>\n",
    "\n",
    "\n",
    "#### Understanding the data flow:\n",
    "\n",
    "Here we will try to understand, at high level and for this particular implementation of the Transformer architecture designed to generate outputs based on input, what is the data input, and how it produces outputs that are fed back into the network. This auto-regressive processing is not specific to Transformers, most systems that generate sequential outputs use it.\n",
    "\n",
    "Let's look at a very simple example:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1JjL4nwTPd_howu8Zi45yFH-HBUfRiSaD\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "1. The `Inputs` are the embedded words in **\"convolutions are dead\"**. We feed this to the encoder.\n",
    "\n",
    "\\\\\n",
    "\n",
    "2. The decoder needs also an input, which is called `Outputs (shifted right)` in the diagram. At the beginning we don't have any output from the decoder yet (`Output probabilities` in the diagram), so we feed it a special character that indicates that we are at the beginning of a sentence: `<SOS>`. This character also has an embedding.\n",
    "\n",
    "\\\\\n",
    "\n",
    "3. We run the Transformer (assume is trained already), and we produce an vector of probabilities `Output probabilities` that has the size of the dictionary we are using, ie as many entries as words I want to consider using. This dictionary is indexed and has a corresponding embedding representation for every entry.\n",
    "\n",
    "\\\\\n",
    "\n",
    "4. We select one word from the `Output probabilities` vector. This can be just picking the one with maximum value. In practice, we can select a few with high probabilities and explore later on which one is best (this is called beaming, but we won't talk about it). Also here, let's clarify, if we want to use a Transformer to generate new text by just using the decoder, then we can sample instead of taking the maximum probability. But for now, let's assume that we want to translate a sentence and therefore we pick the maximum value of the `Output probabilities` vector. In our case this is **\"les\"**\n",
    "\n",
    "\\\\\n",
    "\n",
    "5. We now feed this **\"les\"** word to the decoder from the bottom of the diagram in `Outputs (shifted right)` together with the `<SOS>`: `(<SOS>, \"les\")`\n",
    "\n",
    "\\\\\n",
    "\n",
    "6. Run through the Transformer again and repeat the process to get a vector of probabilities of which the one with highest value will correspond to the word **\"convolucions\"**.\n",
    "\n",
    "\\\\\n",
    "\n",
    "7. We repeat this process until the model predicts `<EOS>`, the end of sentence embedded token.\n",
    "\n",
    "\\\\\n",
    "\n",
    "\\[**NOTE**: during the whole process, the input to the encoder is the whole sentence, not only the previous words, as we did with RNNs and LSTMs\\]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2m2VLbBolPG"
   },
   "source": [
    "#### We will follow [this](https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch) implementation from kaggle of the various components of the Transformer.\n",
    "\n",
    "A few imports we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1670675961506,
     "user": {
      "displayName": "Lluis Guasch",
      "userId": "08850588352998994016"
     },
     "user_tz": 0
    },
    "id": "lZ7lOT6CY1UV",
    "outputId": "feb9df05-08d8-49d1-f3a3-d670275497ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu116\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math,copy,re\n",
    "import warnings\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "# warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jk1whWiz4YBz"
   },
   "source": [
    "#### 3.1 **Embeddings**\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1zl60-CCz6p9OVaNVcO_Ag6G0zPkfO9Zl\" width=\"500\"/>\n",
    "\n",
    "We have seen embeddings in previous lectures (diffusion models, RNNs & LSTMs, and NLP).\n",
    "\n",
    "They take an input that may not be in suitable form for us to operate (words for example), and after tokenising them, represent them in a more suitable form to perform operations with them:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/0*K5a1Ws_nsbEjhbYk.png\" width=\"800\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "and this operations have some interpretation:\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/word2vec/king-analogy-viz.png\" width=\"600\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "So the first step is to add an embedding layer for both inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l31WedvQH0ww"
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            embed_dim: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embed = ### define an embedding layer\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: embedding vector\n",
    "        \"\"\"\n",
    "        out = ### embed inputs\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXUaFsydYh8Z"
   },
   "source": [
    "#### 3.2 **Positional encodings**\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1q3qy8NvOW1ozICjLktNgLLufnREaG9SZ\" width=\"500\"/>\n",
    "\n",
    "Positional encodings are an addition that allows to introduce information about the sequence ordering. Without it, the self-attention mechanism would completely ignore the relative positions of the inputs, and therefore would perform very poorly. \n",
    "\n",
    "Recursive neural networks do not have this problem because they read data sequentally, and this structured input enforces the ordering to be taken into account as the data is fed into the network to produce outputs. But recurrent neural networks are hindered because they naturally give less importance to parts of the input that occured much earlier in the sequence.\n",
    "\n",
    "Positional encodings are only added at the bottom of the encoder and decoder stacks.\n",
    "\n",
    "The original transformer implementation introduced these positional encodings by adding sine and cosine functions to the embeddings. The positional encodings are calculated as:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1WI7aVpCDEclX3HnV6pBY-5F1Jb5roy4p\" width=\"500\"/>\n",
    "\n",
    "and in the original paper the positional encodings where not learned but imposed. But it is also possible to learn this encodings during training (the authors reported no differences by trainging the positional encodings).\n",
    "\n",
    "\\\\\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png\" width=\"800\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "The signals created with these sine and cosine functions can be interleaved or concatenated before adding them:\n",
    "\n",
    "\\\\\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png\" height=\"200\"/>\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png\" height=\"200\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "There are other ways to combine the positional encodings with the embeddings of the input data, for example, you can concatenate the positional encoding to the embedding of the inputs to create a larger tensor.\n",
    "\n",
    "I suspect more strategies will appear as we gain a better understanding on how to optimally encode positions in the inputs.\n",
    "\n",
    "The implementation of the positional encoding is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrhsZi3qH8fE"
   },
   "outputs": [],
   "source": [
    "# register buffer in Pytorch ->\n",
    "# If you have parameters in your model, which should be saved and restored in the state_dict,\n",
    "# but not trained by the optimizer, you should register them as buffers.\n",
    "#\n",
    "# a good discussion on the use of registering buffers: https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723\n",
    "#\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self,max_seq_len,embed_model_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len: length of input sequence\n",
    "            embed_model_dim: demension of embedding\n",
    "        \"\"\"\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.embed_dim = embed_model_dim\n",
    "\n",
    "        pe = torch.zeros(max_seq_len,self.embed_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,self.embed_dim,2):  ### why do you think we take steps of 2 here??\n",
    "                ### implement positional encoding formulas \n",
    "                ### \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            x: output\n",
    "        \"\"\"\n",
    "      \n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.embed_dim)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        # x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False) ### deprecated implementation\n",
    "        x = x + self.pe[:,:seq_len].requires_grad_(False)  ### This is the correct modern implementation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU460ZK73xDO"
   },
   "source": [
    "#### 3.3 **Self-attention mechanism & multi-head attention**\n",
    "\n",
    "Let's start by focusing on understanding one of the encoder layers:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1qujnfANe7-qJSO9Oyw5T82jDirGmQIHf\" width=\"500\"/>\n",
    "\n",
    "and in particular on the self-attention mechanism:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1k5_KWliQJ4WmU7VsoGQZZ9jOM5MER_c_\" width=\"800\"/>\n",
    "\n",
    "How do we go from the word *'Convolutions'* to the vector $\\color{purple}{\\bf z1}$??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7Bqii2MmV4z"
   },
   "source": [
    "\\\\\n",
    "\n",
    "\\\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5Eh5nwbUBhz"
   },
   "source": [
    "#### ***Step 1***\n",
    "\n",
    "Let's assume we have already embed and positionally encoded our inputs, so that the vecor $\\color{orange}{\\bf x1}$ is ready to go into the self-attention layer:(in this case, but could be images or other inputs) into vectors:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1J55o4X3vU06HgcCEQ1j6UzEegO_mPcLu\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D7ONtbjXHfI"
   },
   "source": [
    "#### ***Step 2***\n",
    "\n",
    "Generate a $\\color{red}{\\bf query}$, a $\\color{green}{\\bf key}$, and a $\\color{blue}{\\bf value}$ vector from the input $\\color{orange}{\\bf x1}$:\n",
    "\n",
    "\\\\\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=10WNT0MI_Vduqh9CrwE22jKxacEOeOdYg\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "Where the matrices $\\color{red}{W^Q}$, $\\color{green}{W^K}$, and $\\color{blue}{W^V}$ are composed of trainable parameters.\n",
    "\n",
    "\\\\\n",
    "\n",
    "What are these $\\color{red}{\\bf query}$, a $\\color{green}{\\bf key}$, and a $\\color{blue}{\\bf value}$ vectors? A good explanation from [stack exchange](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms):\n",
    "\n",
    "\n",
    "\n",
    "*The key/value/query concepts come from retrieval systems. For example, when you type a query to search for some video on Youtube, the search engine will map your query against a set of keys (video title, description, etc.) associated with candidate videos in the database, then present you the best matched videos (values).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSInVztGYkTF"
   },
   "source": [
    "#### ***Step 3***\n",
    "\n",
    "Calculate $\\color{olive}{\\bf scores}$ and apply softmax to a normalised dot product between the query of the current word and **all the keys in the input sentence (it looks at what's going on in the input sequence as a whole!)**. Then scale the $\\color{blue}{\\bf values}$ for all the words in the input sentence using the softmax. Here we show the value vector transposed to what it was before for visualisation purposes (it was a horizontal vector and now it we see it as a vertical vector, is not relevant to anything else other than the visualisation)\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=14ltTGJQ-H-kn_0sBTV-FxflV6okZTudr\" width=\"1000\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "An example of a self-attention matrix (softmaxed scores):\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1556/0*qSKUxncfQVhUJeCr.png\" width=\"400\"/>\n",
    "\n",
    "[image source](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)\n",
    "\n",
    "\\\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1hhVkQFblZ0"
   },
   "source": [
    "#### ***Step 4***\n",
    "\n",
    "All the scaled values $\\color{blue}{\\bf v'}$ are added together to obtain $\\color{purple}{\\bf z1}$:\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1aCq95c2P07lnUwkVUqKgVADtysf5ihSU\" width=\"1000\"/>\n",
    "\n",
    "### **Question**:\n",
    "How many value vectors do we have?\n",
    "\n",
    "\\\\\n",
    "\n",
    "When you read the original paper [*Attention is all you need*](https://arxiv.org/pdf/1706.03762.pdf), as I am sure you will (you should!), you will see this diagram to explain this set of operations we just saw:\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/SCALDE.png\" width=\"300\"/>\n",
    "\n",
    "\\[We will have a look at the `Mask(opt.)` a bit later when we see the decoder.\\]\n",
    "\n",
    "The authors refer to this self-attention implementation as scaled dot-product attention:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1WdultedjMzqaSPLlyEXL5GGbwn-do-Mr\" width=\"500\"/>\n",
    "\n",
    "which describes in one formula all the operations that we have seen in the figures above. The squared root of $\\sqrt{d_k}$ helps stabilise the gradients during backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otkGJFQMcwfn"
   },
   "source": [
    "#### ***Step 5***\n",
    "\n",
    "When the Transformer was introduced, they combined this self-attention mechanism with something called **multi-head attention**.\n",
    "\n",
    "Before we jump into it, it may be worth explaining the rational for this multi-headed attention approach. There are many ways of doing this multi-headed attention, but the authors chose to reduce the size of each of the key, query, and value vectors and distribute the operations over a number of independently trainable heads. You could achieve the same dimensional output by using just a single head with larger vectors, but then the resulting attention maps would be averaged over the inputs too much. By splitting them in different heads, and eventually concatenating the output of each head, we get more 'independent' contributions to what the model should attend to. \n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png\" width=\"300\"/>\n",
    "\n",
    "where the value, keys, and queries are passed through an initial linear layer (with trainable parameters, of course) before being used in a scaled dot-product attention layer, and finally concatenated in a single vector.\n",
    "\n",
    "Let's go through the steps:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1M2FkiB_0BDP6MyXMYQckw7rQAPlApkC_\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "The different heads have independent query, key, and value matrices that are independently initialised:\n",
    "\n",
    "\\\\\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1BHCOQ-zLmk-yvCOL0Y_mxjCi5yIp0CcE\" width=\"1000\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kdY6qqqeC4o"
   },
   "source": [
    "#### ***step 6***\n",
    "\n",
    "\n",
    "Then the outputs are concatenated and multiplied with an auxiliary trainable matrix ${\\color{violet}{W^0}}$:\n",
    "\n",
    "\\\\\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1CSM-2CQQLvmq0fcJJMjqj0vV-Q5cpwWW\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "\n",
    "\\[**NOTE**: The dimensions of these vectors are way too small. This is to facilitate the visual representation. In the original paper, the dimensions of the **q**, **k**, and **v** vectors was 64, and they used 8 heads. After concatenation, this results in a 512-dimensional vector which is the size of the original embedding. \\]\n",
    "\n",
    "\\[**NOTE**: Now that we have a bit more context: the size of the embedding vector is kept constant throughout the whole network (in the original publication I think they used 512). That is, the size of ${\\color{purple}{z}}$ is the same as the input size, and it does not change as we go through the blocks in the encoder or decoder.\\]\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1280/0*X0c962yMhgRKfMTD.gif\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adOveZ8EpWao"
   },
   "source": [
    "#### 3.4 **Dimensions, parallelisation and residual connections**\n",
    "\n",
    "At his point, we should ask ourselves three important questions:\n",
    "\n",
    "1. Where is my sentence length dimension? And my batch size?\n",
    "\n",
    "2. Where does the parallelisation of inputs (non-sequential) comes into play?\n",
    "\n",
    "3. How do we keep the positional encoding information through the stacked layers of encoders?\n",
    "\n",
    "\\\\\n",
    "\n",
    "### 1. Keeping track of dimensions:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=18UETfazv0VYSHKYWhID41T3mpxwwB41k\" width=\"1000\"/>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1aBO4HBD20FvnBfryoBehCQdWi4VdKVYi\" width=\"1000\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "### 2. Parallel inputs\n",
    "\n",
    "The second question is answered by noting that the network can consume any of the inputs in the sequence independently:\n",
    "\n",
    "\\\\\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1k5_KWliQJ4WmU7VsoGQZZ9jOM5MER_c_\" width=\"400\"/>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Xxalw7hp2KecUrA2cJK9UARcQqNWlv35\" width=\"400\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "The matrices of weights $\\color{red}{W^Q}$, $\\color{green}{W^K}$, and $\\color{blue}{W^V}$ use the same parameters for all the inputs, which means that the training is [embarassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel)\n",
    "\n",
    "\\\\\n",
    "\n",
    "### 3. Keeping positional encoding information\n",
    "\n",
    "The third question correctly identifies the loss of positional encoding as we traverse the encoder stacks upwards. To avoid loosing this information we add a residual connection that just adds the value of original **`embedding + positional encoding`** to the output of the feed-forward layer at every step:\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=19uqHiNyGgIDll1WlvYsbjPbzrjqDazfu\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COkdCN56xdD2"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, n_heads=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: dimension of embeding vector output\n",
    "            n_heads: number of self attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim    #512 dim\n",
    "        self.n_heads = n_heads   #8\n",
    "        self.single_head_dim = int(self.embed_dim / self.n_heads)   #512/8 = 64  . each key,query, value will be of 64d\n",
    "       \n",
    "        #key,query and value matrixes    #64 x 64   \n",
    "        self.query_matrix = ### linear layer for querys # single key matrix for all 8 keys #512x512\n",
    "        self.key_matrix =   ### linear layer for keys\n",
    "        self.value_matrix = ### linear layer for values\n",
    "        self.out =          ### final linear layer\n",
    "\n",
    "    def forward(self,key,query,value,mask=None):    #batch_size x sequence_length x embedding_dim    # 32 x 10 x 512\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key : key vector\n",
    "           query : query vector\n",
    "           value : value vector\n",
    "           mask: mask for decoder\n",
    "        \n",
    "        Returns:\n",
    "           output vector from multihead attention\n",
    "        \"\"\"\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "        \n",
    "        # query dimension can change in decoder during inference. \n",
    "        # so we can't take general seq_length\n",
    "        seq_length_query = query.size(1)  ### let's park this for now...\n",
    "        \n",
    "        # 32x10x512\n",
    "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)  #batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)\n",
    "        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim) #(32x10x8x64)\n",
    "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim) #(32x10x8x64)\n",
    "       \n",
    "        k = ### generate keys using linear layer defined above     # (32x10x8x64)\n",
    "        q = ### generate querys using linear layer defined above\n",
    "        v = ### generate values using linear layer defined above\n",
    "\n",
    "        q = q.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)\n",
    "        k = k.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
    "        v = v.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
    "       \n",
    "        # computes attention\n",
    "        # adjust key for matrix multiplication\n",
    "        k_adjusted = k.transpose(-1,-2)  #(batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)\n",
    "        product = ### compute Q K product  #(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)\n",
    "      \n",
    "        \n",
    "        # fill those positions of product matrix as (-1e20) where mask positions are 0\n",
    "        if mask is not None:\n",
    "             product = product.masked_fill(mask == 0, float(\"-1e20\"))  ### we will see later in the decoder\n",
    "\n",
    "        #divising by square root of key dimension\n",
    "        product = ### divide by sqrt(dim_k) # / sqrt(64)\n",
    "\n",
    "        #applying softmax\n",
    "        scores = ### apply softmax\n",
    " \n",
    "        #mutiply with value matrix\n",
    "        scores = ### multiply scores and value matrix  #(32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64) \n",
    "        \n",
    "        #concatenated output\n",
    "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim*self.n_heads) ### view only works on contiguous tensors  # (32x8x10x64) -> (32x10x8x64)  -> (32,10,512)\n",
    "                                                                                                                          ### see this: https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107\n",
    "        output = ### apply final linear layer #(32,10,512) -> (32,10,512) ### because we have split the 512 in 8 groups of 64\n",
    "                                                              ### and we want to mantain the embedding dimension (512) throughout the network (from the original implementation)\n",
    "       \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWPnKt_82AEF"
   },
   "source": [
    "### and now we can implement the Transformer Block and the Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GN5wn0KS1-x0"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attention = ### define multihead attention\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim) \n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "                          nn.Linear(embed_dim, expansion_factor*embed_dim),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(expansion_factor*embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout1 = ### define a dropout layer with p=0.2\n",
    "        self.dropout2 = ### define a dropout layer with p=0.2\n",
    "\n",
    "    def forward(self,key,query,value):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           norm2_out: output of transformer block\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        attention_out = ### calculate output of multihead attention #32x10x512\n",
    "        attention_residual_out =  ### add residual connection #32x10x512\n",
    "        norm1_out =  ### apply layer normalisation and dropout to attention_residual_out  #32x10x512\n",
    "\n",
    "        feed_fwd_out =  ### pass norm1_out through feed forward block #32x10x512 -> #32x10x2048 -> 32x10x512\n",
    "        feed_fwd_residual_out =  ### add residual connection #32x10x512\n",
    "        norm2_out =  ### apply layer normalisation and dropout to attention_residual_out  #32x10x512\n",
    "\n",
    "        return norm2_out\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_len : length of input sequence\n",
    "        embed_dim: dimension of embedding\n",
    "        num_layers: number of encoder layers\n",
    "        expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "        n_heads: number of heads in multihead attention\n",
    "        \n",
    "    Returns:\n",
    "        out: output of the encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer =  ### define embedding layer\n",
    "        self.positional_encoder =  ### define positional encoding\n",
    "\n",
    "        self.layers = nn.ModuleList([ ### add transformer blocks   ]) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed_out = ### apply embedding\n",
    "        out =  ### apply positional encoding\n",
    "        for layer in self.layers:\n",
    "            out =  ### run through blocks\n",
    "\n",
    "        return out  #32x10x512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io0P5KYplrbB"
   },
   "source": [
    "#### 3.5 **Masked multi-head attention and other decoder differences**\n",
    "\n",
    "The decoder side is very similar to the encoder. It stacks a series of Multi-head attention combined with FC layers (This is what the `Nx` means next to the encoder and decoder figure).\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=12R5_hjYmB-phlqcV7rdfr2qbZlzCtOOC\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "But there are a few important differences:\n",
    "- The decoder has **`Outputs (shifted right)`** as inputs\n",
    "- The first multi-head attention layer is **`masked`**\n",
    "- Some information is being fed from the **encoder stack**\n",
    "- It has a **`Linear`** + **`Softmax`** layer on output (top of the figure)\n",
    "\n",
    "Breaking them one by one:\n",
    "\n",
    "- The decoder has **`Outputs (shifted right)`** as inputs:\n",
    "\n",
    "This is similar to what we saw in RNNs and LSTMs: this is to ensure that the decoder does not use the current position (the right word to translate for example), and only uses data that has been generated/predicted/translated before in the sentence (input sequence). For example, if we are generating text, the first generated word passed to the decoder is the token <start> and the prediction process continues until the decoder generates a special end token <eos>.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/960/0*u8nSpT8Z8ITwzNLV.gif\" width=\"500\"/>\n",
    "\n",
    "\\\\\n",
    "\n",
    "- The first multi-head attention layer is **`masked`**\n",
    "\n",
    "This is an important one. We do not want the decoder to have access to any information ahead of its current processing position:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1090/0*0pqSkWgSPZYr_Sjx.png\" width=\"300\"/>\n",
    "\n",
    "The word **`am`** should not consider the word **`fine`** in its attention mapping. It should only attend to words (inputs) that precede it in the sentence (sequence). Why? To keep its auto-regressive properties. What does auto-regressive mean? In models that operate with sequential data, it means that the model uses its outputs as inputs for the next sequential step.\n",
    "\n",
    "RNNs and LSTMs are also autoregressive, they keep previous information in hidden- and cell-state vectors, but they are limited in the contextual information that they have available (only in the past, and with vanishing importance with distance). \n",
    "\n",
    "A good intro on autoregressive models [here](https://ml.berkeley.edu/blog/posts/AR_intro/)\n",
    "\n",
    "To restrict the use of 'future' words, we mask the attention matrices:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/0*QYFua-iIKp5jZLNT.png\" height=\"150\"/>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/0*3ykVCJ9okbgB0uUR.png\" height=\"150\"/>\n",
    "\n",
    "This masked self-attention layer also has multiple heads (8 in the original publication).\n",
    "\n",
    "\\\\\n",
    "\n",
    "- Some information is being fed from the **encoder stack**\n",
    "\n",
    "The outputs from the encoder are fed to the **encoder-decoder attention** layers:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Uw-AZWXy_UqHJGYcIpLjvZeBdoJAFHzm\" width=\"300\"/>\n",
    "\n",
    "The **keys** and **values** from the encoder output are combined with the **queries** of the previous decoder layer and used as inputs for these **encoder-decoder attention** layers. This allows every position in the decoder to attend to all positions in the input sequence.\n",
    "\n",
    "\\\\\n",
    "\n",
    "- It has a **`Linear`** + **`Softmax`** layer on output (top of the figure):\n",
    "\n",
    "**QUESTIONS**:\n",
    "\n",
    "1. What do this combination of fully-connected layer and Softmax activations normally do at the end of a network?\n",
    "\n",
    "2. What could be the dimension of this output softmax vector?\n",
    "\n",
    "\\\\\n",
    "\n",
    "We can now implement the Decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHZkQh1_BrEX"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attention = ### multihead attention\n",
    "        self.norm = ### layer norm layer\n",
    "        self.dropout = ### dropout layer\n",
    "        self.transformer_block = ### transformer block\n",
    "        \n",
    "    \n",
    "    def forward(self, key, query, x,mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           mask: mask to be given for multi head attention \n",
    "        Returns:\n",
    "           out: output of transformer block\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        #we need to pass the mask only to fst attention\n",
    "        attention = self.attention(x,x,x,mask=mask) #32x10x512\n",
    "        value = ### apply dropout to attention with skip connection\n",
    "        \n",
    "        out = ### pass through transformer block\n",
    "\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           target_vocab_size: vocabulary size of taget\n",
    "           embed_dim: dimension of embedding\n",
    "           seq_len : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        self.word_embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
    "        self.position_embedding = PositionalEmbedding(seq_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_dim, expansion_factor=4, n_heads=8) \n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector from target\n",
    "            enc_out : output from encoder layer\n",
    "            trg_mask: mask for decoder self attention\n",
    "        Returns:\n",
    "            out: output vector\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        x = ### word embedding  #32x10x512\n",
    "        x = ### add positional encoding #32x10x512\n",
    "        x = ### apply dropout\n",
    "     \n",
    "        for layer in self.layers:\n",
    "            x = layer(enc_out, x, enc_out, mask) \n",
    "\n",
    "        out = ### apply softmax to output of final linear layer (fc_out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHcZztjmZjk1"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpX54KtECmb9"
   },
   "source": [
    "#### Now we have all the elements to implement our Transformer and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-VpZakmC3MI"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_length,num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           embed_dim:  dimension of embedding \n",
    "           src_vocab_size: vocabulary size of source\n",
    "           target_vocab_size: vocabulary size of target\n",
    "           seq_length : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.target_vocab_size = target_vocab_size\n",
    "\n",
    "        self.encoder = TransformerEncoder(seq_length, src_vocab_size, embed_dim, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "        self.decoder = TransformerDecoder(target_vocab_size, embed_dim, seq_length, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "        \n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: target sequence\n",
    "        Returns:\n",
    "            trg_mask: target mask\n",
    "        \"\"\"\n",
    "        batch_size, trg_len = trg.shape\n",
    "        # returns the lower triangular part of matrix filled with ones\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            batch_size, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask    \n",
    "\n",
    "    def decode(self,src,trg):\n",
    "        \"\"\"\n",
    "        for inference\n",
    "        Args:\n",
    "            src: input to encoder \n",
    "            trg: input to decoder\n",
    "        out:\n",
    "            out_labels : returns final prediction of sequence\n",
    "        \"\"\"\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "        out_labels = []\n",
    "        batch_size,seq_len = src.shape[0],src.shape[1]\n",
    "        #outputs = torch.zeros(seq_len, batch_size, self.target_vocab_size)\n",
    "        out = trg\n",
    "        for i in range(seq_len): #10\n",
    "            out = self.decoder(out,enc_out,trg_mask) #bs x seq_len x vocab_dim\n",
    "            # taking the last token\n",
    "            out = out[:,-1,:]\n",
    "     \n",
    "            out = out.argmax(-1)\n",
    "            out_labels.append(out.item())\n",
    "            out = torch.unsqueeze(out,axis=0)\n",
    "          \n",
    "        \n",
    "        return out_labels\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: input to encoder \n",
    "            trg: input to decoder\n",
    "        out:\n",
    "            out: final vector which returns probabilities of each target word\n",
    "        \"\"\"\n",
    "        trg_mask = ### make a target mask\n",
    "        enc_out = ### pass through encoder\n",
    "   \n",
    "        outputs = ### pass through decoder\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHh9FfW7mL6A"
   },
   "source": [
    "#### 3.6 **Regularisation and optimiser**\n",
    "\n",
    "Finally, such complex architecture requires some sort of regularisation in order to be able to train properly.\n",
    "\n",
    "Four regularisation methods are applied to this network:\n",
    "- Layer normalisation applied after the residual connections are added in `Add & Norm` yellow blocks in the diagram. Layer normalisation is similar to batch normalisation but does not depend on batch size. Introduced by Hinton in [this paper](https://arxiv.org/pdf/1607.06450.pdf).\n",
    "- Dropout (after most layers, including after the positional encoding layer) with a dropout rate of $P_{drop}=0.2$ (in the paper I think they use 0.1).\n",
    "- Skip connections to mantain positional encoding information.\n",
    "- Label smoothing (basically add noise to the labels, you can read about it [here](https://paperswithcode.com/method/label-smoothing)). According to the authors: *This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score*.\n",
    "\n",
    "The optimiser used in the original publication was Adam.\n",
    "\n",
    "\n",
    "### Let's test a forward pass through our network and check if at least the dimensions add up to what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rE9zTevAC6wQ"
   },
   "outputs": [],
   "source": [
    "### let's test if it does what we want, at least in terms of dimensions:\n",
    "\n",
    "src_vocab_size = 11\n",
    "target_vocab_size = 11\n",
    "num_layers = 6\n",
    "seq_length= 12\n",
    "\n",
    "\n",
    "# let 0 be sos token and 1 be eos token\n",
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1], \n",
    "                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
    "target = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1], \n",
    "                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\n",
    "\n",
    "print(src.shape,target.shape)\n",
    "model = Transformer(embed_dim=512, src_vocab_size=src_vocab_size, \n",
    "                    target_vocab_size=target_vocab_size, seq_length=seq_length,\n",
    "                    num_layers=num_layers, expansion_factor=4, n_heads=8)\n",
    "\n",
    "print(model(src,target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPe0sgTnKNK4"
   },
   "source": [
    "## 4. Visualisation of attention maps\n",
    "\n",
    "To visualise how words attend to other words we will use an interactive tool called [bertviz](https://github.com/jessevig/bertviz).\n",
    "\n",
    "We have not covered how BERT (Bidirectional Encoder Representations from Transformers) work, here we will just use the visualisation tool provided to provide better intuitions on how words attend to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fTN0oa-4tgF"
   },
   "outputs": [],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVCePBjqg95q"
   },
   "outputs": [],
   "source": [
    "# Load model and retrieve attention weights\n",
    "\n",
    "from bertviz import head_view, model_view\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_version = 'bert-base-uncased'\n",
    "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
    "sentence_a = \"The cat sat on the mat\"\n",
    "sentence_b = \"The cat lay on the rug\"\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "sentence_b_start = token_type_ids[0].tolist().index(1)\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZAEjDIfNXRh"
   },
   "source": [
    "### Head View\n",
    "<b>The head view visualizes attention in one or more heads from a single Transformer layer.</b> Each line shows the attention from one token (left) to another (right). Line weight reflects the attention value (ranges from 0 to 1), while line color identifies the attention head. When multiple heads are selected (indicated by the colored tiles at the top), the corresponding  visualizations are overlaid onto one another.  For a more detailed explanation of attention in Transformer models, please refer to the [blog](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1).\n",
    "\n",
    "#### Usage\n",
    "\n",
    "👉 **Hover** over any **token** on the left/right side of the visualization to filter attention from/to that token. <br/>\n",
    "👉 **Double-click** on any of the **colored tiles** at the top to filter to the corresponding attention head.<br/>\n",
    "👉 **Single-click** on any of the **colored tiles** to toggle selection of the corresponding attention head. <br/>\n",
    "👉 **Click** on the **Layer** drop-down to change the model layer (zero-indexed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3Alvp9Jg-sE"
   },
   "outputs": [],
   "source": [
    "head_view(attention, tokens, sentence_b_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FaBpOD5Nam4"
   },
   "source": [
    "### Model View\n",
    "<b>The model view provides a birds-eye view of attention throughout the entire model</b>. Each cell shows the attention weights for a particular head, indexed by layer (row) and head (column).  The lines in each cell represent the attention from one token (left) to another (right), with line weight proportional to the attention value (ranges from 0 to 1).  For a more detailed explanation, please refer to the [blog](https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1).\n",
    "\n",
    "#### Usage\n",
    "👉 **Click** on any **cell** for a detailed view of attention for the associated attention head (or to unselect that cell). <br/>\n",
    "👉 Then **hover** over any **token** on the left side of detail view to filter the attention from that token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_hkC109hM7C"
   },
   "outputs": [],
   "source": [
    "model_view(attention, tokens, sentence_b_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1653026866667,
     "user": {
      "displayName": "Lluis Guasch",
      "userId": "08850588352998994016"
     },
     "user_tz": -60
    },
    "id": "0XKqp1sbhkhj",
    "outputId": "b1072425-2d89-40a6-a9a1-b7331af79daf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://poloclub.github.io/dodrio/\" width=\"1000\" height=\"800\">\n",
       "</iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://poloclub.github.io/dodrio/\" width=\"1000\" height=\"800\">\n",
    "</iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaB8pn-RjbWl"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6UJT17oL8AI"
   },
   "source": [
    "## 5. `torch.nn.Transformer`\n",
    "\n",
    "Why did we go through all the pain of implementing a transformer if we can just use the [`torch.nn.Transformer`](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) class?\n",
    "\n",
    "The reason is that it is important to understand the underlying architecture of complex networks. In the real world, when you are working on a particular problem, it is quite likely that you need to modify these baseline provided networks to adapt them to the specifications of your problem.\n",
    "\n",
    "Anyone can download a Transformer and use it, but **what will set you appart is your ability to understand what's going on under the hood.**\n",
    "\n",
    "\\\\\n",
    "\n",
    "But, if we just want to get a Transformer model in PyTorch (it implements the architecture described in the original paper) we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azUH3UATEo8n"
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7P50zroPmtsy"
   },
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysf9udXbmx_W"
   },
   "outputs": [],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XToH9Q8jc_Q"
   },
   "source": [
    "\\\\\n",
    "\n",
    "---\n",
    "\n",
    "\\\\\n",
    "\n",
    "## 6. **Useful materials and additional resources**\n",
    "\n",
    "\n",
    "- The original Transformer paper: [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "- A good blog describing these and other components of the Transformers: [The illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "- And another: [Illustrated guide to Transformers](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)\n",
    "- The visualisation tools are available here: [bertviz](https://github.com/jessevig/bertviz)\n",
    "- Brilliant website to visualise Transformers in action: [dodrio](https://poloclub.github.io/dodrio/)\n",
    "- Paper describing GPT-3: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n",
    "- Alphafold Nature paper: [Highly accurate protein structure prediction with AlphaFold](https://www.nature.com/articles/s41586-021-03819-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fl68qmnTP60m"
   },
   "source": [
    "## **Bonus**\n",
    "\n",
    "We can use a pretrained Transformer (as we do not have the resources to train a proper one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dKA2vQjLalH"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2599,
     "status": "ok",
     "timestamp": 1670679387871,
     "user": {
      "displayName": "Lluis Guasch",
      "userId": "08850588352998994016"
     },
     "user_tz": 0
    },
    "id": "5fH8-nqnLcUg",
    "outputId": "ab25cb95-8ff7-4d89-8066-02d6d65c8236"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "question_answering = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FR9L-QgMt_dF"
   },
   "source": [
    "First paragraphs of wiki from its string theory entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44OiCJtTLeIY"
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "In physics, string theory is a theoretical framework in which the point-like particles\n",
    "of particle physics are replaced by one-dimensional objects called strings. String\n",
    "theory describes how these strings propagate through space and interact with each other.\n",
    "On distance scales larger than the string scale, a string looks just like an ordinary\n",
    "particle, with its mass, charge, and other properties determined by the vibrational state\n",
    "of the string. In string theory, one of the many vibrational states of the string\n",
    "corresponds to the graviton, a quantum mechanical particle that carries the gravitational\n",
    "force. Thus, string theory is a theory of quantum gravity.\n",
    "\n",
    "String theory is a broad and varied subject that attempts to address a number of deep\n",
    "questions of fundamental physics. String theory has contributed a number of advances\n",
    "to mathematical physics, which have been applied to a variety of problems in black hole\n",
    "physics, early universe cosmology, nuclear physics, and condensed matter physics, and\n",
    "it has stimulated a number of major developments in pure mathematics. Because string\n",
    "theory potentially provides a unified description of gravity and particle physics, it\n",
    "is a candidate for a theory of everything, a self-contained mathematical model that\n",
    "describes all fundamental forces and forms of matter. Despite much work on these\n",
    "problems, it is not known to what extent string theory describes the real world or\n",
    "how much freedom the theory allows in the choice of its details.\n",
    "\n",
    "String theory was first studied in the late 1960s as a theory of the strong nuclear\n",
    "force, before being abandoned in favor of quantum chromodynamics. Subsequently, it was\n",
    "realized that the very properties that made string theory unsuitable as a theory of\n",
    "nuclear physics made it a promising candidate for a quantum theory of gravity. The\n",
    "earliest version of string theory, bosonic string theory, incorporated only the class\n",
    "of particles known as bosons. It later developed into superstring theory, which posits\n",
    "a connection called supersymmetry between bosons and the class of particles called fermions.\n",
    "Five consistent versions of superstring theory were developed before it was conjectured in\n",
    "the mid-1990s that they were all different limiting cases of a single theory in 11 dimensions\n",
    "known as M-theory. In late 1997, theorists discovered an important relationship called the\n",
    "anti-de Sitter/conformal field theory correspondence (AdS/CFT correspondence), which relates\n",
    "string theory to another type of physical theory called a quantum field theory.\n",
    "\n",
    "One of the challenges of string theory is that the full theory does not have a satisfactory\n",
    "definition in all circumstances. Another issue is that the theory is thought to describe an\n",
    "enormous landscape of possible universes, which has complicated efforts to develop theories\n",
    "of particle physics based on string theory. These issues have led some in the community to\n",
    "criticize these approaches to physics, and to question the value of continued research on\n",
    "string theory unification. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyGZGjrEL2U-"
   },
   "outputs": [],
   "source": [
    "question = \"what is string theory?\"\n",
    "result = question_answering(question=question, context=context)\n",
    "\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Score:\", result['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3I2z9l4upg6"
   },
   "outputs": [],
   "source": [
    "question = \"What is the goal of string theory?\"\n",
    "result = question_answering(question=question, context=context)\n",
    "\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Score:\", result['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fax2xQOxtiaJ"
   },
   "source": [
    "Batch normalisation first paragraphs from wikipedia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StyEVZekL7im"
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Batch normalization (also known as batch norm) is a method used to make training\n",
    "of artificial neural networks faster and more stable through normalization of the\n",
    "layers' inputs by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015.[1]\n",
    "\n",
    "While the effect of batch normalization is evident, the reasons behind its effectiveness\n",
    "remain under discussion. It was believed that it can mitigate the problem of internal\n",
    "covariate shift, where parameter initialization and changes in the distribution of the\n",
    "inputs of each layer affect the learning rate of the network.[1] Recently, some scholars\n",
    "have argued that batch normalization does not reduce internal covariate shift, but rather\n",
    "smooths the objective function, which in turn improves the performance.[2] However, at\n",
    "initialization, batch normalization in fact induces severe gradient explosion in deep networks, which\n",
    "is only alleviated by skip connections in residual networks.[3] Others maintain that batch normalization\n",
    "achieves length-direction decoupling, and thereby accelerates neural networks.[4] More recently a\n",
    "normalize gradient clipping technique and smart hyperparameter tuning has been introduced in\n",
    "Normalizer-Free Nets, so called \"NF-Nets\" which mitigates the need for batch normalization.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnciUqpfQd8i"
   },
   "outputs": [],
   "source": [
    "question = \"what is the purpose of batch normalisation?\"\n",
    "result = question_answering(question=question, context=context)\n",
    "\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Score:\", result['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHpkZfO9RRAZ"
   },
   "source": [
    "On Thursday we will see how this can be greatly improved by using better more modern implementations of Transformers."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNMQbWKMhOdydmDnzakO87/",
   "collapsed_sections": [
    "a5Eh5nwbUBhz",
    "6D7ONtbjXHfI"
   ],
   "provenance": [
    {
     "file_id": "https://github.com/ese-msc-2022/DL_module/blob/main/12_Transformers/12_Transformers_morning/12_Transformers_morning_codealong.ipynb",
     "timestamp": 1670880650967
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
