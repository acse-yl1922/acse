{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMKYhSpnIIdOZEg+KxOyZAG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n","\n","\n","---\n"],"metadata":{"id":"9YehS8enAmDn"}},{"cell_type":"markdown","source":["# **Generative Adversarial Networks (GANs)**\n","\n","#### **Morning contents/agenda**\n","\n","1. Why GANs?\n","\n","2. Recap of generative models and VAEs\n","\n","3. Formulation and training strategy\n","\n","4. Implemention of a simple GAN\n","\n","5. Pros and cons\n","\n","6. Overview of main GAN flavours\n","\n","#### **Learning outcomes**\n","\n","1. Understand the basic principles of adversarial traning (with two competing networks)\n","\n","2. Implementa a Generator and a Discriminator and train them with a simple GAN\n","\n","3. Be aware of the importance of GANs in deep learning, its many variations, benefits, and limitations\n","\n","\n","<br>\n","\n","#### **Afternoon contents/agenda**\n","\n","No afternoon session. Materials provided for Wasserstein GAN (for information only, **you will NOT be assessed on WGANs**)\n","\n","1. Wasserstein GANs theory and implementation example.\n","\n","<br/>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"vA1tJ0r_vPQf"}},{"cell_type":"markdown","source":["## 1. Why GANs?\n","\n","GANs were introduced by Ian Goodfellow and others (including Yoshua Bengio) in 2014.\n","\n","<center><img src=\"https://drive.google.com/uc?id=1RPHM4jNVmYwT0_MTttcFnpYAJdjyQ6uC\" width=\"800\"/></center>\n","\n","<br>\n","\n","- GANs are very effective as a tool to learn data distributions. \n","\n","- They can be used in a wide variety of applications: data generation, clustering, representation learning, translation, etc.\n","\n","- GANs can handle different types of data: audio, images, video, text, etc.\n","\n","<br>\n","\n","There has been very significant improvement from the publication of the [original paper](https://arxiv.org/pdf/1406.2661.pdf). Results in the original publication:\n","\n","<center><img src=\"https://drive.google.com/uc?id=1J5IAUvAv0WPp0KjdEersf41hCPI69w8v\" width=\"600\"/></center>\n","\n","More complex variations on the original idea have resulted in dramatic improvements. For example:\n","\n","- StyleGAN ([Paper](https://arxiv.org/pdf/1812.04948.pdf))"],"metadata":{"id":"sinIebTtMUvs"}},{"cell_type":"code","source":["%%html\n","<iframe src=\"https://thispersondoesnotexist.com/\" width=\"500\" height=\"400\"></iframe>"],"metadata":{"id":"ljhihMQwwybE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<center><img src=\"https://drive.google.com/uc?id=12fF1vr_SNgc5vUaY0WcA0AC87Tv64Sq5\" width=\"800\"/></center>\n","\n","\n","<br>\n","\n","- cycleGAN ([Paper](https://arxiv.org/pdf/1703.10593.pdf))\n","\n","<center><img src=\"https://drive.google.com/uc?id=1duze4mADJf-eGH1_15sNqa4bKsibThKu\" width=\"800\"/></center>\n","\n","<br>\n","\n","\n","<br>\n","\n","---\n","\n","<br>\n"],"metadata":{"id":"Muvvr-f08inR"}},{"cell_type":"markdown","source":["## 2. Recap of generative models and VAEs\n","\n","Generative models are designed to find the distribution that better explains where the samples of a particular dataset come from, and then use this distribution to generate data samples.\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=13yVxGyVbefU6on-QFtgS8PHHOM-CnIdE\" width=\"300\"/></p><p align = \"center\">\n","<i> <a href=\"https://openai.com/blog/generative-models/\">image source</a></i>\n","</p>\n","\n","<br>\n","\n","but we cannot explicitly know the probability density function that describes $p_{data}(x)$. Instead we use a neural network that we will denote $G$ (Generator) to learn $p_{data}(x)$.\n","\n","<br>\n","\n","For a generative network $G$, and given a training real dataset:\n","\n","- $G$ is a mapping function from $z$ to data space\n","- True data distribution $p_{data}(x)$ (<font color='green'>p̂(x)</font> in the figure)\n","- Generated data distribution $p_G(x)$ (<font color='blue'>p(x)</font> in the figure)\n","\n","\n","<p align = \"center\"><img src=\"https://openai.com/content/images/2017/02/gen_models_diag_2.svg\" width=\"800\"/></p><p align = \"center\">\n","<i> <a href=\"https://openai.com/blog/generative-models/\">image source</a></i>\n","</p>\n","\n","#### We want to find a generator $G^*$ that satisfies $p_{G^*} \\approx p_{data}$\n","\n","<br>\n","\n","### **Adversarial loss** for the generative model:\n","\n","To find optimal $G^*$, we update our network parameters to minimise the difference (divergence) between $p_{G}$ and $p_{data}$:\n","\n","$$G^* = \\underset{G}{\\operatorname{argmin}}  Div (P_{data}(x) || P_G(x))$$\n","\n","But we cannot define a simple loss function to measure this divergence explicitly.\n","\n","As we saw yesterday, VAEs training is based only on the evidence lower bound (ELBO), a surrogate (lower bound) of the likelihood. Additionally, the loss used in VAE often are a bit limited in how they measure good network performance.\n","\n","Recap of training a VAE:\n","\n","1. Sample $x_i$ from training data.\n","2. Train an Encoder$(x_i) \\rightarrow z_i$ and a Decoder$(z_i) \\rightarrow \\hat{x}_i$ by using an explicit pixel-wise loss (like MSE).\n","3. After training is complete, sample $z \\approx p(z)$ to generate new data using the decoder.\n","\n","Consquences of using MSE loss in VAEs:\n","\n","- Pixel-wise losses are not 'intelligent' enough\n","- Blurry outputs\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1-m99v3GlBjhWWI1jZC1x0WR7o7pupYsq\" width=\"600\"/></p><p align = \"center\">\n","<i> which image is perceptually better? </a></i>\n","</p>\n","\n","<br>\n","\n","Later today we will implement a simple GAN and confirm that it produces sharper images than the VAE we implemented yesterday:\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1GY4bhQ01eEWGZdx_9oqR0tk7hs_JUGo6\" width=\"400\"/></p><p align = \"center\">\n","<i> VAE results on MNIST </a></i>\n","</p>\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1fXftd0wG_wKAooG_EZUTmXGhnuGc8L53\" width=\"400\"/></p><p align = \"center\">\n","<i> GAN results on MNIST </a></i>\n","</p>\n","\n","<br>\n","\n","\n","GANs provide a method to directly optimise $G^* = \\underset{G}{\\operatorname{argmin}}  Div (P_{data}(x) || P_G(x))$ by using another network called a **Discriminator** for the loss function – the **adversarial loss**.\n","\n","\n","Training GAN $\\rightarrow$ minimising J-S divergence (but other divergences/distances can also be used)\n","\n","The advantages of GANs compared to VAEs are:\n","\n","- Loss function used is more 'flexible' and 'intelligent'.\n","- It produces perceptually better results than VAEs' MSE loss.\n","\n","\n","\n","<br>\n","\n","---\n","\n","<br>\n"],"metadata":{"id":"XxyTU0eAARkw"}},{"cell_type":"markdown","source":["## 3. Formulation and training strategy\n","\n","\n","<center><img src=\"https://drive.google.com/uc?id=166bm5GMoWPS5kQxA4Pt3LOk1mqf73FkS\" width=\"800\"/></center>\n","\n","<br>\n","\n","To generate realistic samples from $G(z)$, we play a zero-sum competition game:\n","\n","- Train $D$ to **correctly label** its inputs as *REAL* or *FAKE*\n","\n","- Train $G$ to **'fool'** $D$ to label $G(z)$ as a *REAL* sample\n","\n","In this way, both Generator and Discriminator improve as the training workflow progresses: $G$ gets better at generating *FAKE* samples, because $D$ is getting better at spotting them. The optimal solution would be met when:\n","\n","$$ {G: p_G = p_{data}} $$\n","\n","<br>\n","\n","To play this zero-sum game, we use a **min-max loss function**:\n","\n","<br>\n","$$\\boxed {\\underset{G}{\\operatorname{min}} \\underset{D}{\\operatorname{max}}  V_{GAN} (G,D) = \\mathbb{E}_{x \\sim p_{data}(x)} [\\log D(x)] \n","+ \\mathbb{E}_{z \\sim p_{z}(z)} [\\log(1 - D(G(z)))]} $$ \n","\n","<br>\n","\n","This equation means:\n","1. Train $D$ to maximise the loss, so that:\n","  - when input is *REAL*: $\\, \\, D(x) \\rightarrow 1$\n","  - when input is *FAKE*: $\\, \\, D(G(z)) \\rightarrow 0$\n","\n","2. Train $G$ to minimise the loss:\n","  - 'Fools' $D$ to output a high-score when when input is *FAKE*: $\\, \\, D(G(z)) \\rightarrow 1$\n","\n","\n","A visual representation of the training process (image adapted from the [original paper](https://arxiv.org/pdf/1406.2661.pdf)):\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1ZmEJOigye_0hb9_PdHla-52-x_uYYEW-\" width=\"800\"/></center>\n","\n","<br>\n","\n","As we mentioned above, we are playing a zero-sum game. But what does this means? A zero-sum game is a mathematical representation of a situation involving two players (or more) where the gains of one player equal the losses of the other. In the context of GANs, what it means that if the Generator gets better, by definition, the Discriminator must get worse. That does not mean that both of them can't get better if we train them in an alternate way, which is exactly what we do in GANs.\n","\n","When we have a look at the loss curves for a GANs, we should see this kind of behaviour:\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1IbN8wG8KdELj6aCtkh1UqGNpZ6TGPAXR\" width=\"400\"/></center>\n","\n","<br>\n","\n","\n","When we train the discriminator $D$, we obtain this expression:\n","\n","$$D^*(x) = {\\underset{D}{\\operatorname{argmax}}} \\left[ p_{data}(x) \\log{D(x)} + p_G(x) \\log{(1-D(x))} \\right] = \\frac{p_{ data}(x) }{ p_{data}(x) + p_G(x) }$$\n","\n","\n","Substituting this into the loss expression gives us:\n","\n","<br>\n","\n","$$ V_{GAN} (G,D^*) = \\mathbb{E}_{x \\sim p_{data}(x)} \\left[\\frac{p_{data}(x) }{ p_{data}(x) + p_G(x) }  \\right] + \\mathbb{E}_{x \\sim p_G} \\left[\\frac{p_G(x)}{ p_{data}(x) + p_G(x) }  \\right] \\\\[20pt] = - \\log(4) + 2 JSD (p_{data} || p_G) $$\n","\n","<br>\n","\n","\\[**NOTE**: you can find the full derivation in the original paper by Goodfellow\\]\n","\n","<br>\n","\n","So, when we train GANs we following this workflow:\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1om1rv0un8TWKifoo35iiu_ukphLsQ138\" width=\"800\"/></center>\n","\n","<br>\n","\n","\n"],"metadata":{"id":"62eBj4KIARh5"}},{"cell_type":"markdown","source":["##### **Intuitive explanation of GANs training procedure**:\n","\n","We can interpret GANs as a competition between a ***Forger*** ($G$) and a ***Detective*** ($D$):\n","\n","<br>\n","\n","<center><img src=\"https://miro.medium.com/max/1400/1*-gFsbymY9oJUQJ-A3GTfeg.webp\" width=\"800\"/> <br> <a href=\"https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f\"> <i> image source </i>  </a> </center>\n","\n","<br>\n","\n","\n","\n","But it is better to thing of it in terms of a ***teacher*** ($D$) and a ***student*** ($G$). Interaction between $G$ and $D$:\n","\n","1. $D$ is leading $G$:\n","  - $D$ is trained first\n","  - $D$ provides knowledge to update $G$\n","\n","2. $D$ needs to teach adapting to the level of $G$:\n","  - Measure the distance between current $p_G$ and $p_{data}$\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1_VwpWfWZlr-ijrK3wUKNOFEO3nwuWb3t\" width=\"800\"/> <br> <a href=\"https://robots.media.mit.edu/wp-content/uploads/sites/7/2021/03/EAAI-What-are-GANs_.pdf\"> <i> image source </i> </a> </center>\n","\n","<br>\n","\n","\n"],"metadata":{"id":"nQ-LcBSuARU7"}},{"cell_type":"code","source":["%%html\n","<iframe src=\"https://poloclub.github.io/ganlab/\" width=\"1200\" height=\"600\"></iframe>"],"metadata":{"id":"wXcJzNfOmFAG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"KIVf_uNQCIaD"}},{"cell_type":"markdown","source":["## 4. Implemention of a simple GAN\n","\n","A few imports before we get started"],"metadata":{"id":"4rS6CGKOyHbd"}},{"cell_type":"code","source":["!pip install livelossplot\n","%pylab inline"],"metadata":{"id":"xGxjbcOPho7r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from livelossplot import PlotLosses\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","\n","import random \n","def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"],"metadata":{"id":"IsZ9BJ4pymFP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"metadata":{"id":"QYwe947Cywd4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will implement a simple GANs using the MNIST dataset.\n","\n","- The Generater network ($G$) consists of 4 Linear layers;\n","- the Discriminator network ($D$) is a classifier and consists of 4 Linear layers.\n","- Latent vector length: 100\n","\n","Network $G$:\n","1. Layer 1: 100 $\\rightarrow$ 256 (100 is length of latent vector)\n","2. Layer 2: 256 $\\rightarrow$ 512\n","3. Layer 3: 512 $\\rightarrow$ 1024\n","4. Layer 4: 1024 $\\rightarrow$ 784 (size of a MNIST image)\n","\n","Apply [`LeakyReLU`](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html) with (alpha=0.2) activation functions for layers 1-3, and `tanh` to layer 4.\n","\n","\\[**NOTE**: `LeakyReLU` helps to mitigate Vanishing gradient problem\\]\n","\n","<br>\n","\n","Network $D$:\n","1. Layer 1: 784 $\\rightarrow$ 1024 \n","2. Layer 2: 1024 $\\rightarrow$ 512 \n","3. Layer 3: 512 $\\rightarrow$ 256 \n","3. Layer 4: 256 $\\rightarrow$ 1\n","\n","Apply leaky_relu(alpha=0.2) activation functions for layers 1-3, and sigmoid to layer 4. Can also apply low rate dropout (0.3) at layers 1-3.\n","\n","\\[**NOTE**: We want to **label if input is a real image**, NOT to find which number is in the image\\]\n","\n","Note although here we have used the Linear layers for MNIST, for larger image-sized implementations, it usually uses convolutional layers as they are better suited for image problems."],"metadata":{"id":"cJNceeGuzAIu"}},{"cell_type":"code","source":["class Generator(nn.Module):\n","    def __init__(self, g_input_dim=100, g_output_dim=28*28):\n","        super().__init__()       \n","        self.fc1 = ### your code goes here\n","        self.fc2 = ### your code goes here\n","        self.fc3 = ### your code goes here\n","        self.fc4 = ### your code goes here\n","    \n","    # forward method\n","    def forward(self, x): \n","        x = ### your code goes here\n","        x = ### your code goes here\n","        x = ### your code goes here\n","        return ### your code goes here\n","    \n","class Discriminator(nn.Module):\n","    def __init__(self, d_input_dim=28*28):\n","        super().__init__()\n","        self.fc1 = ### your code goes here\n","        self.fc2 = ### your code goes here\n","        self.fc3 = ### your code goes here\n","        self.fc4 = ### your code goes here\n","    \n","    # forward method\n","    def forward(self, x):\n","        x = ### your code goes here\n","        x = ### your code goes here\n","        x = ### your code goes here\n","        x = ### your code goes here\n","        x = ### your code goes here\n","        x = ### your code goes here\n","        return ### your code goes here\n","    \n","\n","# build model\n","G = Generator().to(device)\n","D = Discriminator().to(device)"],"metadata":{"id":"k-LDihNb0MlL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Print $G$ and $D$"],"metadata":{"id":"kfRhiWPWy_2b"}},{"cell_type":"code","source":["G"],"metadata":{"id":"1nfRttXa0RgX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["D"],"metadata":{"id":"vW9fnZRH0R4R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loss and hyper-parameters:\n","- Loss criterion: BCELoss\n","- batch_size = 100\n","- learning_rate = 0.0002\n","- Optimiser: Adam"],"metadata":{"id":"9W6jVyHI0ZZ7"}},{"cell_type":"code","source":["# define loss\n","criterion = ### your code goes here\n","z_dim = 100\n","bs = 100  ## batch_size\n","\n","\n","# optimiser\n","lr = 0.0001 \n","G_optimizer = ### your code goes here\n","D_optimizer = ### your code goes here"],"metadata":{"id":"uaNY8vk60ZLJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MNIST Dataset\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.5), std=(0.5))])\n","\n","train_dataset = MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n","\n","# Data Loader (Input Pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)"],"metadata":{"id":"lx4ZjdKg0ZFV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Comments on the transform `transforms.Normalize(mean=(0.5), std=(0.5))])`**\n","\n","In principle you would like to normalise the dataset properly as we have been doing so far. But if you try these values (mean of MNIST is 0.1307 and it's standard deviation is 0.3081), the GAN does not train properly at all. See [this](https://discuss.pytorch.org/t/gan-training-fails-for-different-image-normalization-constants/10574/4) where someone reports the same behaviour, and a potential solution by adding fancy modifications to the network. This is one of the many reasons why GANs are difficult to train."],"metadata":{"id":"X_YIuaacVHSR"}},{"cell_type":"markdown","source":["#### **Question**:\n","We are not splitting the data now, why? \n","\n","In fact, we did not even bother downloading the test dataset... why? Are we not going to check how well our network **generalises**?\n","\n","<br>\n","\n","<br>"],"metadata":{"id":"yEKEddgO1zQi"}},{"cell_type":"markdown","source":["As we learned, GANs aims to let the generator and the discriminator compete, here are the steps to set up a training workflow in GANs:\n","\n","1. Fix $G$, train $D$ to distinguish between real images $x$ and generated fake images $G(z)$:\n","  * $D(x) \\rightarrow$ *high score*\n","  * $D(G(z)) \\rightarrow$ *low score*\n","\n","2. Fix $D$, train $G$ to \"fool\" $D$:\n","  * sample a vector $z$,\n","  * generate a fake image $G(z)$,\n","  * input $G(z)$ into $D$,\n","  * train $G$ to increase the score of $D(G(z))$ (here $G$ and $D$ can be understood as one combined network, but layers of $D$ are frozen).\n","\n","\\[**NOTE**: If you found one of the losses becomes zero at the first several epochs, it means the training is not robust due to the initialisation (one network outperforms the other and the other cannot catch up anymore). In this case, we need to restart the process.\\]\n","\n","Define a training loop for $D$:"],"metadata":{"id":"mGklEHpb0wja"}},{"cell_type":"code","source":["def D_train(x):\n","    #-------------- Function of the discriminator training -------------------#\n","    D.train()\n","    D_optimizer.zero_grad()\n","\n","    # train discriminator on real data -- assign high score (use 1 here)\n","    x_real, y_real = x.view(-1, 28*28), torch.ones(bs, 1)  # we are assigning the label 'real data' to the samples (don't care anymore about what number they are)\n","    x_real, y_real = x_real.to(device), y_real.to(device)\n","\n","    D_output = ### your code goes here\n","    D_real_loss = ### your code goes here\n","\n","    # train discriminator on fake data -- assign low score (use 0 here)\n","    # sample vector and produce generator output\n","    z = torch.randn(bs, z_dim).to(device)\n","    x_fake, y_fake = G(z), torch.zeros(bs, 1).to(device)\n","\n","    D_output = ### your code goes here\n","    D_fake_loss = ### your code goes here\n","\n","    # combine the losses\n","    D_loss = ### your code goes here\n","\n","    # model update \n","    D_loss.backward()\n","    D_optimizer.step()\n","        \n","    return  D_loss.data.item()  ### deprecated version of loss.detach(), basically gets access to the tensor without the computational graph attached"],"metadata":{"id":"tWPmsUf31dWe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["and for $G$:"],"metadata":{"id":"AO4Hx6Md0weU"}},{"cell_type":"code","source":["def G_train(x):\n","    #-------------- Function of the generator training -------------------#\n","    G.train()\n","    G_optimizer.zero_grad()\n","\n","    # sample vector and produce generator output\n","    z = ### your code goes here\n","    G_output = ### your code goes here\n","\n","    # obtain scores from D for the generated data\n","    D_output = ### your code goes here\n","\n","    # train generator to \"fool\" discriminator\n","    y = torch.ones(bs, 1).to(device)\n","    G_loss = ### your code goes here\n","\n","    # model update \n","    G_loss.backward()\n","    G_optimizer.step()\n","        \n","    return G_loss.data.item()  ### deprecated version of loss.detach(), basically gets access to the tensor without the computational graph attached"],"metadata":{"id":"NBKsW86d0wNR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And loop over epochs to train:"],"metadata":{"id":"K0ySfyMN0wDQ"}},{"cell_type":"code","source":["n_epoch = 200 # about 40 minutes\n","groups = {'Loss': ['D_Loss', 'G_Loss']}\n","liveloss = PlotLosses(groups=groups)\n","\n","for epoch in range(1, n_epoch+1):  \n","  D_losses, G_losses = [], []\n","  logs = {}\n","  for batch_idx, (x, _) in enumerate(train_loader):      # _ is just a throwaway variable that we don't use\n","    logs['D_Loss'] = D_train(x)\n","    logs['G_Loss'] = G_train(x)\n","  liveloss.update(logs)\n","  liveloss.draw()\n","\n","  # save every 20th epochs\n","  if(np.mod(epoch, 20) == 0):\n","    torch.save(G.state_dict(), \"./Generator_{:03d}.pth\".format(epoch))"],"metadata":{"id":"kdeg1DuK1rcS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["where we observe the quintessential behaviour of GANs losses."],"metadata":{"id":"373cqB1ECsHF"}},{"cell_type":"markdown","source":["#### Sample random latent vectors, and input into generator to generate new images."],"metadata":{"id":"ZcGjw8uY0v4i"}},{"cell_type":"code","source":["# from torchvision.utils import save_image\n","set_seed(0)\n","\n","epoch = 20  ## get the model after 20 epochs and use it:\n","G.load_state_dict(torch.load(\"./Generator_{:03d}.pth\".format(epoch)))\n","\n","\n","with torch.no_grad():\n","    test_z = torch.randn(bs, z_dim).to(device)\n","    generated = G(test_z)\n","\n","    # save_image(generated.view(generated.size(0), 1, 28, 28), './sample_' + '.png')\n","fig, axarr = plt.subplots(10, 10, figsize=(12, 12))\n","for ax, img in zip(axarr.flatten(), generated.view(generated.size(0), 28, 28).cpu()):\n","  ax.imshow(img, cmap=\"gray\")\n","plt.title('Epoch = {:03d}'.format(epoch))"],"metadata":{"id":"rl0qRqfn3NPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from torchvision.utils import save_image\n","set_seed(0)\n","\n","epoch = 60\n","G.load_state_dict(torch.load(\"./Generator_{:03d}.pth\".format(epoch)))\n","\n","\n","with torch.no_grad():\n","    test_z = torch.randn(bs, z_dim).to(device)\n","    generated = G(test_z)\n","\n","    # save_image(generated.view(generated.size(0), 1, 28, 28), './sample_' + '.png')\n","fig, axarr = plt.subplots(10, 10, figsize=(12, 12))\n","for ax, img in zip(axarr.flatten(), generated.view(generated.size(0), 28, 28).cpu()):\n","  ax.imshow(img, cmap=\"gray\")\n","plt.title('Epoch = {:03d}'.format(epoch))"],"metadata":{"id":"TKZ-PzRY3NNU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from torchvision.utils import save_image\n","set_seed(0)\n","\n","epoch = 140\n","G.load_state_dict(torch.load(\"./Generator_{:03d}.pth\".format(epoch)))\n","\n","\n","with torch.no_grad():\n","    test_z = torch.randn(bs, z_dim).to(device)\n","    generated = G(test_z)\n","\n","    # save_image(generated.view(generated.size(0), 1, 28, 28), './sample_' + '.png')\n","fig, axarr = plt.subplots(10, 10, figsize=(12, 12))\n","for ax, img in zip(axarr.flatten(), generated.view(generated.size(0), 28, 28).cpu()):\n","  ax.imshow(img, cmap=\"gray\")\n","plt.title('Epoch = {:03d}'.format(epoch))"],"metadata":{"id":"PzX_w7L63NJd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from torchvision.utils import save_image\n","set_seed(0)\n","\n","epoch = 200\n","G.load_state_dict(torch.load(\"./Generator_{:03d}.pth\".format(epoch)))\n","\n","\n","with torch.no_grad():\n","    test_z = torch.randn(bs, z_dim).to(device)\n","    generated = G(test_z)\n","\n","    # save_image(generated.view(generated.size(0), 1, 28, 28), './sample_' + '.png')\n","fig, axarr = plt.subplots(10, 10, figsize=(12, 12))\n","for ax, img in zip(axarr.flatten(), generated.view(generated.size(0), 28, 28).cpu()):\n","  ax.imshow(img, cmap=\"gray\")\n","plt.title('Epoch = {:03d}'.format(epoch))"],"metadata":{"id":"1Vg_Mbo53NGz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare against **VAE** result (using a VAE network with similar complexity):\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1GY4bhQ01eEWGZdx_9oqR0tk7hs_JUGo6\" width=\"500\"/> <i> <br>VAE result </i></center>\n","\n","<br>\n","\n","\n","<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"B-0mPxxG3e-1"}},{"cell_type":"code","source":[],"metadata":{"id":"B86qb1oU3M-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. **Pros** & **Cons**\n","\n","- GANs produce “sharper” and more “perceptually realistic” results\n","\n","<br>\n","\n","- VAEs are stable in training, and converge faster\n","- GANs are hard to train, and have unclear stopping criteria\n","\n","<br>\n","\n","- VAEs provide generative model and inference model in a single network\n","  - Learn an encoder-decoder pair\n","- GANs only provide a generative model\n","\n","<br>\n","\n","- VAEs are better justified theoretically\n","- GANs **implicitly** maximise likelihood through the combination of two networks\n","\n","<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"qCfv3-CAtDYh"}},{"cell_type":"markdown","source":["## 6. Overview of main GAN flavours\n","\n","There are many different variations or flavours based on the original idea of using two competing networks.\n","\n","Based on the objective function (loss) used:\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1ZbbmIsqb_Y8DSew7ynZx2OPE2FRUMp-w\" width=\"700\"/> </center>\n","\n","<br>\n","\n","---\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1hga3rZYlJcJWNho7lsEk4T7nS9zfoMdy\" width=\"700\"/> </center>\n","\n","<br>\n","\n","---\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=12y0Ri1zzTLAb6FWz4BCB55ItZmVkVQsC\" width=\"700\"/> </center>\n","\n","<br>\n","\n","---\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1kjyGES9AnLWwLwlx4FUMw03DzshuHFb9\" width=\"700\"/> </center>\n","\n","<br>\n","\n","---\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1QGp-dmCGLKNdzIcxyJPNPLX0qNIAwB7u\" width=\"700\"/> </center>\n","\n","<br>\n","\n","---\n","\n","\n","\n","Some references for images used in figures above:\n","\n","- [WGANs](https://arxiv.org/pdf/1701.07875.pdf)\n","- [cycle GANs](https://junyanz.github.io/CycleGAN/)\n","- [star GANs](https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf)\n","- [Progressively growing GANs](https://arxiv.org/abs/1710.10196)"],"metadata":{"id":"w79Q-Km3tDPB"}},{"cell_type":"code","source":[],"metadata":{"id":"ilQs7-PPB3ni"},"execution_count":null,"outputs":[]}]}