{"cells":[{"cell_type":"markdown","metadata":{"id":"d9xQUNYrhiHy"},"source":["<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"7KKPP4hWhEb_"},"source":["## **Morning Session 6: Autoencoders and Variational Autoencoders**\n","\n","# Introduction\n","\n","In this session you will implement two simple generative models. The first will be an Autoencoder, and the second a Variational Autoencoder, both will generate handwritten digits from MNIST. \n","\n","The code is based on a resource by Alexander Van de Kleut https://avandekleut.github.io/vae/ who introduces Autoencoders and then Variational Autoencoders (VAE's).\n"]},{"cell_type":"markdown","metadata":{"id":"iy0_k0AljB4x"},"source":["# 0 Preliminaries"]},{"cell_type":"markdown","metadata":{"id":"MI_buJ8ZnbwM"},"source":["## 0.1 Mount Google-drive\n","\n","We start by mounting Google-drive\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AeHI3nr0Vlmu","outputId":"3af2d2f5-f4d7-406c-b846-a78f403f316f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"nVLjd2zshtea"},"source":["## 0.2 Import libraries\n","\n","We will need the following libraries\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmiwE9CdhEsk","outputId":"0b096df5-c11e-413b-9da1-b584b4a4eff3"},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["import torch  # Pytorch\n","import torch.nn as nn  # Neural network module\n","import torch.nn.functional as fn  # Function module\n","from torchvision import datasets  # Datasets from torchvision\n","from torchvision import transforms  # Transforms from torchvision\n","\n","import matplotlib.pyplot as plt  # Plotting using matplotlib\n","import numpy as np  # Numpy\n","\n","device = 'cuda'  # Set out device to GPU\n","\n","print('done')  # Let me know this cell has finished"]},{"cell_type":"markdown","metadata":{"id":"zVk-cLfDIbAp"},"source":["## 0.3 Load MNIST\n","\n","We load the MNIST data-set\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BeWBQhwA_t9Y"},"outputs":[],"source":["# MNIST Test dataset and dataloader declaration\n","batch_size = 128\n","data = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=True, download=True,\n","                   transform=transforms.Compose([transforms.ToTensor(),])\n","                   ),\n","                   batch_size=batch_size, shuffle=True) # Load MNIST. Use the Data Loader to shuffle and batch images\n","\n","images, labels = next(iter(data)) # A trick for getting a batch out of the dataloader object\n","plt.imshow(images[0].squeeze()) # Show the first image from the batch\n","plt.show()\n","\n","print('number of batches = '+str(len(data))) # Print num batches\n","print('number of images = '+str(len(data)*batch_size)) # Print num images\n"]},{"cell_type":"markdown","metadata":{"id":"Wmd7EWXAiz0g"},"source":["# 1 The Autoencoder\n","\n","This tutorial will teach you to build an **Autoencoder** which is capable of generating handwritten digits after training on the MNIST dataset. \n","\n","This week you have seen that Generative Models capture the joint probability distribution of an observed and a latent variable. An Autoencoder is a \"Deep Generative Model\" where an observed variable is transformed to a latent space using an \"Enconder\" neural network, and the latent space is transformed back to the observed variable space using a \"Decoder\" neural network. The following diagram shows this visually\n"]},{"cell_type":"markdown","metadata":{"id":"RNxjcOxXSUMA"},"source":["## 1.1 Architecture\n","\n","![AE_Diagram_nNotation.jpg](https://avandekleut.github.io/assets/vae/autoencoder.png)"]},{"cell_type":"markdown","metadata":{"id":"fEfu0Y8AiT7B"},"source":["## 1.2 Encoder\n","\n","Auto-encoders use an encoder-decoder framework, where the encoder and decoder are neural networks. We will build an Autoencoder where the Encoder network and the Decoder network have 2 fully-connected layers.\n","\n","The architecture of the Encoder is as follows:\n","\n","1. The input layer: 784 nodes (becuase the MNIST images are 28x28 pixels)\n","2. The first layer: 512 nodes\n","3. The second: 256 nodes\n","\n","Use ReLU for your activation function.\n","\n","The outline of the encoder module is below, complete the following exercises\n","\n","---\n","\n","*To learn more about PyTorch neural network modules* https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UYWUD2UCV1rR","outputId":"924088e2-0ef3-4134-85c6-4d1c648f56fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["class Encoder(nn.Module):  # The Encoder inherits the properties of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n","  def __init__(self):\n","    '''\n","    Class contains the Encoder (image -> latent).\n","    '''\n","\n","    super(Encoder, self).__init__()\n","    self.layer0 =   # Image to hidden, fully connected\n","    self.transform0 = \n","    self.layer1 =   # Image to hidden, fully connected\n","    self.transform1 = \n","\n","  def forward(self, x):  # Custom pytorch modules should follow this structure \n","    '''\n","    x: [float] the MNIST image\n","    '''\n","\n","    x =   # Reshape the input into a vector (nD to 1D) \n","    x =   # Run Image through Linear transform then ReLu activation function\n","    x =   # Run Image through Linear transform then ReLu activation function\n","    return x\n","  \n","  print('done') # Show when the module has run"]},{"cell_type":"markdown","metadata":{"id":"oG9RkUtQX5Pn"},"source":["## 1.3 Decoder\n","\n","The decoder will have the same architecture as the decoder but in reverse. Therefore\n","\n","2. The first layer: 256 nodes\n","3. The second layer: 512 nodes\n","4. The output layer will have 784 nodes.\n","\n","Again, use ReLU activations, apart from the output layer which will use a Sigmoid acitvations (because MNIST images pixel valeus are between 0 and 1)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ym17ROhYiyuI","outputId":"b3ace8c4-9029-46a7-cfba-bbe824defe0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["class Decoder(nn.Module):\n","  def __init__(self):\n","    '''\n","    Class contains the Decoder (latent -> image).\n","    '''\n","\n","    super(Decoder, self).__init__()\n","    self.layer1 =   # Connectivity Latent to Hidden\n","    self.activation1 = \n","    self.layerOut =   # Connectivity Hidden to Image\n","    self.activationOut = \n","\n","  def forward(self, z):\n","    '''\n","    z: [float] a sample from the latent variable\n","    '''\n","\n","    z =   # Run Image through Linear transform then ReLu activation function\n","    z =   # Run Image through Linear transform then Sigmoid activation function\n","    return   # Reshape the vector into an image\n","\n","print('done')"]},{"cell_type":"markdown","metadata":{"id":"q60K_brCsofN"},"source":["## 1.4 Autoencoder\n","\n","The Autoencoder structure inherits from the Encoder and the Decoder. This is done by \"dependency injection\" (where the Enconder and Decoder class are assigned to properties of the Autoencoder). The forward module of the Autoencoder should take and image, run the Encoder (image -> latent) then run the Decoder to make a prediction (latent -> generated image).\n","\n","The Autoencoder is below, and the latent space will have variable dimensionality.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ow2gPg6Ds6qU","outputId":"b3b13a46-d792-4148-e4a2-addc438fc0b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["class Autoencoder(nn.Module):\n","  def __init__(self, dims_latent):\n","    '''\n","    Class combines the Encoder and the Decoder with an Autoencoder latent space.\n","\n","    dims_latent: [int] the dimension of (number of nodes in) the mean-field gaussian latent variable\n","    '''\n","\n","    super(Autoencoder, self).__init__()\n","    self.encoder = Encoder()\n","    self.decoder = Decoder()\n","\n","    self.latentIn =   # Hidden to latent, fully connected\n","    self.latentOut =   # Connectivity Latent to Hidden\n","    self.activationOut = \n","\n","  def forward(self, x):\n","    '''\n","    x - [float] A batch of images from the data-loader\n","    '''\n","\n","    x =   # Run the image through the Encoder\n","    z =   # Take the output of the encoder and get the latent vector \n","    z =   # Take the latent vector and make the input for the Decoder\n","    return   # Return the output of the decoder (the predicted image)\n","\n","print('done')"]},{"cell_type":"markdown","metadata":{"id":"4m7Pc5JLiWc4"},"source":["## 1.5 Training\n","\n","We train the Autoencoder by stocastic gradient descent, you have seen this process several times in different forms. My implementation of this process is below:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wN4ylzq9uYuj","outputId":"74f932e8-528e-495a-bc84-62da1cf1d362"},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["def train(autoencoder, data, kl_div_on=True, epochs=10):\n","  opt = torch.optim.Adam(autoencoder.parameters())\n","  for epoch in range(epochs):  # Run data over numerous epochs\n","    for :  # Iterate over the batches of images and labels\n","      batch =  # Send batch of images to the GPU\n","        # Set optimiser grad to 0\n","      x_hat =  # Generate predicted images (x_hat) by running batch of images through autoencoder\n","      loss =   # Calculate L2 loss\n","        # Back-propagate\n","        # Step the optimiser\n","  return autoencoder  # Return the trained autoencoder (for later analysis)\n","\n","\n","dims_latent = 2  # Maybe increase this an try the t-sne algorithm for visualisation?!\n","autoencoder = \n","autoencoder = train(autoencoder, data, 10)\n","\n","print('done')"]},{"cell_type":"markdown","metadata":{"id":"7RIUDk7THkE7"},"source":["## 1.6 Latent space visualisation\n","\n","Have a look at the latent space before and after training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73SYUGLcutdA"},"outputs":[],"source":["def plot_2D_latent_space(autoencoder, data, num_batches=100):\n","    '''\n","    Plot a the latent vector for a lots of images\n","    '''\n","    for n , (x, y) in enumerate(data):  # enumerate means n = n + 1 per iteration. x is the batch of images, y are the class labels\n","        z =  # Run the batch through the Encoder and Latent input layer\n","        z =   # Send latent vector to cpu\n","        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10')  # Scatter plot of latent vector colourised by the class label y\n","        if n > num_batches:  # stop after n > maximum number of batches\n","          plt.colorbar()\n","          break\n","\n","def plot_2D_latent_sample(autoencoder, image):\n","    '''\n","    Plot a the latent vector for a single image\n","    '''\n","    z =  # Run the image through the Encoder and Latent input layer\n","    z =  # Send latent vector to cpu\n","    plt.scatter(z[:, 0], z[:, 1], marker='x', c='k')  # Scatter plot of latent vector\n","\n","plot_2D_latent_space(autoencoder, data)\n","plt.show()\n","\n","x,y = next(iter(data)) # Get next iteration from data iterator\n","print(y)\n","\n","n = 0  # Choose a label\n","print(y[n]) # Print out chose label\n","\n","image = x[n]\n","plt.figure()\n","plot_2D_latent_space(autoencoder, data)\n","plot_2D_latent_sample(autoencoder, image)"]},{"cell_type":"markdown","metadata":{"id":"q73FYqU-H4Ke"},"source":["## 1.7 Generate new images\n","\n","We need to see what our generative model can do! Lets take some random samples from the latent space and see what our model produces."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rlWG-J9wT7aZ"},"outputs":[],"source":["def plot_sample(autoencoder, z):\n","    img = # turn z into image data\n","    plt.imshow(img)\n","    plt.show()\n","    print(z)\n","\n","z = torch.tensor([-10, 5]).to(device).float()\n","plot_sample(autoencoder, z)"]},{"cell_type":"markdown","metadata":{"id":"rl7GedbxIGpT"},"source":["## 1.8 Generate interpolation grids \n","\n","Looking at random samples is all very well, what we really want to see is whether the labels in the latent space acutally generate what we expect. Plot a grid showing the outputs of different parts of the latent space. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qysv18AVH9A3"},"outputs":[],"source":["def plot_grid(autoencoder, n=(10, 10), param_range=(-3, 3)):\n","  x_range = torch.linspace(param_range[0], param_range[1], n[0])\n","  y_range = torch.linspace(param_range[0], param_range[1], n[1])\n","  grid_y, grid_x = torch.meshgrid(x_range, y_range)\n","  \n","  z = torch.zeros(2,n[0]*n[1])\n","  z[1] = grid_x.reshape(n[0]*n[1],1).squeeze()\n","  z[0] = grid_y.reshape(1,n[0]*n[1]).permute(1,0).flip(1).squeeze()\n","  z = z.to(device)\n","\n","  fig, axarr = plt.subplots(n[0], n[1], figsize=(10, 10))\n","  im_s = np.zeros((n[0]*n[1], 28**2))\n","  for n in range(0,n[0]*n[1]):\n","    decoder_input = autoencoder.activationOut(autoencoder.latentOut(z[:,n]))\n","    im_s[-n-1,:] = autoencoder.decoder(decoder_input).flatten().to('cpu').detach().numpy()\n","\n","  for ax, img in zip(axarr.flatten(), im_s):\n","    ax.imshow(img.reshape(28,28), cmap=\"gray\")\n","\n","plot_grid(autoencoder, param_range=(-50, 50))"]},{"cell_type":"markdown","metadata":{"id":"izhIvsHslOtj"},"source":["# 2 The Variational Autoencoder\n","\n","This tutorial will teach you to build a **Variational Autoencoder** (VAE) which is capable of generating handwritten digits after training on the MNIST dataset. \n","\n","In the Theory session you learnt that VAE's and Autoencoders are identical in structure apart from in their latent space, which we force to be a distribution of our choosing. The following diagram shows the VAE which also uses the reparameterisation trick, which is the most common approach to enable VAE training. An alternative approach **does not** use the reparameterisation trick and the set-up will be identical to the autoencoder above, but in this case the gradient of the loss-function should use the log-derivative trick."]},{"cell_type":"markdown","metadata":{"id":"TfYavbsJHSRJ"},"source":["## 2.1 Architecture\n","\n","![VAE_Diagram.jpg](https://avandekleut.github.io/assets/vae/variational-autoencoder.png)"]},{"cell_type":"markdown","metadata":{"id":"QfIdkB0Fm9bH"},"source":["## 2.2 Encoder \n","\n","The set-up of the Encoder will be identical to the Autoencoder architecture\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zqbpHxTom8Vl"},"outputs":[],"source":["class VAE_Encoder(nn.Module):  # The Encoder inherits the properties of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n","  def __init__(self):\n","    '''\n","    Class contains the Encoder (image -> latent).\n","    '''\n","\n","    super(VAE_Encoder, self).__init__()\n","    self.layer0 = nn.Linear(784, 512)  # Image to hidden, fully connected\n","    self.transform0 = nn.ReLU()\n","    self.layer1 = nn.Linear(512, 256)  # Image to hidden, fully connected\n","    self.transform1 = nn.ReLU()\n","\n","  def forward(self, x):  # Custom pytorch modules should follow this structure \n","    '''\n","    x: [float] the MNIST image\n","    '''\n","\n","    x = torch.flatten(x, start_dim = 1)  # Reshape the input into a vector (nD to 1D) \n","    x = self.transform0(self.layer0(x))  # Run Image through Linear transform then ReLu activation function\n","    x = self.transform1(self.layer1(x))  # Run Image through Linear transform then ReLu activation function\n","    return x\n"]},{"cell_type":"markdown","metadata":{"id":"g77zVB-snR3H"},"source":["## 2.3 Decoder \n","\n","The set-up of the Encoder will be identical to the Autoencoder architecture\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlEha_-KoAe_","outputId":"af407252-cc02-46a3-eae0-65c5f8ccaecb"},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["class VAE_Decoder(nn.Module):\n","  def __init__(self):\n","    '''\n","    Class contains the Decoder (latent -> image).\n","    '''\n","\n","    super(VAE_Decoder, self).__init__()\n","    self.layer1 = nn.Linear(256, 512)  # Connectivity Latent to Hidden\n","    self.activation1 = nn.ReLU()\n","    self.layerOut = nn.Linear(512, 784)  # Connectivity Hidden to Image\n","    self.activationOut = nn.Sigmoid()\n","\n","  def forward(self, z):\n","    '''\n","    z: [float] a sample from the latent variable\n","    '''\n","\n","    z = self.activation1(self.layer1(z))  # Run Image through Linear transform then ReLu activation function\n","    z = self.activationOut(self.layerOut(z))  # Run Image through Linear transform then Sigmoid activation function\n","    return  z.reshape((-1,1,28,28))  # Reshape the vector into an image\n","\n","print('done')"]},{"cell_type":"markdown","metadata":{"id":"l65mZcZ8oHRy"},"source":["## 2.4 Variational Autoencoder\n","\n","The VAE structure also inherits from the Encoder and the Decoder. The forward module of the VAE should take and image, run the encoder (image -> latent) then run the decoder to make a prediction (latent -> generated image).\n","\n","The VAE is below, and the latent space will have variable dimensionality. We will use the Reparameterisation trick in our VAE.\n","\n","The loss functions of VAE:\n","* latent space KL divergence: $\\mathbb{KL}\\left( \\mathcal{N}(\\mu, \\sigma^2 \\mathbf{\\mathit I}) \\parallel \\mathcal{N}(\\mathit 0, \\mathbf{\\mathit I}) \\right) = \\sum_{x \\in X} \\left( \\sigma^2 + \\mu^2 - \\log \\sigma - \\frac{1}{2} \\right)\n","$\n","* $L_2$ for generation loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sipeHxFAQ9to","outputId":"98570f02-c235-4b73-d99a-0191b8d473a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["class VariationalAutoencoder(nn.Module):\n","  def __init__(self, dims_latent):\n","    '''\n","    Class combines the Encoder and the Decoder.\n","\n","    dims_latent: [int] the dimension of (number of nodes in) the mean-field gaussian latent variable\n","    '''\n","    \n","    super(VariationalAutoencoder, self).__init__()\n","    self.encoder = VAE_Encoder()\n","    self.decoder = VAE_Decoder()\n","\n","    self.layerMu = nn.Linear(256, dims_latent)  # Hidden to latent, fully connected\n","    self.layerSig = nn.Linear(256, dims_latent)  # Hidden to latent, fully connected\n","    self.distribution =   # Sample epsilon from N(0,1)\n","\n","    self.latentOut = nn.Linear(dims_latent, 256)  # Connectivity Latent to Hidden\n","    self.activationOut = nn.ReLU()\n","\n","\n","  def vae_latent_space(self, x):\n","    mu =   # Turn the output of the Encoder into Mu\n","    sigma =  # Exponential activation ensures positivity for Sigma\n","    z =  # z = mu + sigma * epsilon\n","    kl_div =  # latent space loss -- KL_divergence\n","    return z, kl_div\n","\n","  def forward(self, x):\n","    x = self.encoder(x)\n","    z, kl_div = self.vae_latent_space(x)\n","    z = self.activationOut(self.latentOut(z))  # Take the latent vector and make the input for the Decoder\n","    return self.decoder(z), kl_div\n","\n","print('done')"]},{"cell_type":"markdown","metadata":{"id":"1GbvzByjooPQ"},"source":["## 2.5 Training\n","\n","Once we have a VAE which contains the re-parameterised latent space, we can train as previous."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gfnxl-HuoiWH","outputId":"72e14c9f-a537-4117-8904-990314250033"},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["def train(autoencoder, data, kl_div_on=True, epochs=10):\n","  opt = torch.optim.Adam(autoencoder.parameters())\n","  for epoch in range(epochs):  # Run data over numerous epochs\n","    for batch, label in data:  # Iterate over the batches of images and labels\n","      batch = batch.to(device)  # Send batch of images to the GPU\n","      opt.zero_grad()  # Set optimiser grad to 0\n","      # Generate predicted images (x_hat) by running batch of images through autoencoder; also generates KL div\n","      loss =  # Calculate combined loss (L2 + KL)\n","      loss.backward()  # Back-propagate\n","      opt.step()  # Step the optimiser\n","  return autoencoder  # Return the trained autoencoder (for later analysis)\n","\n","\n","dims_latent = 2  # Maybe increase this an try the t-sne algorithm for visualisation?!\n","VAE = VariationalAutoencoder(dims_latent).to(device)\n","VAE = train(VAE, data, 10)\n","\n","print('done')"]},{"cell_type":"markdown","metadata":{"id":"pEiPy1cHqC-C"},"source":["## 2.6 Latent space visualisation\n","\n","Have a look at the latent space before and after training (a one tweak is needed).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MualgAgAqPuJ"},"outputs":[],"source":["def plot_2D_latent_space(autoencoder, data, num_batches=100):\n","    '''\n","    Plot a the latent vector for a lots of images\n","    '''\n","    for n , (x, y) in enumerate(data):  # enumerate means n = n + 1 per iteration. x is the batch of images, y are the class labels\n","        z, KL = autoencoder.vae_latent_space(autoencoder.encoder(x.to(device)))  # Run the batch through the Encoder and Latent input layer\n","        z = z.to('cpu').detach().numpy()  # Send latent vector to cpu\n","        plt.scatter(z[:, 0], z[:, 1], c=y, cmap='tab10')  # Scatter plot of latent vector colourised by the class label y\n","        if n > num_batches:  # stop after n > maximum number of batches\n","          plt.colorbar()\n","          break\n","\n","def plot_2D_latent_sample(autoencoder, image):\n","    '''\n","    Plot a the latent vector for a single image\n","    '''\n","    z, KL = autoencoder.vae_latent_space(autoencoder.encoder(image.to(device)))  # Run the image through the Encoder and Latent input layer\n","    z = z.to('cpu').detach().numpy()  # Send latent vector to cpu\n","    plt.scatter(z[:, 0], z[:, 1], marker='x', c='k')  # Scatter plot of latent vector\n","\n","plot_2D_latent_space(VAE, data)\n","plt.show()\n","\n","x,y = next(iter(data)) # Get next iteration from data iterator\n","print(y)\n","\n","n = 0  # Choose a label\n","print(y[n]) # Print out chose label\n","\n","image = x[n]\n","plt.figure()\n","plot_2D_latent_space(VAE, data)\n","plot_2D_latent_sample(VAE, image)"]},{"cell_type":"markdown","metadata":{"id":"5HKAur45uHcD"},"source":["## 2.7 Generate new images\n","\n","Now we can generate new images BUT we can also determine the likelihood of a given image by sampling from the standard normal distribution $z_i \\sim \\mathcal{N}(0, \\mathbf{I})$!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1bPj90BudJP"},"outputs":[],"source":["def plot_sample(autoencoder, z):\n","    img = autoencoder.decoder(autoencoder.activationOut(autoencoder.latentOut(z))).to('cpu').detach().numpy()[0,0]\n","    plt.imshow(img)\n","    plt.show()\n","    plt.savefig(\"./generated_image.jpg\")  # Save the figure to google drive!\n","    print(z)\n","\n","z = torch.randn(2).to(device).float()\n","plot_sample(VAE, z)"]},{"cell_type":"markdown","metadata":{"id":"lKoZ2hHHuw-l"},"source":["# 3 Other learning resources\n","\n","https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html\n","\n","https://arxiv.org/abs/1606.05908\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f4IsVjzHn9a0"},"source":["# 4 Resources worth reading\n","\n","**Loss functions for computer vision**\n","https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a\n","\n","**Conditional Variational Autoencoders** These are mentioned in the Theory presentation https://wiseodd.github.io/techblog/2016/12/17/conditional-vae/ (Note, I think the code uses tensorflow)\n","\n","**Normalising flows** are similar to VAE's but each layer has the same dimensionality as the input and uses the change of variables equation https://arxiv.org/pdf/1606.04934.pdf\n","\n","**Modern architectures**\n","\n","Autoregressive models (PixelCNN) https://bjlkeng.github.io/posts/pixelcnn/\n","\n","Nouveau VAE https://arxiv.org/abs/2007.03898\n","\n","Very deep VAE's  https://arxiv.org/pdf/2011.10650.pdf\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}