{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNy+n9qx2jITvy0DEcoBF93"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n","\n","\n","---\n"],"metadata":{"id":"9YehS8enAmDn"}},{"cell_type":"markdown","source":["# **CNNs: convolutional neural networks (part 2)**\n","\n","#### **Morning contents/agenda**\n","\n","1. Commonly used datasets in computer vision\n","\n","2. Important CNN architectures\n","\n","3. U-nets and upsampling (unpooling & transpose convolutions)\n","\n","4. Transfer learning\n","\n","5. Summary of CNNs\n","\n","#### **Learning outcomes**\n","\n","1. Awareness of well-established CNN architectures\n","\n","2. Undersand how to upsample data\n","\n","3. Understand how and why transfer learning is used\n","\n","#### **Afternoon contents/agenda**\n","\n","1. Inspection of CNN filters\n","\n","2. Transfer learning from ImageNet to Bees and Ants\n","\n","#### **Learning outcomes**\n","\n","1. Become familiar with the effect that filters have (sometimes you can interpret them, sometimes they have abstracted the data too far to develop intuitions)\n","\n","2. Hands-on knowledge on how to apply transfer learning\n","\n","\n","<br/>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"BV8Up_Q3Z5La"}},{"cell_type":"code","source":["!pip install pycm livelossplot\n","%pylab inline\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from livelossplot import PlotLosses\n","from pycm import *\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.datasets\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","from torchsummary import summary\n","\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_kYki018eTFJ","executionInfo":{"status":"ok","timestamp":1669557984803,"user_tz":0,"elapsed":19824,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"}},"outputId":"94edf3c9-2fcf-4ba8-ef4f-3f89ac337768"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pycm\n","  Downloading pycm-3.6-py2.py3-none-any.whl (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 454 kB/s \n","\u001b[?25hCollecting livelossplot\n","  Downloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from pycm) (1.21.6)\n","Collecting art>=1.8\n","  Downloading art-5.8-py2.py3-none-any.whl (595 kB)\n","\u001b[K     |████████████████████████████████| 595 kB 36.3 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n","Requirement already satisfied: ipython==7.* in /usr/local/lib/python3.7/dist-packages (from livelossplot) (7.9.0)\n","Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.3)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (57.4.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (5.1.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.4.2)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (2.0.10)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 45.3 MB/s \n","\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (2.6.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.2.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.7.5)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython==7.*->livelossplot) (0.8.3)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.*->livelossplot) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.*->livelossplot) (0.2.5)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (4.1.1)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (6.0)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (21.3)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (6.0.4)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->livelossplot) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.11.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython==7.*->livelossplot) (0.7.0)\n","Installing collected packages: jedi, art, pycm, livelossplot\n","Successfully installed art-5.8 jedi-0.18.2 livelossplot-0.5.5 pycm-3.6\n","Populating the interactive namespace from numpy and matplotlib\n"]}]},{"cell_type":"code","source":["def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LI8sNA9feT3H","executionInfo":{"status":"ok","timestamp":1669557984804,"user_tz":0,"elapsed":14,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"}},"outputId":"b625cfb2-e224-4dea-e861-2ac576dab611"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cuda installed! Running on GPU!\n"]}]},{"cell_type":"markdown","source":["## 1. Inspection of CNN filters\n","\n","Pytorch provides users with a rich set of pre-trained neural network architectures. These have mostly been pre-trained on imagenet.   \n","[```torchvision.models```](https://pytorch.org/vision/stable/models.html) provides us with an interface to these pretrained deep neural networks.\n","\n","- Load a pretrained [AlexNet](https://arxiv.org/abs/1404.5997) model from ```torchvision.models``` ([Source Code](https://pytorch.org/vision/stable/_modules/torchvision/models/alexnet.html) for AlexNet in Pytorch).\n","\n","- Obtain the weight kernels of the first and second convolutional layers and display them.\n","\n","- Do it again for one (or more) layer of a ResNet18 model and/or any other model you like."],"metadata":{"id":"sinIebTtMUvs"}},{"cell_type":"code","source":["from torchvision import models\n","from torchsummary import summary\n","\n","alexnet = ### instantiate a pretrained alexnet\n","### print alexnet\n","### print a summary of alexnet"],"metadata":{"id":"pskzXya6gUN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["first_layer = ### extract the features of the first layer\n","### print the first layer\n","weights = ### extract weights from the first layer\n","\n","# Normalisation for plotting because imshow does not like negative values\n","min_w, max_w =  weights.min(), weights.max()\n","weights -= min_w\n","weights /= (max_w-min_w)\n","\n","# plot the filters:\n","fig, axarr = plt.subplots(8, 8, figsize=(12, 12))\n","axarr = axarr.flatten()\n","for ax, kernel in zip(axarr, weights.cpu().numpy()):  # need to send the weights to the cpu for plotting\n","  ax.imshow(np.swapaxes(kernel, 0, 2))"],"metadata":{"id":"UDXRepBaEI2O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["secondconv_layer = ### extract the features of the second convolutional layer\n","### print the layer\n","weights = ### extract weights\n","print(weights.shape)  ### check it has the shape you expect\n","\n","# Normalisation for plotting because imshow does not like negative values\n","min_w, max_w =  weights.min(), weights.max()\n","weights -= min_w\n","weights /= (max_w-min_w)\n","\n","fig, axarr = plt.subplots(12, 16, figsize=(12, 12))\n","axarr = axarr.flatten()\n","filter_chan = ### define which channel you want to plot\n","print(\"\")\n","print(\"filters: \", filter_chan)\n","print(\"\")\n","for ax, kernel in zip(axarr, weights[:,filter_chan,:,:].cpu().numpy()):\n","  ax.imshow(kernel)#np.swapaxes(kernel, 0, 2))\n","  ax.set_axis_off()"],"metadata":{"id":"p7U9nKHSERCQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And now for the `resnet`:"],"metadata":{"id":"R3e8Cnb4Fb1x"}},{"cell_type":"code","source":["resnet = ### get an instance of a pre-trained resnet\n","### print it\n","### print the summary"],"metadata":{"id":"nb_S_g2NFV2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["first_layer = ### extract first layer features\n","### print first layer\n","weights = ### extract weights\n","print(weights.shape) ### print shape of weights\n","\n","# Normalisation for plotting because imshow does not like negative values\n","min_w, max_w =  weights.min(), weights.max()\n","weights -= min_w\n","weights /= (max_w-min_w)\n","\n","print('shape of weights: ', weights.shape)\n","\n","fig, axarr = plt.subplots(8, 8, figsize=(12, 12))\n","axarr = axarr.flatten()\n","\n","for ax, kernel in zip(axarr, weights.cpu().numpy()):   # plot the three channels at the same\n","  ax.imshow(np.swapaxes(kernel, 0, 2))                 # time as an RGB image\n","  # ax.imshow(kernel)                                  # imshow does not like this shape (that's why we swap the axes in the line above)\n","\n","\n","# for ax, kernel in zip(axarr, weights[:,1,:,:].cpu().numpy()):  # plot one channel at a time\n","  # ax.imshow(kernel)                                            # (second dimension of the tensor)"],"metadata":{"id":"2adT4ZkBFZAa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also try the `inception_v3` model:"],"metadata":{"id":"izEN3IenIfa5"}},{"cell_type":"code","source":["### your code goes here"],"metadata":{"id":"djyZjduDHDR4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### your code goes here"],"metadata":{"id":"ehqpXgjtHnv-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Transfer learning from ImageNet to Bees and Ants\n","\\[*adapted from [this PyTorch tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)*\\]\n","\n","The basic principle behind transfer learning is to leverage features learned on one dataset and recycle them to perform tasks on another dataset. \n","\n","Instead of training a network from randomly initialised weights, we start from a network with weights trained in a different domain and fine-tune it to a different source task. By doing this, the weights of the network already have useful properties that can improve optimisation.\n","\n","To be able to apply transfer learning effectively, the data distribution of the data that a very powerful model was trained on should follow a similar distribution as the other dataset that we are trying to apply transfer learning to.    \n","\n","For example:  \n","_We want to create a new classifier for cats and dogs given only a small set of say 100 training images of each category._\n","\n","Large neural networks that have been trained on ImageNet or CIFAR have similar categories in their dataset, say horses and maybe cows and many more categories of natural images.  \n","\n","The rationale is that since we've already learned a set of efficient filters on ImageNet, we can simply use a deep network as a feature extractor and only retrain the final layer —or alternatively fine tune all layers- of a given network to adapt it to our particular task\n","\n","\n","Transfer learning is a powerful tool because it can:\n","- prevent poor training from random weights when only small datasets are available.\n","- reduce training time for tasks that use data with 'similar' distributions\n","\n","<br>\n","\n","We will apply transfer learning to a small dataset containing images of bees and ants by transfering learning done by training with the ImageNet dataset.\n","\n","ImageNet is arguably the most popular dataset for benchmarking classification models. It contains around 14 million annotated natural images spread over 22 thousand categories. Images are of size 3 x 224 x 224, with normalised means and stds of [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225]. In transfer learning is common practice to use the means and standard deviations of the data used for pretraining to normalise the new dataset. Note that the most popular networks (VGG, ResNet, AlexNet, etc..) have been design to take as input 3 x 224 x 224 images to accomodate for ImageNet.\n","\n","<br>\n","\n","<p style=\"text-align:center;\"><img src=\"https://paperswithcode.com/media/datasets/ImageNet-0000000008-f2e87edd_Y0fT5zg.jpg\" alt=\"drawing\" width=\"600\"/>\n","<i>ImageNet samples</i>\n","</p>\n","\n","<br>\n","\n","\n","\n","Perform the following tasks:\n","\n","0. Adapt the training, validation and evaluation functions to the appropriate input size\n","\n","1. [Download](https://download.pytorch.org/tutorial/hymenoptera_data.zip) the dataset into your current directory (you can do it using the code provided below). Use [```torchvision.datasets.ImageFolder```](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html) to load the training and validation data into a Pytorch ```Dataset```. Investigate and visualize a few examples of the dataset. Remember to transform the data into a tensor. What other pre-processing is required?\n","\n","2. Instantiate an untrained ResNet18 from [```torchvision.models```](https://pytorch.org/vision/stable/models.html). Reinitialise the last connected layer ```model.fc``` with a new fully connected layer with appropriate input and output sizes\n","\n","3. Train the newly initialised ResNet from scratch. What do you notice?\n","\n","4. Now instantiate the pre-trained ReseNet18 by passing the argument ``pretrained=True`` and perform fine-tuning using a smaller learning rate of ```1e-3```\n","\n","5. Use the provided ``set_parameters_requires_grad`` and ``get_params_to_update`` functions to repeat the step above but now freezing optimisation for all layers except the final classifying layer (instead of retraining the whole network with a smaller learning rate).\n","\n","6. Finally, train a ResNet on MNIST from scratch and use those weights to repeat step 5 on the bees and ants dataset. Is this a suitable problem?"],"metadata":{"id":"rfOjp9orI4g3"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TMw6UvW2JdQt","executionInfo":{"status":"ok","timestamp":1669558075097,"user_tz":0,"elapsed":44945,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"}},"outputId":"b7013d2a-6008-429d-d073-58dfc3f83ca8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"markdown","metadata":{"id":"zcRkTyYxxIA9"},"source":["### 2.0 Adapting training, validation and evaluation functions to our problem size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKoKWyIgwZQ2"},"outputs":[],"source":["def train(model, optimizer, criterion, data_loader):\n","    model.train()\n","    train_loss, train_accuracy = 0, 0\n","    for X, y in data_loader:\n","        X, y = X.to(device), y.to(device)\n","        optimizer.zero_grad()\n","        a2 = model(X.view(### adapt size here))\n","        loss = criterion(a2, y)\n","        loss.backward()\n","        train_loss += loss*X.size(0)\n","        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n","        optimizer.step()  \n","        \n","    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)\n","  \n","def validate(model, criterion, data_loader):\n","    model.eval()\n","    validation_loss, validation_accuracy = 0., 0.\n","    for X, y in data_loader:\n","        with torch.no_grad():\n","            X, y = X.to(device), y.to(device)\n","            a2 = model(X.view(### adapt size here))\n","            loss = criterion(a2, y)\n","            validation_loss += loss*X.size(0)\n","            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n","            \n","    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)\n","  \n","def evaluate(model, data_loader):\n","    model.eval()\n","    ys, y_preds = [], []\n","    for X, y in data_loader:\n","        with torch.no_grad():\n","            X, y = X.to(device), y.to(device)\n","            a2 = model(X.view(### adapt size here))\n","            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","            ys.append(y.cpu().numpy())\n","            y_preds.append(y_pred.cpu().numpy())\n","            \n","    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)"]},{"cell_type":"markdown","metadata":{"id":"-dAUDSUqtp9r"},"source":["### 2.1 Loading and Visualising the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpqBDWTegydz"},"outputs":[],"source":["!wget -nc https://download.pytorch.org/tutorial/hymenoptera_data.zip && unzip -oq hymenoptera_data.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TeHS9U038ms_"},"outputs":[],"source":["seed = 42\n","lr = 1e-2\n","momentum = 0.9\n","batch_size = 64\n","test_batch_size = 1000\n","n_epochs = 30"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ClWBbBL1iNke"},"outputs":[],"source":["from torchvision import datasets, transforms, models\n","\n","transform = transforms.Compose([\n","        transforms.ToTensor(),\n","    ])\n","\n","train_ds = datasets.ImageFolder(\"./hymenoptera_data/train\", transform=transform)\n","test_ds = datasets.ImageFolder(\"./hymenoptera_data/val\", transform=transform)\n","\n","print(train_ds)\n","print(train_ds.classes)\n","print(train_ds.class_to_idx)\n","print(train_ds[0]) # an example of calling  __getitem__, which is what the dataloader does\n","print(train_ds.samples[0]) # get image path inside samples\n","print(\"\\n\\n\")\n","\n","# Get mean and std\n","tmp_loader = DataLoader(train_ds, batch_size=1, num_workers=0)\n","data = next(iter(tmp_loader))\n","mean = [torch.mean(data[0][0][i].flatten()).item() for i in range(3)]\n","std = [torch.std(data[0][0][i].flatten()).item() for i in range(3)]\n","print(mean, std)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6rg29zjqkb1l"},"outputs":[],"source":["def show_batch(dataset, nr=4, nc=4):\n","  fig, axarr = plt.subplots(nr, nc, figsize=(10, 10))\n","  for i in range(nr):\n","      for j in range(nc):\n","          idx = random.randint(0, len(train_ds))\n","          sample, target = train_ds[idx]\n","          try:\n","            axarr[i][j].imshow(sample) # if PIL\n","          except:\n","            axarr[i][j].imshow(sample.permute(1,2,0)) # if tensor of shape CHW\n","          target_name = train_ds.classes[target]\n","          axarr[i][j].set_title(\"%s (%i)\"%(target_name, target))\n","\n","  fig.tight_layout(pad=1.5)\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3IffdzCmaTQ"},"outputs":[],"source":["show_batch(train_ds, 5, 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DLYvyI3mlYj"},"outputs":[],"source":["# Fix image sizes with transforms\n","train_transform = transforms.Compose([\n","        ### add a transform that does a random crop with the right size here\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor()\n","    ])\n","test_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","    ])\n","\n","train_ds = datasets.ImageFolder(\"./hymenoptera_data/train\", transform=train_transform)\n","test_ds = datasets.ImageFolder(\"./hymenoptera_data/val\", transform=test_transform)\n","\n","show_batch(train_ds, 5, 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSHLIl2Vjir6"},"outputs":[],"source":["# Finally add normalisation to transforms\n","train_transform = transforms.Compose([\n","        ### add a transform that does a random crop with the right size here\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std),\n","    ])\n","test_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std),\n","    ])\n","\n","train_ds = ### create a training ImageFolder\n","test_ds =  ### create a testing ImageFolder\n","\n","# Create dataloader\n","train_loader = ### create a dataloader for the training data\n","test_loader =  ### create a dataloader for the testing data"]},{"cell_type":"markdown","metadata":{"id":"38oqk6JAgcAH"},"source":["### 2.2 Training a newly initialized Resnet18\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OjS87IIyiW4_"},"outputs":[],"source":["set_seed(seed)\n","\n","model = models.resnet18().to(device)\n","print(model)\n","model.fc = nn.Linear(model.fc.in_features, 2).to(device)  ### this line is provided, but what does it do?\n","\n","optimizer = ### define your optimiser (SGC with momentum)\n","criterion = ### define your loss\n","\n","liveloss = PlotLosses()\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy.item()\n","\n","    validation_loss, validation_accuracy = validate(model, criterion, test_loader)\n","    logs['val_' + 'log loss'] = validation_loss.item()\n","    logs['val_' + 'accuracy'] = validation_accuracy.item()\n","\n","    liveloss.update(logs)\n","    liveloss.draw()\n","\n","test_loss, test_accuracy = validate(model, criterion, test_loader)    \n","print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy.item())\n","print(\"\")\n","\n","model_save_name = 'resnet18_bees_and_ants_classifier_full_training_set_baseline.pt'\n","path = F\"/content/gdrive/My Drive/models/{model_save_name}\" \n","torch.save(model.state_dict(), path)"]},{"cell_type":"markdown","metadata":{"id":"P_-rX8y8xShf"},"source":["Our model is not doing too well. The lack of data is taking a toll on our training and giving a large generalisation error."]},{"cell_type":"markdown","metadata":{"id":"xRTvtY_wzYS5"},"source":["### 2.3 Finetuning a pre-trained Resenet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dw6iEkZDzcrX"},"outputs":[],"source":["set_seed(seed)\n","\n","train_transform = transforms.Compose([\n","        ### add a transform that does a random crop with the right size here\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ])\n","test_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ])\n","\n","train_ds = datasets.ImageFolder(\"./hymenoptera_data/train\", transform=train_transform)\n","test_ds = datasets.ImageFolder(\"./hymenoptera_data/val\", transform=test_transform)\n","\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n","\n","model = models.resnet18(pretrained=True).to(device)\n","model.fc = nn.Linear(model.fc.in_features, 2).to(device)\n","\n","optimizer = ### define your optimiser (SGC with momentum) and make sure to adapt the learning rate to 'fine-tune' the network\n","criterion = ### define your loss\n","\n","liveloss = PlotLosses()\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy.item()\n","\n","    validation_loss, validation_accuracy = validate(model, criterion, test_loader)\n","    logs['val_' + 'log loss'] = validation_loss.item()\n","    logs['val_' + 'accuracy'] = validation_accuracy.item()\n","\n","    liveloss.update(logs)\n","    liveloss.draw()\n","    \n","test_loss, test_accuracy = validate(model, criterion, test_loader)    \n","print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy.item())\n","print(\"\")\n","\n","model_save_name = 'resnet18_bees_and_ants_classifier_full_training_set_imagenet_finetune.pt'\n","path = F\"/content/gdrive/My Drive/models/{model_save_name}\" \n","torch.save(model.state_dict(), path)"]},{"cell_type":"markdown","source":["This is much better now."],"metadata":{"id":"3Li-0gq5QfKf"}},{"cell_type":"markdown","metadata":{"id":"0AeBUMRFj5H5"},"source":["### 2.4 Pre-trained Resenet as a Feature Extraction Tool\n","\n","Use the following provided functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-e4S1a6yV-d"},"outputs":[],"source":["def set_parameter_requires_grad(model, requires_grad=False):\n","    \"\"\"https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\"\"\"\n","    for param in model.parameters():\n","        param.requires_grad = requires_grad\n","    return None\n","\n","def get_params_to_update(model):\n","    \"\"\" Returns list of model parameters that have required_grad=True\"\"\"\n","    params_to_update = []\n","    for name,param in model.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","    return params_to_update"]},{"cell_type":"markdown","source":["to adapt the training to only update the last layer of the network"],"metadata":{"id":"BuedxsJTjpMr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHyiZZiCi7BE"},"outputs":[],"source":["set_seed(seed)\n","\n","train_transform = transforms.Compose([\n","        ### add a transform that does a random crop with the right size here\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","test_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","\n","train_ds = datasets.ImageFolder(\"./hymenoptera_data/train\", transform=train_transform)\n","test_ds = datasets.ImageFolder(\"./hymenoptera_data/val\", transform=test_transform)\n","\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n","\n","model = ### get an instance of a pre-trained resnet-18\n","### use the provdied set_parameter_requires_grad to disable training\n","model.fc = ### add a last layer of the network that you need. Newly initialised layers automatically have requires_grad=True\n","\n","optimizer = ### define optimiser and use the provided get_params_to_update function to tell it to only update what you want\n","criterion = nn.CrossEntropyLoss()\n","\n","liveloss = PlotLosses()\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy.item()\n","\n","    validation_loss, validation_accuracy = validate(model, criterion, test_loader)\n","    logs['val_' + 'log loss'] = validation_loss.item()\n","    logs['val_' + 'accuracy'] = validation_accuracy.item()\n","\n","    liveloss.update(logs)\n","    liveloss.draw()\n","    \n","test_loss, test_accuracy = validate(model, criterion, test_loader)    \n","print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy.item())\n","print(\"\")\n","\n","model_save_name = 'resnet18_bees_and_ants_classifier_full_training_set_imagenet_feature_extract.pt'\n","path = F\"/content/gdrive/My Drive/models/{model_save_name}\" \n","torch.save(model.state_dict(), path)"]},{"cell_type":"markdown","metadata":{"id":"N5DmQlc_ebYf"},"source":["<img src=\"https://pbs.twimg.com/media/Ev-f6AaU8AgMeRd.jpg\" alt=\"drawing\" width=\"600\"/>"]},{"cell_type":"markdown","metadata":{"id":"qRArCrs3j-12"},"source":["### 2.5 What if pretrained on MNIST instead?"]},{"cell_type":"markdown","metadata":{"id":"GvQ8SKahEfqV"},"source":["Training a Resnet on MNIST from scratch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvoZPexxj93l"},"outputs":[],"source":["train_transform = transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,)),  \n","        transforms.Lambda(lambda x: x.expand(3, 224, 224)),   # expand to 3 channels             \n","    ])\n","test_transform = transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,)),\n","        transforms.Lambda(lambda x: x.expand(3, 224, 224)),   # expand to 3 channels      \n","    ])\n","\n","mnist_train = MNIST(\"./\", download=True, train=True, transform=train_transform)\n","mnist_test = MNIST(\"./\", download=True, train=False, transform=test_transform)\n","\n","train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)\n","test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=4)\n","\n","model = models.resnet18().to(device)\n","model.fc = ### add the last layer you need here\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","### ~ 7 min per epoch here ###\n","liveloss = PlotLosses()\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy.item()\n","\n","    validation_loss, validation_accuracy = validate(model, criterion, test_loader)\n","    logs['val_' + 'log loss'] = validation_loss.item()\n","    logs['val_' + 'accuracy'] = validation_accuracy.item()\n","\n","    liveloss.update(logs)\n","    liveloss.draw()\n","\n","model_save_name = 'resnet18_mnist_classifier_full_training_set_baseline_.pt'\n","path = F\"/content/gdrive/My Drive/models/{model_save_name}\" \n","torch.save(model.state_dict(), path)\n","\n","\n","test_loss, test_accuracy = validate(model, criterion, test_loader)    \n","print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy.item())\n","print(\"\")"]},{"cell_type":"markdown","metadata":{"id":"j38pm3TBExK6"},"source":["### 2.6 Feature Extraction on the ResNet pretrained on MNIST\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15DVykO4kaw_"},"outputs":[],"source":["set_seed(seed)\n","\n","train_transform = transforms.Compose([\n","        ### add a transform that does a random crop with the right size here\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.1307, 0.1307, 0.1307], [0.3081, 0.3081, 0.3081]),\n","    ])\n","test_transform = transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.1307, 0.1307, 0.1307], [0.229, 0.3081, 0.3081]),\n","    ])\n","\n","train_ds = datasets.ImageFolder(\"./hymenoptera_data/train\", transform=train_transform)\n","test_ds = datasets.ImageFolder(\"./hymenoptera_data/val\", transform=test_transform)\n","\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n","\n","model = models.resnet18().to(device)\n","model.fc = nn.Linear(model.fc.in_features, 10).to(device)\n","path = F\"/content/gdrive/My Drive/models/{model_save_name}\" \n","model.load_state_dict(torch.load(path))\n","model.fc = #### add the last layer you need here\n","\n","optimizer = ### use SGD and fine-tune your network\n","criterion = ### define your loss\n","\n","liveloss = PlotLosses()\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy.item()\n","    liveloss.update(logs)\n","    liveloss.draw()\n","    logs['val_' + 'log loss'] = 0.\n","    logs['val_' + 'accuracy'] = 0.\n","\n","model_save_name = 'resnet18_bees_and_antes_classifier_full_training_set_mnist_transfer.pt'\n","path = F\"/content/gdrive/My Drive/models/{model_save_name}\" \n","torch.save(model.state_dict(), path)\n","\n","\n","test_loss, test_accuracy = validate(model, criterion, test_loader)    \n","print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy.item())\n","print(\"\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wVDOioLU_32t"},"source":["As expected, hand-written digits and bees do not share data distribution shapes."]},{"cell_type":"markdown","source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","---\n","---\n","\n","## ***Clarifying issues with loss averaging:***"],"metadata":{"id":"aWu5w9zJB5oa"}},{"cell_type":"code","source":["def train(model, optimizer, criterion, data_loader):\n","    model.train()\n","    train_loss, train_accuracy = 0, 0\n","    for X, y in data_loader:\n","        X, y = X.to(device), y.to(device)\n","        optimizer.zero_grad()\n","        a2 = model(X.view(-1, 3, 224, 224))\n","        loss = criterion(a2, y)\n","        loss.backward()\n","        train_loss += loss*X.size(0) ## this undoes normalisation\n","        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n","        optimizer.step()  \n","        \n","    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)"],"metadata":{"id":"M66z-bLYB-X9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The cross entropy loss provided by PyTorch has a normalisation added to it:\n","\n","[`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n","\n","This line:\n","\n","`train_loss += loss*X.size(0)`\n","\n","undoes the normalisation done by `torch.nn.CrossEntropyLoss` and adds the value to the accumulated losses over minibatches.\n","\n","Then, once we have gone over all the data (that is, over all the minibatches), we will have completed one epoch, and we want to know the value of the normalised loss. To get it, we divide it by the total number of samples in my data:\n","\n","`return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)`\n","\n","A similar business is going on with the accuracy. In this case we use scikit-learn:\n","\n","[`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n","\n","where we do:\n","\n","`train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)`\n","\n","and then we divide the accuracy by the total length of the data.\n","\n","<br>\n","<br>\n","\n","Finally, this line before the data_loader is iterated:\n","\n","`train_loss, train_accuracy = 0, 0`\n","\n","resets the value of the loss value at every epoch.\n","\n","\n","---\n","\n","## Alternative implementations:"],"metadata":{"id":"uAuNMG7tCTVQ"}},{"cell_type":"markdown","source":["Simply modify the default behaviour of the loss and the accuracy calculations:\n","\n","`torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, `**`reduction='mean'`**`, label_smoothing=0.0)`\n","\n","change default to **`reduction='sum'`** when you define the loss.\n","\n","<br>\n","\n","And for the accuracy, just change the line:\n","\n","`train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy(),`**`normalize=False`**`)`\n","\n","### **Now I just accumulate the sum of the loss, so it is naturally scaled to the batch size**"],"metadata":{"id":"2kWwC4xMGXfP"}}]}