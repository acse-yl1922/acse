{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZAqrTaTGmLp"
   },
   "source": [
    "## **Afternoon Session 7: Variational Autoencoders**\n",
    "\n",
    "# Introduction\n",
    "\n",
    "We are going to build a linear and convolutional VAE for the chest medmnist dataset.\n",
    "\n",
    "## 0.1 Installing MedMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4zyBfxBDJxb",
    "outputId": "cb9a8daf-e947-4018-f96e-68545ed6dfdf"
   },
   "outputs": [],
   "source": [
    "# !pip install medmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqoIGvDXHAKe"
   },
   "source": [
    "## 0.2 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIwV1csTDefr"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FC5TwQ4fHk5o"
   },
   "source": [
    "## 0.3 Download the dataset\n",
    "\n",
    "Tip: there are often examples in the README files:\n",
    "\n",
    "https://github.com/MedMNIST/MedMNIST/blob/main/README.md\n",
    "\n",
    "I learnt how to do this using the example in \n",
    "\n",
    "https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMLwtvQ_Dg9C",
    "outputId": "ad70aa15-0c3f-4559-f2e2-b6b6291d6574"
   },
   "outputs": [],
   "source": [
    "data_flag = 'chestmnist'\n",
    "download = True\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*batch_size, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hRK7VCzHqUc"
   },
   "source": [
    "## 0.4 Check the dataset\n",
    "\n",
    "It's always a good idea to check the dataset before using it. \n",
    "\n",
    "According to https://medmnist.com/, the dataset should consist of 28x28 images. The chest class has 112,120 images with splits for:\n",
    "\n",
    "- training\t(78,468 images)\n",
    "- validation (11,219 images)\n",
    "- testing (22,433 images)\n",
    "\n",
    "__Can you:__\n",
    "\n",
    "1. Check the number of batches in the dataloader? (Hint: check the length)\n",
    "2. Get the first batch of images out of the dataloader?\n",
    "3. Plot the first image in the batch?\n",
    "4. How many pixels does an image contain?\n",
    "5. What is the range of the images?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "lO8uYHMBET1T",
    "outputId": "cb6f9ef5-0ee6-4913-f558-54de2b8564f3"
   },
   "outputs": [],
   "source": [
    "print(train_loader)\n",
    "print('Batch size: '+str(train_loader.batch_size))\n",
    "print()  # Check the number of batches\n",
    "print('Num images: '+str(train_loader.batch_size*train_loader.__len__()))\n",
    "print()\n",
    "\n",
    "  # Get the first batch of images\n",
    "\n",
    "print('Image shape: '+str(images.shape))  # NOTE: The shape is n_ims, n_channels, img_dims \n",
    "\n",
    "plt.imshow()  # Plot the first image in the batch\n",
    "plt.colorbar()\n",
    "\n",
    "print()  # How many pixels are in the image\n",
    "print('Range: max, '+str(float(images.max()))+' min,'+str(float(images.min())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OJy6svbKmTT"
   },
   "source": [
    "## 1.1 The Encoder\n",
    "\n",
    "The encoder takes an image and develops a latent vector. Use Linear layers and ReLU activation functions to generate the Mu and Sigma latent vectors.\n",
    "\n",
    "**In the**\n",
    "\\_\\_init\\_\\_()\n",
    "**function...**\n",
    "\n",
    "1. How many nodes should the first layer have?\n",
    "2. How would you choose the dimensionality reduction schedule?\n",
    "3. Design layers to generate latent vectors. Do they have activation functions?\n",
    "\n",
    "**In the**\n",
    "forward()\n",
    "**function...**\n",
    "\n",
    "1. Can you reshape the image to a vector?\n",
    "2. Can you Implement the latent vectors (mu/sigma) so they are returned with the correct values?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpeHPmBPKmoI"
   },
   "outputs": [],
   "source": [
    "class VAE_Encoder(nn.Module):  # The Encoder inherits the properties of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "  def __init__(self):\n",
    "    '''\n",
    "    Class contains the Encoder (image -> latent).\n",
    "    '''\n",
    "    super(VAE_Encoder, self).__init__()\n",
    "\n",
    "    self.layer1 = nn.Sequential(\n",
    "        nn.Linear(),  # How many nodes should the first layer have?\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "        nn.Linear(),  # How would you choose a dimensionality reduction schedule?\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.layer3 = nn.Sequential(\n",
    "        nn.Linear(),   # How would you choose a dimensionality reduction schedule?\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.layerMu = nn.Sequential(\n",
    "        nn.Linear(128, 16),\n",
    "        # Does this need an activation?\n",
    "    )\n",
    "\n",
    "    self.layerSigma = nn.Sequential(\n",
    "        nn.Linear(128, 16),\n",
    "        # Does this need an activation?\n",
    "    )\n",
    "\n",
    "  def forward(self, x):  # Custom pytorch modules should follow this structure \n",
    "    '''\n",
    "    x: [float] the MNIST image\n",
    "    '''\n",
    "\n",
    "    x =   # Reshape the input into a vector (nD to 1D) \n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    mu =  # Implement Mu\n",
    "    sigma = # Implement Sigma\n",
    "    return mu, sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4od0XCpOJpn"
   },
   "source": [
    "## 1.2 Encoding an image\n",
    "\n",
    "The encoder will take an image and generate 2 latent vectors (mu/sigma). It is important to show that the encoder works correcly.\n",
    "\n",
    "**Can you**\n",
    "\n",
    "1. Get the first image from the first batch?\n",
    "2. Run the image through the encoder?\n",
    "3. Are the elements of mu positive and negative?\n",
    "4. Are the elements of sigma all positive?\n",
    "\n",
    "**If the answer to questions 3 and 4 is not \"Yes\", then return to the encoder and make corrections.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jySOXNp5M0IV",
    "outputId": "6fe1b57a-7c68-4062-884f-5a954f2b428b"
   },
   "outputs": [],
   "source": [
    "encoder = VAE_Encoder()\n",
    "\n",
    "  # Get the first batch from the dataloader\n",
    "  # Get the first image from the batch\n",
    "mu, sigma = encoder(images[0].unsqueeze(0))\n",
    "\n",
    "print(mu.detach()) # Check whether the elements are correct? \n",
    "print(sigma.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAFwqO9BStds"
   },
   "source": [
    "## 1.3 The Decoder\n",
    "\n",
    "The decoder takes a latent vector and develops an image. Use Linear layers and ReLU activation functions to generate an image.\n",
    "\n",
    "Remember, if you implement the stochastic sampling layer in the decoder, then you won't be able to make deterministic preditions. \n",
    "\n",
    "**In the**\n",
    "\\_\\_init\\_\\_()\n",
    "**function...**\n",
    "\n",
    "1. Should the latent vector layer have an activation function?\n",
    "2. How would you choose the dimensionality expansion schedule?\n",
    "3. What activation function should the output layer have?\n",
    "\n",
    "**In the**\n",
    "forward()\n",
    "**function...**\n",
    "\n",
    "1. Can you reshape the vector to an image?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jz9HlR8lSsu2"
   },
   "outputs": [],
   "source": [
    "class VAE_Decoder(nn.Module):  # The Encoder inherits the properties of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "  def __init__(self):\n",
    "    '''\n",
    "    Class contains the Decoder (latent -> image).\n",
    "    '''\n",
    "\n",
    "    super(VAE_Decoder, self).__init__()\n",
    "\n",
    "    self.layerLatent = nn.Sequential(\n",
    "        nn.Linear(16, ), # Choose the dimensionality expansion schedule\n",
    "        # Does the latent vector have an activation?\n",
    "    )\n",
    "\n",
    "    self.layer1 = nn.Sequential(\n",
    "        nn.Linear(), # Choose the dimensionality expansion schedule\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "        nn.Linear(),  # Choose the dimensionality expansion schedule\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.layer3 = nn.Sequential(\n",
    "        nn.Linear(),  # Choose the dimensionality expansion schedule\n",
    "         # What activation for the output layer?\n",
    "    )\n",
    "\n",
    "  def forward(self, z):  # Custom pytorch modules should follow this structure \n",
    "    '''\n",
    "    x: [float] the MNIST image\n",
    "    '''\n",
    "\n",
    "    z = self.layerLatent(z)\n",
    "    z = self.layer1(z)\n",
    "    z = self.layer2(z)\n",
    "    z = self.layer3(z)\n",
    "    return   # Reshape the vector into an image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvCf5XZ1XUIZ"
   },
   "source": [
    "## 1.4 Decoding a latent vector\n",
    "\n",
    "The encoder will take a latent vector (z) and generate an image. It is important to show that the encoder works correcly.\n",
    "\n",
    "**Can you**\n",
    "\n",
    "1. Generate a vector of zeros with the correct shape?\n",
    "2. Run the latent vector through the decoder?\n",
    "3. Is the image the correct shape (1,1,28,28)?\n",
    "4. Does the image look like random noise?\n",
    "\n",
    "**If the answer to questions 3 and 4 is not \"Yes\", then return to the decoder and make corrections.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "qQDzq72EWS6O",
    "outputId": "9cb6209d-70a9-494f-b873-8787b3ee5ff5"
   },
   "outputs": [],
   "source": [
    "decoder = VAE_Decoder()\n",
    "\n",
    "z = torch.zeros(1,16)  # Generate a random vector with the correct shape\n",
    "image = decoder(z)  # Run the vector through the decoder\n",
    "\n",
    "print('Image shape: '+str(image.shape))  # Check the shape of the image\n",
    "\n",
    "plt.imshow(image.detach().cpu().squeeze())  # Does the image look like random noise?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXh14IV5X-5O"
   },
   "source": [
    "## 2.1 Autoencoder\n",
    "\n",
    "The output of the encoder is two latent vectors, mu and sigma. Together these represent the latent space distribution because they define a Gaussian distribution.\n",
    "\n",
    "Now we build the autoencoder. The autoencoder uses an *encoder* to generate the latent mu and sigma and a *decoder* to generate an image using the latent vector z. The latent vector z is found be stochastic sampling of the latent space distribution.\n",
    "\n",
    "**In**\n",
    "sample_latent_space()\n",
    "**Can you**\n",
    "\n",
    "1. Define an equation to sample from the latent distribution?\n",
    "\n",
    "**In**\n",
    "forward()\n",
    "**Can you**\n",
    "\n",
    "1. Take the image and generate the latent vectors mu & sigma?\n",
    "2. Take the latent vectors mu & sigma and generate the latent vector sample?\n",
    "3. Take the latent vector sample and generate the reconstructed image?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yz52IrPpYEvU",
    "outputId": "fe6f5fe1-09d1-4da3-9e3b-fad6436608e6"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "  def __init__(self, device):\n",
    "    '''\n",
    "    Class combines the Encoder and the Decoder with a VAE latent space.\n",
    "    '''\n",
    "    super(VAE, self).__init__()\n",
    "    self.device = device\n",
    "    self.encoder = VAE_Encoder()\n",
    "    self.decoder = VAE_Decoder()\n",
    "    self.distribution = torch.distributions.Normal(0, 1)  # Sample from N(0,1)\n",
    "\n",
    "  def sample_latent_space(self, mu, sigma):\n",
    "    z =   ## (1) Sample the latent distribution\n",
    "    kl_div = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()  # A term, which is required for regularisation\n",
    "    return z, kl_div\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "    x - [float] A batch of images from the data-loader\n",
    "    '''\n",
    "\n",
    "      ## (1) Generate the latent vectors Mu and Sigma\n",
    "      ## (2) Generate the latent vector sample \n",
    "      ## (3) Generate the reconstructed image\n",
    "    return z, kl_div\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlx-mQmSbnjg"
   },
   "source": [
    "## 2.2 Generating a reconstruction\n",
    "\n",
    "It is important to test that the VAE works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "flVrCJtSZRLK",
    "outputId": "b7d27613-aa3e-4479-8137-8e16dec40767"
   },
   "outputs": [],
   "source": [
    "vae = VAE('cpu')\n",
    "\n",
    "images, labels = next(iter(train_loader))  # Get the first batch of images\n",
    "print(images[0].shape)  # Get the first image from the batch\n",
    "recon, _ = vae(images[10].unsqueeze(0))  # Are mu and sigma correct\n",
    "\n",
    "plt.figure(); plt.imshow(images[0].squeeze())\n",
    "plt.figure(); plt.imshow(recon.cpu().detach().squeeze())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDuEgpiFbwDB"
   },
   "source": [
    "## 3.1 Training the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CL1LgPG8bvMj",
    "outputId": "78f99102-62be-4dbc-948d-7387b29b8a8d"
   },
   "outputs": [],
   "source": [
    "def train(autoencoder, data, kl_div_on=True, epochs=10, device='cpu'):\n",
    "  opt = torch.optim.Adam(autoencoder.parameters())\n",
    "  for epoch in range(epochs):  # Run data over numerous epochs\n",
    "    for batch, label in tqdm(data):  # Iterate over the batches of images and labels\n",
    "      batch = batch.to(device)  # Send batch of images to the GPU\n",
    "      opt.zero_grad()  # Set optimiser grad to 0\n",
    "      x_hat, KL = autoencoder(batch)  # Generate predicted images (x_hat) by running batch of images through autoencoder\n",
    "      loss = ((batch - x_hat)**2).sum() + KL  # Calculate combined loss\n",
    "      loss.backward()  # Back-propagate\n",
    "      opt.step()  # Step the optimiser\n",
    "  return autoencoder  # Return the trained autoencoder (for later analysis)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "dims_latent = 2  # Maybe increase this an try the t-sne algorithm for visualisation?!\n",
    "vae = VAE(device).to(device)\n",
    "vae = train(vae, train_loader, epochs=10, device=device)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTcT5k6TjO2X"
   },
   "source": [
    "## 4.1 Mode collapse\n",
    "\n",
    "Generate some images with the trained VAE. Do you notice that all the recons look similar?\n",
    "\n",
    "Generative models have a tendency towards mode collapse, where they generate the \"average\" of the dataset rather than a meaningful recon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "WAyFrQeR50oA",
    "outputId": "3ff454e0-234b-462c-efc0-bec13fed560b"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))  # Get the first batch of images\n",
    "print(images.shape)  # Get the first image from the batch\n",
    "\n",
    "_, ax = plt.subplots(2, 5, figsize=[18.5, 6])\n",
    "for n, idx  in enumerate(torch.randint(0,images.shape[0], (5,))):\n",
    "  recon, _ = vae(images[idx].unsqueeze(0).cuda())  # Are mu and sigma correct\n",
    "  ax[0, n].imshow(images[idx].squeeze())\n",
    "  ax[1, n].imshow(recon.cpu().detach().squeeze())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YDzNVfADwVh"
   },
   "source": [
    "## 4.2 Convolutional VAE\n",
    "\n",
    "By increasing the complexity of the model we can generate more complex images. The best starting point is to replace Linear layers with Conv layers.\n",
    "\n",
    "In a convolutional Encoder:\n",
    "- Increase the number of channels using __Conv2D__\n",
    "- Compress the spatial dimensions are compressed using __MaxPool2d__\n",
    "- It is important to pad the starting image to a power of two so the dimensionality reduction works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEhoWBUMDu1e"
   },
   "outputs": [],
   "source": [
    "class VAE_Encoder_Conv(nn.Module):\n",
    "  def __init__(self):\n",
    "    '''\n",
    "    Class contains the Encoder (image -> latent).\n",
    "    '''\n",
    "    super(VAE_Encoder_Conv, self).__init__()\n",
    "\n",
    "    self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(1,  20, 3, padding=3),  # Pad so that image dims are 2^n\n",
    "        nn.GELU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )  # Dims in 32x32 -> out 16x16\n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "        nn.Conv2d(20, 40, 3, padding=\"same\"),\n",
    "        nn.MaxPool2d(2)\n",
    "    )  # Dims in 16x16 -> out 8x8\n",
    "\n",
    "    self.layer3 = nn.Sequential(\n",
    "        nn.Conv2d(40, 60, 3, padding=\"same\"),\n",
    "        nn.GELU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )  # Dims in 8x8 -> out 4x4\n",
    "\n",
    "    self.layerMu = nn.Sequential(\n",
    "        nn.Conv2d(60, 120, 1),\n",
    "    )  # Dims in 4x4 -> out 4x4\n",
    "\n",
    "    self.layerSigma = nn.Sequential(\n",
    "        nn.Conv2d(60, 120, 1),\n",
    "    )  # Dims in 4x4 -> out 4x4\n",
    "\n",
    "  def forward(self, x, print_shape=False):  # Custom pytorch modules should follow this structure \n",
    "    '''\n",
    "    x: [float] the MNIST image\n",
    "    '''\n",
    "\n",
    "    x = self.layer1(x)\n",
    "    if print_shape:\n",
    "      print('L1: '+str(x.shape))\n",
    "    x = self.layer2(x)\n",
    "    if print_shape:\n",
    "      print('L2: '+str(x.shape))\n",
    "    x = self.layer3(x)\n",
    "    if print_shape:\n",
    "      print('L3: '+str(x.shape))\n",
    "    mu =  self.layerMu(x)\n",
    "    sigma = torch.exp(self.layerSigma(x))\n",
    "    return mu, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIRd27uFGrD1",
    "outputId": "53eeb3e7-aaa5-4f98-c862-4ef0b3aded86"
   },
   "outputs": [],
   "source": [
    "encoder = VAE_Encoder_Conv()\n",
    "\n",
    "images, labels = next(iter(train_loader))  # Get the first batch of images\n",
    "print('Input shape: '+str(images[0].shape))  # Get the first image from the batch\n",
    "mu, sigma = encoder(images[0].unsqueeze(0), print_shape=True)  # Are mu and sigma correct\n",
    "\n",
    "print('Mu shape: '+str(mu.detach().shape))\n",
    "print('Sigma shape:'+str(sigma.detach().shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_nEcvcyODKU"
   },
   "source": [
    "In a convolutional Decoder:\n",
    "- Decrease the number of channels using __Conv2D__\n",
    "- Expand the spatial dimensions using __Upsample__\n",
    "- Dont forget to remove the padding from the output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8b2AcrxD89F"
   },
   "outputs": [],
   "source": [
    "class VAE_Decoder_Conv(nn.Module):  # The Encoder inherits the properties of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "  def __init__(self):\n",
    "    '''\n",
    "    Class contains the Decoder (latent -> image).\n",
    "    '''\n",
    "\n",
    "    super(VAE_Decoder_Conv, self).__init__()\n",
    "\n",
    "    self.layerLatent = nn.Sequential(\n",
    "        nn.Conv2d(120, 60, 1),\n",
    "        nn.GELU(),\n",
    "        nn.Upsample(scale_factor=2, mode='nearest')\n",
    "    )  # Dims in 4x4 -> out 8x8\n",
    "\n",
    "    self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(60, 40, 3, padding='same'),\n",
    "        nn.GELU(),\n",
    "        nn.Upsample(scale_factor=2, mode='nearest')\n",
    "    )  # Dims in 8x8 -> out 16x16\n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "        nn.Conv2d(40, 20, 3, padding='same'),\n",
    "        nn.GELU(),\n",
    "        nn.Upsample(scale_factor=2, mode='nearest')\n",
    "    )  # Dims in 16x16 -> out 32x32\n",
    "\n",
    "    self.layer3 = nn.Sequential(\n",
    "        nn.Conv2d(20, 1, 3, padding='same'),\n",
    "        nn.GELU(),\n",
    "        nn.Sigmoid()\n",
    "    )  # Dims in 32x32 -> out 32x32\n",
    "\n",
    "  def forward(self, z, print_shape=False):  # Custom pytorch modules should follow this structure \n",
    "    '''\n",
    "    x: [float] the MNIST image\n",
    "    '''\n",
    "\n",
    "    z = self.layerLatent(z)\n",
    "    z = self.layer1(z)\n",
    "    if print_shape:\n",
    "      print('L1: '+str(z.shape))\n",
    "    z = self.layer2(z)\n",
    "    if print_shape:\n",
    "      print('L2: '+str(z.shape))\n",
    "    z = self.layer3(z)\n",
    "    if print_shape:\n",
    "      print('L3: '+str(z.shape))\n",
    "    return z[:,:,2:-2,2:-2]  # Crop the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcLv2SW0OTJ-"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "zlXYVN47J2U5",
    "outputId": "ee736180-bc0f-4b2b-e62e-6d0fb9cba589"
   },
   "outputs": [],
   "source": [
    "decoder = VAE_Decoder_Conv()\n",
    "\n",
    "latent_vec = torch.zeros(1,120,4,4)\n",
    "\n",
    "print('Latent shape: '+str(latent_vec.shape))\n",
    "\n",
    "recon = decoder(latent_vec, print_shape=True)\n",
    "\n",
    "print('Image shape: '+str(recon.shape))\n",
    "\n",
    "plt.imshow(recon.detach().cpu().squeeze())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zah6yCejOciK"
   },
   "source": [
    "You don't need to make many changes for Autoencoding though.\n",
    "\n",
    "An interesting challenge is to explore the effect of changing the spatial/channel dimensions in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2HndoU9Letx",
    "outputId": "a62401dc-498a-400d-c31f-90db0e0c1079"
   },
   "outputs": [],
   "source": [
    "class VAE_Conv(nn.Module):\n",
    "  def __init__(self, device):\n",
    "    '''\n",
    "    Class combines the Encoder and the Decoder with a VAE latent space.\n",
    "    '''\n",
    "    super(VAE_Conv, self).__init__()\n",
    "    self.device = device\n",
    "    self.encoder = VAE_Encoder_Conv()\n",
    "    self.decoder = VAE_Decoder_Conv()\n",
    "    self.distribution = torch.distributions.Normal(0, 1)  # Sample from N(0,1)\n",
    "\n",
    "  def sample_latent_space(self, mu, sigma):\n",
    "    z = mu + sigma * self.distribution.sample(mu.shape).to(self.device)  # Sample the latent distribution\n",
    "    kl_div = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()  # A term, which is required for regularisation\n",
    "    return z, kl_div\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "    x - [float] A batch of images from the data-loader\n",
    "    '''\n",
    "\n",
    "    mu, sigma = self.encoder(x)  # Run the image through the Encoder\n",
    "    z, kl_div = self.sample_latent_space(mu, sigma)  # Take the output of the encoder and get the latent vector \n",
    "    z = self.decoder(z)  # Return the output of the decoder (the predicted image)\n",
    "    return z, kl_div\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8rF8nm4OsAr"
   },
   "source": [
    "Notice how the appearance of the reconstruction contains correlated noise. This is exactly what we want so that we can achieve visually \"correct\" appearances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "oU-gY_9DLt8Z",
    "outputId": "c517a9f2-122e-4bb1-f5d7-e19d09eedc8d"
   },
   "outputs": [],
   "source": [
    "vae = VAE_Conv('cpu')\n",
    "\n",
    "images, labels = next(iter(train_loader))  # Get the first batch of images\n",
    "print(images[0].shape)  # Get the first image from the batch\n",
    "recon, _ = vae(images[10].unsqueeze(0))  # Are mu and sigma correct\n",
    "\n",
    "plt.figure(); plt.imshow(images[0].squeeze())\n",
    "plt.figure(); plt.imshow(recon.cpu().detach().squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSiUwiiJMcdI",
    "outputId": "76534fda-5386-4ccb-d435-3e6f88c78205"
   },
   "outputs": [],
   "source": [
    "def train(autoencoder, data, kl_div_on=True, epochs=10, device='cpu'):\n",
    "  opt = torch.optim.Adam(autoencoder.parameters(), lr=1e-4, betas=(0.9, 0.999), weight_decay=0.5)\n",
    "  for epoch in range(epochs):  # Run data over numerous epochs\n",
    "    for batch, label in tqdm(data):  # Iterate over the batches of images and labels\n",
    "      batch = batch.to(device)  # Send batch of images to the GPU\n",
    "      opt.zero_grad()  # Set optimiser grad to 0\n",
    "      x_hat, KL = autoencoder(batch)  # Generate predicted images (x_hat) by running batch of images through autoencoder\n",
    "      loss = ((batch - x_hat)**2).sum() + KL  # Calculate combined loss\n",
    "      loss.backward()  # Back-propagate\n",
    "      opt.step()  # Step the optimiser\n",
    "  return autoencoder  # Return the trained autoencoder (for later analysis)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "vae_conv = VAE_Conv(device).to(device)\n",
    "vae_conv = train(vae_conv.train(True), train_loader, epochs=5, device=device)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "sFQcMTE2Ml9T",
    "outputId": "a693c850-8e3a-4155-fbd1-e711aa29c13a"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))  # Get the first batch of images\n",
    "print(images.shape)  # Get the first image from the batch\n",
    "vae_conv1 = vae_conv1.eval()\n",
    "\n",
    "_, ax = plt.subplots(2, 5, figsize=[18.5, 6])\n",
    "for n, idx  in enumerate(torch.randint(0,images.shape[0], (5,))):\n",
    "  recon, _ = vae_conv1(images[idx].unsqueeze(0).cuda())  # Are mu and sigma correct\n",
    "  ax[0, n].imshow(images[idx].squeeze())\n",
    "  ax[1, n].imshow(recon.cpu().detach().squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
