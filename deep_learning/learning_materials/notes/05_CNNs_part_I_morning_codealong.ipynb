{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNVv/aWagomz5U8tgyA2uE3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n","\n","\n","---\n"],"metadata":{"id":"9YehS8enAmDn"}},{"cell_type":"markdown","source":["# **CNNs: convolutional neural networks**\n","\n","\n","#### **Morning contents/agenda**\n","\n","1. What are convolutions?\n","\n","2. How do we use them? (`Torch` layer operations)\n","\n","3. Visual roadmap of a CNN\n","\n","4. Implementation of a network similar to LeNet5\n","\n","5. Training our LeNet5-like network on `FasionMNIST`\n","\n","\n","#### **Learning outcomes**\n","\n","1. Have a clear idea of how convolutions work\n","\n","2. Understand the parameters trained in a CNN\n","\n","3. CNN architectures and combinations with other types of layers\n","\n","4. Implementation of  asimple CNN in `PyTorch`\n","\n","<br>\n","\n","#### **Afternoon contents/agenda**\n","\n","1. Dropout and batch normalisation\n","\n","2. Training with data augmentation\n","\n","#### **Learning outcomes**\n","\n","1. Implement dropout and batchnorm layers in `PyTorch`\n","\n","2. Perform data augmentations and understand its effects\n","\n","\n","<br/>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"dSO29p0jx5hv"}},{"cell_type":"code","source":["!pip install pycm livelossplot\n","%pylab inline\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from livelossplot import PlotLosses\n","from pycm import *\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.datasets\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","from torchsummary import summary"],"metadata":{"id":"_kYki018eTFJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"],"metadata":{"id":"LI8sNA9feT3H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n"],"metadata":{"id":"J-VVpU1MJJOB"}},{"cell_type":"markdown","source":["So far we have learned how to implement a simple FFN in PyTorch, how to train it, and how to find optimal hyperparameters for it. Today we will apply this concepts to a new architecture: CNNs.\n","\n","CNNs are widely used mostly, **but not only**, in computer vision problems:\n","\n","<center><img src=\"https://drive.google.com/uc?id=1kdQpSH9fLNYyBtZvQ0KXDBqBlr_0OEbb\" width=\"600\"/></center>\n"],"metadata":{"id":"Z-BLv6L2_LaF"}},{"cell_type":"code","source":[],"metadata":{"id":"soisumq_5CmM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. What are convolutions?\n","\n","Convolutions are operations between two functions or discrete representations where the output is the result of multiplying and adding their elements at different relative positions.\n","\n","An image is perhaps better suited to describe this:\n","\n","<!---<br>\n","\n"," <center><img src=\"https://drive.google.com/uc?id=13olO6eqI89ukoZ7hqZXZL4_NqzalsP6W\" width=\"800\"/></center>\n","\n","<br> --->\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=16sYFaFyVNQw4NIiIU_OHnJlsvMDrqcPJ\" width=\"800\"/></center>\n","\n","<br>\n","\n","And there is a bit to unpack here. The first equation describes a convolution operation between $x$ and $W$, which results in $y$. In essence this is what convolutions do. The other two lines are there to highlight how we will be using convolutions in the layers of networks. An example of convolving and applying a ReLU activation from [this blog](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/):\n","\n","<br>\n","\n","\n","<center><img src=\"https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif?w=480&zoom=2\" width=\"600\"/></center>\n","\n","<center><img src=\"https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-07-at-6-18-19-pm.png?w=1496\" width=\"600\"/></center>\n","\n","<br>\n","\n","...but let's park the activations and biases for now.\n","\n","Convolutions can be described with only 4 parameters:\n","\n","- input size\n","- filter or kernel size\n","- stride\n","- padding\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1V81dhsD0eD5dvIul8tJmwrDhQcAuUhMG\" width=\"800\"/></center>\n","\n","<br>\n","\n","A few simple examples to help clarify convolutions with different strides and paddings:\n","\n","<img src=\"https://drive.google.com/uc?id=1kYve-IfJv5a25KxKDv4D7Zrf69i9LD7b\" width=\"200\"/>\n","<img src=\"https://drive.google.com/uc?id=142TRExJSGa4mGn-8M6aLksGkTQSPtcXS\" width=\"200\"/>\n","<img src=\"https://drive.google.com/uc?id=19N9b64dbLhSRpRvq-DJk184TSfVDeQGm\" width=\"200\"/>\n","<img src=\"https://drive.google.com/uc?id=1-LAL5YPnwnXiFHZNBFTWC17UFP6dDRzA\" width=\"200\"/>\n","\n","<br>\n","\n","**Question**: what are the padding and stride values for each one of the above cases?\n","\n","And now we see that the size of the output depends on all four parameters of the convolution: input size, filter size, stride, and padding.\n","\n","<br>\n","\n","A more detailed explanation of convolutions from a more mathematical perspective can be found in this brilliant (as always) [video by 3blue1brown](https://www.youtube.com/watch?v=KuXjwB4LzSA)\n","\n","<br/>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"sinIebTtMUvs"}},{"cell_type":"markdown","source":["## 2. How do we use them? (Torch layer operations)\n","\n","Now that we are familiar with convolution operations, how do we use them in convolutional neural networks?\n","\n","The procedure is similar to what we have already seen in feed-forward networks, but the exchange of information between layers is now done using convolutions instead of linear mapppings (where all outputs of one layer are used to compute the inputs of the next).\n","\n","So, instead of this kind of architecture (FFN):\n","\n","<br>\n","\n","<center><img src=\"https://thumbs.gfycat.com/AdorableJoyfulLemming-max-1mb.gif\" alt=\"network\" width=\"500\"/></center>\n","\n","<br>\n","\n","we now use CNNs, and we do not have to alter the original dimensions of the data (no need to perform the flatten operation in the first step of the animation above):\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1_9GLVFv8GkQbKsCHt3qZQWrQyPMezUNC\" width=\"800\"/></center>\n","\n","<br>\n","\n","This is one of the main advantages of using CNNs, it preserves some information about the spatial distribution of the data. The image above corresponds to the architecture of [LeNet-5](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf), introduced by Yann LeCun,\u0002 L\u0003eon Bottou\u0002, Yoshua Bengio\u0002, and Patrick Ha\u0004ffner in **1998**\n","\n","The number of trainable parameters in FFNs is uniquely defined by the number of neurons (or units) in each layer, plus the bias term if we use it. These trainable parameters are the weights (and biases) that connect neurons from one layer to neurons to the next layer.\n","\n","The number of trainable parameters in a CNN depends on the size of the filters at each layer and the number of channels (or feature maps) in each layer. The trainable parameters in the convolutional layers of a CNN are the coefficients of the filters (commonly referred to as convolutional kernels), denoted by $W_i$ in the figure above.\n","\n","\n","\n","To better illustrate how the input and output channels are defined in convolutional layers, consider:\n","\n","<center></center>\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1sMStdtSRWhuPvSuAMGEwJg6huRl22U5f\" width=\"800\"/></p><p align = \"center\">\n","<i>3-channel input (RGB image) and 2-channel output example</i>\n","</p>\n","\n","#### **Exercise**:\n","**If I have a layer with 10 features, each of size 50x50, and I apply 25 filters of size 3x3x10 and stride 1, no padding. What is the size and number channels of the next layer?**\n","\n","#### **Answer**:\n","\n","<br>\n","\n","#### **Exercise**:\n","**Given the network below, how many trainable parameters do we have?**\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1-IqVfFHi_foHyqpVHMqsdPzrKZcy_3jB\" width=\"800\"/></center>\n","\n","<br>\n","\n","#### **Answer**:"],"metadata":{"id":"yPC2aDTe83n_"}},{"cell_type":"code","source":["print('The network has', 0, 'parameters') ## add your calculation where the '0' is"],"metadata":{"id":"TI3Pb_ug32Rx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the LeNet-5 representation we also have **subsampling** and **full-connection** layers. We already know what a full-connection layer is, as they are the same kind of layers we used in FFNs. \n","\n","Subsampling (and upsampling, as we will see next week) are operations that reduce the dimensionality of the data. The two most common ones are average pooling and max pooling:\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1ksmCBAjlh7nSrxBORzUq4LBhLXkCG372\" width=\"600\"/></center>\n","\n","<br>"],"metadata":{"id":"a_ou_CIz31jX"}},{"cell_type":"markdown","source":["## 3. Visual roadmap of a CNN\n","\n","Let's play a bit with the tools provided by PyTorch to implement convolutional layers with a simple one-layer operation example. We will also implement some of the concepts discussed yesterday like dropout and batch normalisation.\n","\n","PyTorch classes we will use:\n","\n","- [`nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html): Convolutional layers are parameterized by their kernel-weights and biases and are often used to reduce the spatial dimensionality.\n","\n","- [`nn.ConvTranspose2d`](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html): Transposed convolutions (not deconvolutions!); similar to convolutions, but normally used to upsample (increase the spatial dimensionality). [Interesting blog discussing problems with tranposed convolutions](https://distill.pub/2016/deconv-checkerboard/) and [intuitive animations of transposed convolutions](https://github.com/vdumoulin/conv_arithmetic)\n","\n","- [`nn.UpsamplingBilinear2d`](https://pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d.html) for upsampling (also check nearest neighbor upsampling, `nn.Upsample`)\n","\n","- [`nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html): Pooling layers summarize spatial information (also check  `nn.AvgPool2d`)\n","\n","- [`nn.Dropout2d`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html): Also exists in more dimensions: Can be use to regularise training of deep networks\n","\n","- Batch normalisation: Shift and center the distribution of the weights to a centered Gaussian distribution by keeping a running average of mini-batch properties. Introduced in [this paper](https://arxiv.org/abs/1502.03167). Originally, it was thought that doing batch normalisation would reduce the internal covariate shift and accelerate training, but a [later paper](https://arxiv.org/abs/1805.11604) questioned if that was the real reason why it was working so well. It seems to help learning in very deep convolutional neural networks, but it is not really well understood why this is the case.\n","\n","\\[**NOTE**: The pytorch documentation is extremely well organised and I highly recommend you use it to your own advantage.\\]"],"metadata":{"id":"YFbooZHDG8S7"}},{"cell_type":"code","source":["# get an image of a cat\n","\n","from PIL import Image  # PIL is hte Python Imaging Library\n","import requests        # library that provides an easy way to make http requests\n","from io import BytesIO # let's us read raw bites as a file\n","\n","url = \"https://cataas.com/cat\" # cat as a service!\n","response = requests.get(url)   # requests a cat\n","img = np.array(Image.open(BytesIO(response.content)).convert('L')).astype(float) # BytesIO tells python to read it as a file (and .content extracts only the image bytes)\n","plt.imshow(img, cmap=\"gray\");   # matplotlib likes numpy arrays"],"metadata":{"id":"p-829pmvKcQ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# apply some layer operations to the cat image:\n","\n","# first convert the cat image to a torch tensor:\n","x = torch.from_numpy(img).view(1, 1, img.shape[0], img.shape[1]).float()   # load the cat image as a tensor\n","\n","\n","convolution = ### define a convolutional layer\n","transposed_convolution = ### define a transposed convolutional layer\n","upsampling = ### define an uplinear layer\n","pool = ### define a maxpool layer\n","dropout = ### define a dropout layer (check dropout and dropout2d and try to see if you can tell what the difference between the two is)\n","# dropout = \n","batchnorm = ### define a bathcnorm layer \n","\n","plt.imshow(img)\n","plt.colorbar()\n","\n","fig, axarr = plt.subplots(2, 3, figsize=(24, 12))\n","for ax, op, name in zip(axarr.flatten(), [convolution, transposed_convolution, upsampling, pool, dropout, batchnorm], [\"conv\", \"conv_transposed\", \"upsample\", \"pool\", \"dropout\", \"batchnorm\"]):\n","  filtered = op(x)\n","  im = ax.imshow(filtered[0, 0].detach().numpy())\n","  ax.set_title(name, fontsize=18)\n","  fig.colorbar(im, ax=ax, fraction=0.03)\n","plt.show()\n"],"metadata":{"id":"kQmTxtGsHw52"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we can put it all together and see how a **trained** CNN operates:"],"metadata":{"id":"1PUuHzR0L4Cz"}},{"cell_type":"code","source":["%%html\n","<iframe src=\"https://poloclub.github.io/cnn-explainer/\" width=\"1200\" height=\"700\"></iframe>"],"metadata":{"id":"K-GEr-5tIOol"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","---\n","\n","<br>"],"metadata":{"id":"BMJUaOF1MDED"}},{"cell_type":"markdown","source":["## 4. Implementation of a network similar to LeNet5\n","\n","We will now use the layer classes we just saw to implement a version of Yann LeCun's LeNet-5 introduced in [this paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf):\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1_9GLVFv8GkQbKsCHt3qZQWrQyPMezUNC\" width=\"800\"/></center>\n","\n","<br>\n","\n","- Here the network is shown to have input's of size 32x32, so we will tell our first convolutional layer to add some padding to our 28x28 FasionMNIST images.  \n","- All convolutional layers with trainable parameters should have:\n","  - kernel-size=5\n","  - stride 1\n","  - padding 2\n","- All MaxPool layers use a kernel size 2 and a stride value of 2.\n","- Use ReLUs for all activations.\n","- Use bias terms (default in most PyTorch layers).\n","\n","\n","\n","Implement our version of LeNet-5 according to the instructions above:"],"metadata":{"id":"krG3YDwCMF4w"}},{"cell_type":"code","source":["class LeNet5(nn.Module):\n","  def __init__(self):\n","    super(LeNet5, self).__init__()\n","    # define a 2D convolutional layer\n","    # define a maxpool layer\n","    # new 2D convolutional layer\n","    # another maxpool layer\n","    # first linear layer\n","    # second linear layer\n","    # final output layer\n","    # activation function\n","    \n","  def forward(self, x):\n","    # activate pass through the first layer\n","    # activate pass through the second layer\n","    # activate pass through the third layer\n","    # activate pass through the fourth layer\n","    # flatten (return a \"flattened\" view of the 3d tensor as inputs for the fully connected layer)\n","    # activate pass through fifth layer\n","    # activate pass through last layer\n","    return self.output(x)                                         # return output\n","  \n","x = torch.randn((1, 1, 28, 28))\n","model = LeNet5()\n","y = model(x)\n","print(y)\n","print(model)"],"metadata":{"id":"fCCVRHDeNDSF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get the FashionMNIST dataset:"],"metadata":{"id":"H57J_ckZNFeL"}},{"cell_type":"code","source":["mnist_train = MNIST(\"./\", download=True, train=True)\n","mnist_test = MNIST(\"./\", download=True, train=False) "],"metadata":{"id":"Ba3ZEvLZND8k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instantiate and create a ```StratifiedShuffleSplit``` using sklearn.\n","1. Create a ```sklearn.model_selection.StratifiedShuffleSplit``` object with 1-split and a test-size of 10%.\n","2. Get the training and validation indices from the shuffel-split"],"metadata":{"id":"Lndb4nqhOFcF"}},{"cell_type":"code","source":["# split the data\n","shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split(mnist_train.train_data, mnist_train.train_labels) \n","indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]"],"metadata":{"id":"ca114gl7NTyb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Standardise and split the MNIST dataset:\n","\n","The original MNIST data is given in gray-scale values between 0 and 255.\n","You will need to write a normalisation method that takes in a ```torch.Tensor``` and performs normalisation.\n","The mean of MNIST is 0.1307 and it's standard deviation is 0.3081 (after division by 255)."],"metadata":{"id":"lSXLG4ksOUxR"}},{"cell_type":"code","source":["def apply_standardization(X): # define an standardisation function\n","  ### your code here\n","  return X"],"metadata":{"id":"pzaVKkcaORaS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# standardise the data\n","X_train, y_train = apply_standardization(mnist_train.train_data[indices[0]].float()), mnist_train.train_labels[indices[0]]\n","X_val, y_val = apply_standardization(mnist_train.train_data[indices[1]].float()), mnist_train.train_labels[indices[1]]\n","X_test, y_test =  apply_standardization(mnist_test.test_data.float()), mnist_test.test_labels"],"metadata":{"id":"NpRWm3ByOUMq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instantiate a ```torch.utils.data.TensorDataset``` for training, validation and test data:\n","\n","Remember that we use TensorDataset to be able to operate on the dataset without having to load it all in memory.\n","\n","And remember that torch likes all categorical data to be in a ```.long()``` format."],"metadata":{"id":"bDeXtYyxPhpr"}},{"cell_type":"code","source":["# create the TensorDatasets containing mnist_train, mnist_validate, and mnist_test\n","mnist_train =    ### your code here\n","mnist_validate = ### your code here\n","mnist_test =     ### your code here"],"metadata":{"id":"aiNJegkdPcZw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's visualise an example of the images and check whether the data is normalised properly (compute .mean() and .std() on the training set.)"],"metadata":{"id":"YFumPl0lPn-A"}},{"cell_type":"code","source":["plt.imshow(X_train[0], cmap = 'gray')\n","print(X_train.mean(), X_train.std())"],"metadata":{"id":"_lZSJgKiPnTQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Provided Train, Validation and Evaluate Functions\n","\n","There is an error in these functions. Can you spot it?"],"metadata":{"id":"t4v_90F-XALf"}},{"cell_type":"code","source":["def train(model, optimizer, criterion, data_loader):\n","    model.train()\n","    train_loss, train_accuracy = 0, 0\n","    for X, y in data_loader:\n","        X, y = X.to(device), y.to(device)\n","        optimizer.zero_grad()\n","        a2 = model(X.view(-1, 28*28)) \n","        loss = criterion(a2, y)\n","        loss.backward()\n","        train_loss += loss*X.size(0)\n","        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n","        optimizer.step()  \n","        \n","    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)\n","  \n","def validate(model, criterion, data_loader):\n","    model.eval()\n","    validation_loss, validation_accuracy = 0., 0.\n","    for X, y in data_loader:\n","        with torch.no_grad():\n","            X, y = X.to(device), y.to(device)\n","            a2 = model(X.view(-1, 28*28))\n","            loss = criterion(a2, y)\n","            validation_loss += loss*X.size(0)\n","            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n","            \n","    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)\n","  \n","def evaluate(model, data_loader):\n","    model.eval()\n","    ys, y_preds = [], []\n","    for X, y in data_loader:\n","        with torch.no_grad():\n","            X, y = X.to(device), y.to(device)\n","            a2 = model(X.view(-1, 28*28)) \n","            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","            ys.append(y.cpu().numpy())\n","            y_preds.append(y_pred.cpu().numpy())\n","            \n","    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)"],"metadata":{"id":"eF6ACgGlPuJg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Set the hyperparameters of your model\n","\n","- Seed: 42\n","- learning rate: 1e-2\n","- Optimizer: SGD\n","- momentum: 0.9\n","- Number of Epochs: 30\n","- Batchsize: 64\n","- Test Batch Size (no effect on training apart from time): 1000\n","- Shuffle the training set every epoch: Yes"],"metadata":{"id":"alGiYGXzYDqY"}},{"cell_type":"code","source":["seed = \n","lr = \n","momentum = \n","batch_size = \n","test_batch_size = \n","n_epochs = "],"metadata":{"id":"u7UXTrQTX9-i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Perform the training of the network and validation\n","\n","- Instantiate our model, optimizer and loss function\n","- Set the random number generator seed using ```set_seed``` to make everything reproducible.\n","- Use a sensible loss (criterion) for the multi-class classification problem."],"metadata":{"id":"xmSrFBryYHNG"}},{"cell_type":"code","source":["def train_model(momentum):\n","  set_seed(seed)\n","  model = LeNet5().to(device)\n","  optimizer = ### use SGD with momentum\n","  criterion = ### define the loss\n","  \n","  train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0)\n","  validation_loader = DataLoader(mnist_validate, batch_size=test_batch_size, shuffle=False, num_workers=0)\n","  test_loader = DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False, num_workers=0)\n","  \n","  liveloss = PlotLosses()\n","  for epoch in range(30):\n","      logs = {}\n","      train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","      logs['' + 'log loss'] = train_loss.item()\n","      logs['' + 'accuracy'] = train_accuracy.item()\n","\n","      validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n","      logs['val_' + 'log loss'] = validation_loss.item()\n","      logs['val_' + 'accuracy'] = validation_accuracy.item()\n","\n","      liveloss.update(logs)\n","      liveloss.draw()\n","      \n","  return model\n","\n","model = train_model(0.5)"],"metadata":{"id":"kJ6Xr5acYvsi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","Results obtained with the feed-forward network from previous sessions:\n","\n","<img src=\"https://drive.google.com/uc?id=1dcJ-7XgGceI7XtZ_wWhHhsUAv4G5vk4Z\" width=\"600\"/>\n","\n","<br>"],"metadata":{"id":"9UFo-wuZcdb_"}},{"cell_type":"markdown","source":["### Use the evaluation function defined above to make predictions.\n","\n","This method performs the same as validate but doesn't report losses, but simply returns all predictions on a given dataset (training, validation, test-set)"],"metadata":{"id":"hrBcU1Edcwij"}},{"cell_type":"code","source":["validation_loader = DataLoader(mnist_validate, batch_size=test_batch_size, shuffle=False, num_workers=0) # create a validation_loader\n","y_pred, y_gt = evaluate(model, validation_loader) # generate predictions and ground truths by evaluating the model"],"metadata":{"id":"nWkXr0EvY0JR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","fig, ax = plt.subplots(figsize=(6,6))\n","ConfusionMatrixDisplay.from_predictions(y_gt, y_pred, ax=ax, colorbar=False, cmap='bone_r')\n","plt.show()"],"metadata":{"id":"fWavLY-Ic2Cs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fully connected network confusion matrix was:\n","\n","<img src=\"https://drive.google.com/uc?id=1kzR5_QdAb8Me_lYBThhrE7MJIWfw5ZGg\" width=\"370\"/>\n"],"metadata":{"id":"_15i1HxufOel"}},{"cell_type":"markdown","source":["## Run a grid-search\n","\n","We will do a very simple hyperparameter grid-search by testing the optimal value of momemntum for this task.\n","\n","For that, we will define a new training function:"],"metadata":{"id":"EDJhqJyrl4gE"}},{"cell_type":"code","source":["model_m02 = train_model(0.2)"],"metadata":{"id":"uEkoEssjm8Qh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_m08 = train_model(0.8)"],"metadata":{"id":"3vczDdYHm8IP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set the best momentum value we have found:\n","momentum = 0.5"],"metadata":{"id":"ygg7LuCj24bV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the model on the full dataset and evaluate on the test set"],"metadata":{"id":"M7m0pFIpn-01"}},{"cell_type":"code","source":["mnist_train = MNIST(\"./\", download=True, train=True) # reload MNIST (why do we do this?)\n","\n","X_train, y_train = apply_standardization(mnist_train.train_data.float()), mnist_train.train_labels\n","mnist_train = TensorDataset(X_train, y_train)\n","train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4) # turn on multi-process data loading with the specified number of loader worker processes.\n","\n","set_seed(seed)\n","model = LeNet5().to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n","criterion = nn.CrossEntropyLoss()\n","\n","liveloss = PlotLosses()\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy.item()\n","    liveloss.update(logs)\n","    liveloss.draw()\n","    logs['val_' + 'log loss'] = 0.\n","    logs['val_' + 'accuracy'] = 0.\n","    \n","    \n","validation_loader = DataLoader(mnist_validate, batch_size=test_batch_size, shuffle=False, num_workers=0)\n","test_loader = DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False, num_workers=0)\n","\n","test_loss, test_accuracy = validate(model, criterion, test_loader)    \n","print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy.item())\n","print(\"\")\n"],"metadata":{"id":"9casp_cldQ1L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### [**Note on num_workers**]\n","\n","[`num_workers`](https://pytorch.org/docs/stable/data.html#multi-process-data-loading) have (or maybe had) a famous bug where setting it to any number bigger than 0 would result in memory leaks. The reason for that is related to how the processes are forked using `copy-on-write`. In principle, copy-on-write should not happen as we are only accessing data, but in reality it does happen because we are modifying something called reference count which results in actual writes on the data we are reading, and that activates the `write` in the `copy-on-write`. The result of that is that as we are iterating over the dataloader, we start creating more and more memory copies of our data which eventually can throw an 'out memory error'. More on this here: https://pytorch-dev-podcast.simplecast.com/episodes/dataloader-with-multiple-workers-leaks-memory."],"metadata":{"id":"V8Bfp1i0oKg-"}},{"cell_type":"code","source":[],"metadata":{"id":"wXcJzNfOmFAG"},"execution_count":null,"outputs":[]}]}