{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOUXDvSefwxMwcEJ6iCQQdw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n","\n","\n","---\n"],"metadata":{"id":"9YehS8enAmDn"}},{"cell_type":"markdown","source":["# **CNNs: convolutional neural networks (part 2)**\n","\n","#### **Morning contents/agenda**\n","\n","1. Commonly used datasets in computer vision\n","\n","2. Important CNN architectures\n","\n","3. U-nets and upsampling (unpooling & transpose convolutions)\n","\n","4. Transfer learning\n","\n","5. Summary of CNNs\n","\n","#### **Learning outcomes**\n","\n","1. Awareness of well-established CNN architectures\n","\n","2. Undersand how to upsample data\n","\n","3. Understand how and why transfer learning is used\n","\n","#### **Afternoon contents/agenda**\n","\n","1. Inspection of CNN filters\n","\n","2. Transfer learning from ImageNet to Bees and Ants\n","\n","#### **Learning outcomes**\n","\n","1. Become familiar with the effect that filters have (sometimes you can interpret them, sometimes they have abstracted the data too far to develop intuitions)\n","\n","2. Hands-on knowledge on how to apply transfer learning\n","\n","\n","<br/>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"9CY6juJtSkmD"}},{"cell_type":"code","source":["!pip install pycm livelossplot\n","%pylab inline\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from livelossplot import PlotLosses\n","from pycm import *\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.datasets\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","from torchsummary import summary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_kYki018eTFJ","executionInfo":{"status":"ok","timestamp":1669203793540,"user_tz":0,"elapsed":5084,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"}},"outputId":"e1a74213-70d8-49e0-8304-e9d8c1c4e3a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pycm in /usr/local/lib/python3.7/dist-packages (3.6)\n","Requirement already satisfied: livelossplot in /usr/local/lib/python3.7/dist-packages (0.5.5)\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from pycm) (1.21.6)\n","Requirement already satisfied: art>=1.8 in /usr/local/lib/python3.7/dist-packages (from pycm) (5.7)\n","Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n","Requirement already satisfied: ipython==7.* in /usr/local/lib/python3.7/dist-packages (from livelossplot) (7.9.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.7.5)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (2.0.10)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (2.6.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.2.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.8.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (57.4.0)\n","Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.18.2)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (5.1.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.4.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython==7.*->livelossplot) (0.8.3)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.*->livelossplot) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.*->livelossplot) (0.2.5)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (4.1.1)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (6.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.8.2)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (6.0.4)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (21.3)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->livelossplot) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.11.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython==7.*->livelossplot) (0.7.0)\n","Populating the interactive namespace from numpy and matplotlib\n"]}]},{"cell_type":"code","source":["def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LI8sNA9feT3H","executionInfo":{"status":"ok","timestamp":1669128190108,"user_tz":0,"elapsed":11,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"}},"outputId":"a33995c5-cef7-435e-e39d-2bf4a6a64766"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cuda installed! Running on GPU!\n"]}]},{"cell_type":"markdown","source":["## 1. Commonly used datasets in computer vision\n","\n","As we saw on the first week, the network capacity has to be adjusted in order to avoid overfitting to the data. In other words, very deep networks with large number of trainable parameters require big datasets because they have a lot of capacity to accomodate variations in the data. \n","\n","So far we have seen MNIST and similarly-smalled sized datasets:\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=\"400\"/></p><p align = \"center\">\n","<i>MNIST dataset: 60k training & 10k test images</i>\n","</p>\n","\n","\n","\n","\n","\n","<br>\n","\n","It is often desirable to have datasets of natural images, as they can be used for a broader range of applications than MNIST-like datasets. [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) are two datasets of natural images with 10 and 100 classes respectively:\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://production-media.paperswithcode.com/datasets/4fdf2b82-2bc3-4f97-ba51-400322b228b1.png\" width=\"400\"/></p><p align = \"center\">\n","<i>CIFAR-10 dataset: 50k training & 10k test images</i>\n","</p>\n","\n","<br>\n","\n","<p align = \"center\"><img src=\"https://miro.medium.com/max/1400/0*fqFMfJeP6CuBTuYc.webp\" width=\"400\"/></p><p align = \"center\">\n","<i>CIFAR-100 dataset: 50k training & 10k test images</i>\n","</p>\n","\n","<br>\n","\n","But larger datasets exist as well. [ImageNet](https://www.image-net.org/) has been used in various competitions, and it contains more than 14 million images and 20k classes:\n","\n","<p align = \"center\"><img src=\"https://i0.wp.com/syncedreview.com/wp-content/uploads/2020/06/Imagenet.jpg?fit=1400%2C600&ssl=1\" width=\"800\"/></p><p align = \"center\">\n","<i>ImageNet: >14M images and 20k classes </i>\n","</p>\n","\n","\n","\n","\n"],"metadata":{"id":"NVjTNdKHVF0H"}},{"cell_type":"markdown","source":["## 2. Important CNN architectures\n","\n","Since their introduction in 1998 with LeNet-5, convolutional neural networks have evolved significantly and competed for the top spot in computer vision tasks.\n","\n","You can find a [good overview here](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d) (the images below are from this website).\n","\n","<br>\n","\n","<img src=\"https://drive.google.com/uc?id=1BGcWnSRGLJmVzfRSQBujnkHtF9wTeQuq\" width=\"600\"/>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1PDNr20s96ddbabkX5dfjnUemw2ZMWvU4\" width=\"800\"/></center>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1FycJ5amqUL-Z_NXKtfmbejqID6pTFSsQ\" width=\"800\"/></center>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1yrpMY7PuyVG68M6gsnQhEHdGCbMZSYzo\" width=\"800\"/></center>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=16TLC9m8JvZw1V5fVx3cqzDowfGk4TKUT\" width=\"800\"/></center>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=178gJwbpE12TzKbHQfq3q8CTg4X-12gSZ\" width=\"800\"/></center>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=14tHV7AMln-qH4DwpjDZUbSrj6o0Jx1YX\" width=\"800\"/></center>\n","\n","<br>\n","\n","As you can see, network sizes increase over time thanks to advances in computational power (better GPUs with more memory, etc):\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1Bv0msGB95GXeiuKs5GQI6TMHYQQljz1z\" width=\"600\"/></center>\n","\n","<br>\n","\n","But even this numbers are considered small in modern architectures. For example:\n","\n","- **DALL·E-2** has 3.5 billion parameters\n","- **GPT-3** has 175 billion parameters (largest network so far, I think)\n","\n","\n","<br>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"sinIebTtMUvs"}},{"cell_type":"markdown","source":["## 2. U-nets and upsampling (unpooling & transpose convolutions)\n","\n","What are the outputs of the CNNs we have seen so far?\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1xSZ3Tb6mJgHit51fEZF9P5UojA0Tossm\" width=\"600\"/></center>\n","\n","<br>"],"metadata":{"id":"yPC2aDTe83n_"}},{"cell_type":"markdown","source":["<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1EITH6oofcQurxnnXNKriZ1dBkWmerlTX\" width=\"600\"/></center>\n","\n","<br>\n","\n","But CNNs have other applications. In the field of computer vision, a very common architecture is the **U-Net** which is a type of convolutional autoencoder (we will see autoencoders tomorrow):\n","\n","<p align = \"center\"><img src=\"https://drive.google.com/uc?id=1C5oJBIihVeyditn1I0nIjlZfEpkIhH2F\" width=\"800\"/></p><p align = \"center\">\n","<i> sources: <a href=\"https://arxiv.org/pdf/1505.04597.pdf\">original unet</a>, <a href=\"https://www.kaggle.com/c/tgs-salt-identification-challenge\"> seismic segmentation</a></i>\n","</p>\n","\n","<br>\n","\n","An important operation we perform to generate U-Net (and other architectures) is upscaling. The most common methods are:\n","\n","- nearest neighbour\n","- \"bed of nails\"\n","- Max unpooling\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1b1-3ncTi9cO7XFFuFljvfkwFlhKFHltU\" width=\"800\"/></center>\n","\n","<br>\n","\n","and\n","\n","- Transposed convolution or up-convolution, but **not deconvolution!** (blog with more details [here](https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8)):\n","\n","<center><img src=\"https://miro.medium.com/max/1400/1*kOThnLR8Fge_AJcHrkR3dg.gif\" width=\"400\"/></center>\n","\n","A couple of simple examples ([source](https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8)):\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1TuolaC0D0VWoItqHHoonFSufsOW9N0dR\" width=\"800\"/></center>\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1LxFq4_-uKooB96xuUjyKyG8Rb5u-6zU-\" width=\"800\"/></center>\n","\n","<br>\n","\n","Transpose convolutions are a bit tricky and can lead to checkerboard imprints on the outputs:"],"metadata":{"id":"oAWL3EvbUEOV"}},{"cell_type":"code","source":["%%html\n","<iframe src=\"https://distill.pub/2016/deconv-checkerboard/\" width=\"1000\" height=\"500\"></iframe>"],"metadata":{"id":"wXcJzNfOmFAG","colab":{"base_uri":"https://localhost:8080/","height":525},"executionInfo":{"status":"ok","timestamp":1669206503317,"user_tz":0,"elapsed":8,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"}},"outputId":"f5945aa5-a3af-4034-8ec2-5c1dc522411e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe src=\"https://distill.pub/2016/deconv-checkerboard/\" width=\"1000\" height=\"500\"></iframe>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["## 3. Transfer learning\n","\n","What is transfer learning and why is it useful? A definition from the Deep Learning book by Goodfellow et al (2016):\n","\n","*Transfer learning and domain adaptation refer to the situation where what has been learned in one setting ... is exploited to improve generalization in another setting.*\n","\n","<br>\n","\n","- The most well-known CNN designs are **available** on-line and have been **successfully trained** on very large number of images (millions).\n","\n","- In many applications we often work with a relatively **small number of images** (or data in general).\n","\n","- The idea of transfer learning is to use an existing trained CNN model which tries to solve a problem of similar nature and **tailor the model** to our particular application.\n","\n","The two main strategies are:\n","\n","1. **Add one (or a few layers) or retrain the last layers of a pre-trained network**: This strategy assumes that the filters of most of the network do a good job at extracting data features we can use. The last layers, then, act as a final fine-tunning to capture the specific features of our data.\n","\n","2. **Retrain the whole network with small learning rates:** This strategy assumes that as a whole, the network captures data features well, and it only needs a bit of a ‘nudge’ to adapt the network parameters to our particular problem. In this case, we want to keep the underlying abstraction that the network does at different scales, but fine-tune it to our problem.\n","\n","We will see examples of both in this afternoon exercise."],"metadata":{"id":"4LzLdZmAdnPd"}},{"cell_type":"markdown","source":["## 4. Receptive field\n","\n","The receptive field is defined as the region of the input that affects the output, and it can be defined between adjacent or non-adjacent CNN layers.\n","\n","<br>\n","\n","<center><img src=\"https://drive.google.com/uc?id=1GfG26m6Xee9qhyiA-ARvbTnzAxbSYo--\" width=\"800\"/></center>\n","\n","<br>\n","\n","Why are we interested in the receptive field in our network?\n","\n","Because the receptive field will determine what are the hyperparameters\n","I need to ensure full receptive field on my inputs:\n","\n","- filter size and stride\n","- number of convolutional layers in the network\n","\n","[Here](https://www.baeldung.com/cs/cnn-receptive-field-size) you can find a more detailed explanation with a few formulas to compute receptive fields."],"metadata":{"id":"eKJiEBxrgHc3"}}]}