{"cells":[{"cell_type":"markdown","metadata":{"id":"jhbKV3-dAa2d"},"source":["<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n","\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"rQORldhuYS7_"},"source":["# Sentiment analysis with GloVe (Solution)\n","\n","### Exercise objectives:\n","- Learn how to embed data with GloVe (and similar embedding like Word2Vec)\n","- Build a data pipeline to prepare text data\n","- Train a simple LSTM model for sentiment analysis\n","\n","<hr>\n","\n","\n","# The data\n","\n","Today we will use the IMDB movie review dataset. It is a classic data set that can be downloaded staight from Pytorch. It contains reviews of different movies, as well as a target: 1 for bad review, 2 for a good review. The goal is to train an LSTM model to predict whether a review is good, or bad.\n","\n","# Installing Dependencies\n","\n","But first, let's install a few libraries that you are unlikely to have if you are running this on your colab instance:"]},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T10:56:06.628896Z","start_time":"2022-12-09T10:48:31.760441Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11079,"status":"ok","timestamp":1670759195354,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"JET7pqXzYS8C","outputId":"8ae46a1d-45c0-490d-9355-755b33b56dfe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchdata\n","  Downloading torchdata-0.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[K     |████████████████████████████████| 4.5 MB 4.7 MB/s \n","\u001b[?25hCollecting portalocker>=2.0.0\n","  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchdata) (1.13.0+cu116)\n","Collecting urllib3>=1.25\n","  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 53.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchdata) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchdata) (4.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2022.9.24)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 63.3 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (3.0.4)\n","Installing collected packages: urllib3, portalocker, torchdata\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed portalocker-2.6.0 torchdata-0.5.0 urllib3-1.25.11\n"]}],"source":["!pip install torchdata"]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T11:05:42.134671Z","start_time":"2022-12-09T11:05:39.702494Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5968,"status":"ok","timestamp":1670759201316,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"rzuGT6bFYS8D","outputId":"5b941faa-8edd-4b19-b041-331551b19df5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"]}],"source":["!pip install nltk"]},{"cell_type":"markdown","metadata":{"id":"XmrW5-EblSWM"},"source":["## Download NLTK files\n","\n","We also need to download a few key files for NLTK (this could take a minute or two o run)"]},{"cell_type":"code","execution_count":3,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T12:33:40.374621Z","start_time":"2022-12-09T12:33:40.366004Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2721,"status":"ok","timestamp":1670759204031,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"2tZGs_brYS8E","outputId":"6148f460-0dee-4005-b32e-6847df2612a8"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"markdown","metadata":{"id":"DdAbnwemljk3"},"source":["# Importing torch \n","\n","Let's import torch and a few common dependencies, and make sure we set our device to the GPU:"]},{"cell_type":"code","execution_count":4,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T10:48:09.786005Z","start_time":"2022-12-09T10:48:09.444438Z"},"executionInfo":{"elapsed":522,"status":"ok","timestamp":1670759204550,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"v1rF6Q_4YS8F"},"outputs":[],"source":["import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":5,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T11:12:27.293028Z","start_time":"2022-12-09T11:12:27.285440Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5599,"status":"ok","timestamp":1670759210148,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"Yr4ItymxYS8F","outputId":"0a1fec1d-03bc-4951-ab4f-1498c2dbdd13"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":5}],"source":["import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"markdown","metadata":{"id":"WYBrwQ1rlvI4"},"source":["# Importing and visualizing the data\n","\n","Running the cell below will import that IMDB movie reviews in your notebook, and we can then visualize some results. We can import the reviews straight from datasets."]},{"cell_type":"code","execution_count":6,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T11:01:45.019715Z","start_time":"2022-12-09T11:01:44.140864Z"},"executionInfo":{"elapsed":18944,"status":"ok","timestamp":1670759229089,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"vLJ58FP0YS8G"},"outputs":[],"source":["from torchtext import data,datasets\n","from sklearn.model_selection import train_test_split\n","import torchdata\n","\n","train_dataset, test_dataset  = datasets.IMDB(root = '.data', split = ('train', 'test'))\n","\n","# Let's create a true test data and a true validation data as lists:\n","test_dataset, val_dataset = train_test_split(list(test_dataset), train_size=.8)"]},{"cell_type":"markdown","metadata":{"id":"zU1u79i0nYx2"},"source":["## Let's see the 5 first reviews\n","\n","Note that they are tuples: 1 means a bad review, 2 means a good review."]},{"cell_type":"code","execution_count":7,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T11:02:05.364343Z","start_time":"2022-12-09T11:02:05.299756Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1670759229091,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"Axkzp6H7YS8H","outputId":"0320b1ad-3dce-4a55-bbd0-46b317545d3f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1,\n","  'I was looking for a documentary of the same journalistic quality as Frontline or \"Fog of War\" (by Errol Morris). Instead I was appalled by this shallow and naive account of a very complex and disturbing man and his regime: Alberto Fujimori. This movie should be called \"The return of Fujimori\". The director presumes she made a \"perfect\" movie because alienates both pro and anti-Fujimori factions when in fact it is a very biased and unprofessional piece of work. <br /><br />The movie has few crucial facts wrong: <br /><br />1) She uses the so called \"landslide\" election of 1995 in which Fujimori was re-elected with 65% of the vote, as an example of the massive popular support of Fujimori. But we all now know to be the fruit of a very organized electoral fraud.<br /><br />2) The movie states that Sendero Luminoso (Shining Path) killed 60,000 people. In fact, the Truth Commission\\'s final report states that there were 69,280 deaths due to political violence in Peru. 33% of those were caused by SL. That leaves the other 67% in the hands of the police, military and other groups. The fact that she uses the same misleading information that Fujimori has been using for 10 years it is another example of how terrible this movie is. <br /><br />For any person with some education on Peruvian politics and history, Fujimori is clearly a consummated manipulator, a delusional character and remorseless egomaniac. His regime was very far from being democratic. He is still a menace to Peruvians. Despite these facts the director lets Fujimori tell the story. Not only on how he wants the camera to be positioned but the narrative and direction of the film seem to be part of his political agenda. He always seems to have the last word. There are no journalistic \"cojones\", just soft questions and unchallenged remarks. Where is Oriana Fallaci when we need her? The director, when questioned after the screening, didn\\'t hide the fact that she was deeply impressed by Fujimori, his charm and intelligence. Yes, she has been definitely charmed by him, and you can tell by looking at this film. It\\'s obvious she has a very hard time to digest the multitude of facts that point towards his responsibility on the corruption, murder and deception that took place. She assured the gasping audience that Fujimori was really a \"patriot\" when few moments earlier, one of the leading Peruvian journalists was very adamant in telling us that Fujimori was, above all, a \"traitor\". She went on to say that despite all the accusations not \"a single dollar\" was found on any bank account on his name, etc, etc. It was like hearing again the same gang of ruthless thugs that ruled the country for 10 years defending their master. It was a sad moment for journalism.<br /><br />This film makes injustice to history. It is an insult to hundreds of dead people, disappeared or unjustly incarcerated by Fujimori\\'s regime. No wonder she later confessed that all the Peruvian intellectuals she befriended while making the movie felt betrayed by it. Unbiased? The words \"oportunistic\", \"naïve\" and \"denial\" come to my mind instead.'),\n"," (2,\n","  \"'Captain Corelli's Mandolin' is a fantastic film in itself. It is nothing like the book, which may disapoint its ardent followers. Yet, viewed on it's own, the film is a masterpiece. The views are spectacular and the acting isn't too bad either!! Nicolas Cage was brilliant-so different from his usual action hero type characters. Penelope cruz is superb and really holds the film together. I think that this film has to be judged as an indivdual project-not related to the book. Louis de Bernieres gave up rights to the film script, so the film is an interpretation of the director, john maddon. Go and see this film with an open mind-you'll love it; because underneath is the touching story of love and war.\"),\n"," (2,\n","  'Now, lissen you guys, I LOVED THIS FILM, though not quite as much as FAREWELL TO THE KING, another beloved John Milius epic. It was fun, a lot more than if it were based on a Tennessee Williams drama. It\\'s a great yarn, with a whiff of political correctness. I love this film for its beautiful photography, its humor and its attenuated criticism of the Bad Guys (Berbers) and the REAL Bad Guys, the spear- carriers for the acquisitive \\'civilized\\' world, with their repeating rifles, artillery and large gunboats out there in the harbor. <br /><br />The standout scene is the Berber encampment with blue-gray smoke from the cooking fires rising into the chill desert air. It is visually eloquent, highly evocative.<br /><br />Set in 1904 Morocco, WIND features a helpless American woman (Candace Bergen) who is taken hostage by a dashing, albeit immodest, Berber bandit (Sean Connery-the very model ofa Scottish Muslim nomad). The exciting story is based on a few historical facts. The photography is Milius beautiful, punctuated by Jerry Goldsmith\\'s outstanding score.<br /><br />Mrs Pedicaris and the Raisuli conduct protracted foreplay and bounce around in the desert between oases. Even though the Raisuli proudly traces his lineage back to the apes, he is a perfect gentleman - he even lets her keep her head after she beats him at chess! A Marine detachment storms the Bashaw\\'s palace, putting out the fires of competing hegemonies with gasoline. Don\\'t mess with the Corps, Abdul. <br /><br />There are many entertaining stereotypes:<br /><br />Despicable Sultan - resembles a dissipated ferret. Definitely not a Liberal.<br /><br />Cruel German Officer - a large, bellicose Dachshund sporting a monocle. He gallantly chooses to fight the Raisuli with swords instead of gunning him down in the manner of Indiana Jones. Noblesse oblige, by way of Von Clausewitz?<br /><br />Dashing Marine Officer - kicks the crap out of the Bashaw of Tangier\\'s army and storms his palace while chewing tobacco. His speech is mildly aphasic. The Bashaw begged him not to breathe on him.<br /><br />The Berbers - a horde of groveling sycophants led by a charming megalomaniac. None of them take baths, except perhaps in camel urine.<br /><br />President Teddy Roosevelt is undeservedly portrayed as vacuous and preoccupied with guns, toys and stuffed grizzly bears.<br /><br />Beautiful American widow - gives the men a lesson in courage, as do her two children. She evidently has a huge supply of clean, starched clothes and rarely has a hair out of place. <br /><br />The Raisuli sends Teddy Roosevelt a message, thanking him for the gift of a Remington repeating rifle, declaring,<br /><br />\"MEESTER ROOSEVELT, YOU ARE THE LION AND AIEE AM THE BREAKING WIND.\"<br /><br />How true.<br /><br />Please do not take my acerbic remarks to mean that I did not like the film. I had almost as much fun writing this as watching da Pitcher.'),\n"," (1,\n","  \"I think vampire movies (usually) are wicked. Even if the film itself isn't all that good, I still like it 'cos its got vampires in it. But this stinks. It really does give vampire movies a bad name.<br /><br />For a start, the cheapness really shows. I'm not usually that bothered about low budget films - one of my favourite all-time movies is El Mariachi which only cost $7000 - but I hate this. The actions a load of crap as well, resorting to a 'stylish' wobbling camera which gives you headache.<br /><br />Theres not much more to say other than don't watch this. I bought it for £1.50 as it was an ex-rental and I feel cheated out of my money, even for that low price.\"),\n"," (1,\n","  \"...and boy is the collision deafening. A female telephone lineman is taken over by the spirit of a recently-deceased ninja, strips down to her undies, pours tomato juice on her body so her boyfriend can lick it off, performs a seductive dance, then goes off to kill the policemen who killed the ninja she's possessed by. Only to be hunted down by a one-eyed ninja master. Just like in real life, eh? Enlivened only by Sho Kosugi's martial arts choreography (and his declining to put his obnoxious kids in this one), you really have to see this to believe it. It's the ultimate mix of totally at-odds genres.\")]"]},"metadata":{},"execution_count":7}],"source":["val_dataset[:5]"]},{"cell_type":"markdown","metadata":{"id":"fcsmirLuoIa1"},"source":["## Choosing one example\n","\n","We will need one example of text to prepare it. So, let's take the first review of your validation set:"]},{"cell_type":"code","execution_count":8,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T12:28:07.180134Z","start_time":"2022-12-09T12:28:07.171558Z"},"colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"elapsed":253,"status":"ok","timestamp":1670759229335,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"M8FDmQt2YS8I","outputId":"456d3323-ef37-49c8-9f10-3d441d431ecc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'I was looking for a documentary of the same journalistic quality as Frontline or \"Fog of War\" (by Errol Morris). Instead I was appalled by this shallow and naive account of a very complex and disturbing man and his regime: Alberto Fujimori. This movie should be called \"The return of Fujimori\". The director presumes she made a \"perfect\" movie because alienates both pro and anti-Fujimori factions when in fact it is a very biased and unprofessional piece of work. <br /><br />The movie has few crucial facts wrong: <br /><br />1) She uses the so called \"landslide\" election of 1995 in which Fujimori was re-elected with 65% of the vote, as an example of the massive popular support of Fujimori. But we all now know to be the fruit of a very organized electoral fraud.<br /><br />2) The movie states that Sendero Luminoso (Shining Path) killed 60,000 people. In fact, the Truth Commission\\'s final report states that there were 69,280 deaths due to political violence in Peru. 33% of those were caused by SL. That leaves the other 67% in the hands of the police, military and other groups. The fact that she uses the same misleading information that Fujimori has been using for 10 years it is another example of how terrible this movie is. <br /><br />For any person with some education on Peruvian politics and history, Fujimori is clearly a consummated manipulator, a delusional character and remorseless egomaniac. His regime was very far from being democratic. He is still a menace to Peruvians. Despite these facts the director lets Fujimori tell the story. Not only on how he wants the camera to be positioned but the narrative and direction of the film seem to be part of his political agenda. He always seems to have the last word. There are no journalistic \"cojones\", just soft questions and unchallenged remarks. Where is Oriana Fallaci when we need her? The director, when questioned after the screening, didn\\'t hide the fact that she was deeply impressed by Fujimori, his charm and intelligence. Yes, she has been definitely charmed by him, and you can tell by looking at this film. It\\'s obvious she has a very hard time to digest the multitude of facts that point towards his responsibility on the corruption, murder and deception that took place. She assured the gasping audience that Fujimori was really a \"patriot\" when few moments earlier, one of the leading Peruvian journalists was very adamant in telling us that Fujimori was, above all, a \"traitor\". She went on to say that despite all the accusations not \"a single dollar\" was found on any bank account on his name, etc, etc. It was like hearing again the same gang of ruthless thugs that ruled the country for 10 years defending their master. It was a sad moment for journalism.<br /><br />This film makes injustice to history. It is an insult to hundreds of dead people, disappeared or unjustly incarcerated by Fujimori\\'s regime. No wonder she later confessed that all the Peruvian intellectuals she befriended while making the movie felt betrayed by it. Unbiased? The words \"oportunistic\", \"naïve\" and \"denial\" come to my mind instead.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["example = val_dataset[0][1]\n","example"]},{"cell_type":"markdown","metadata":{"id":"_viiDpMDoaSC"},"source":["# Time to play with GloVe\n","\n","We want to embed our text with a pre-trained model. In the lecture, we talked about Word2Vec, which is one of the best embeddings. But Word2Vec is harder to implement in PyTorch because it is not offered as a pre-trained layer. Instead, we will play with GloVe, which is very similar to Word2Vec in concept and is readily available in PyTorch.\n","\n","## Download GloVe\n","\n","There are different versions of GloVe, depending on the size of the vector you want to use for your embedding, and the size of your vocabulary. Here, to make things as fast as possible, let's download the version with vectors embedded in 50 dimensions, and the smallest vocabulary possible. This is still a large file (862 Mb) so on my system this took almost 3 minutes to run: be patient!"]},{"cell_type":"code","execution_count":9,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T14:59:52.269152Z","start_time":"2022-12-09T14:59:51.781462Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":175525,"status":"ok","timestamp":1670759404857,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"NNbz_Ki0YS8I","outputId":"175d86c8-1ed7-457a-9139-c284dae8c199"},"outputs":[{"output_type":"stream","name":"stderr","text":[".vector_cache/glove.6B.zip: 862MB [02:39, 5.41MB/s]                           \n","100%|█████████▉| 19999/20000 [00:00<00:00, 43242.41it/s]\n"]}],"source":["from torchtext.vocab import GloVe\n","\n","glove = GloVe(dim='50', name='6B', max_vectors=20000)"]},{"cell_type":"markdown","metadata":{"id":"MGRUNm38qFCl"},"source":["## Words to vectors\n","\n","Now that we have GloVe downloaded, we can see how it works. We can, for instance, very easily find the vector for any word. Here is how we can find the representation of the word 'king':"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1670759404858,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"vQvhqzFRqXsC","outputId":"2a36595f-5123-460c-9f2a-38795bc347fe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.5045,  0.6861, -0.5952, -0.0228,  0.6005, -0.1350, -0.0881,  0.4738,\n","        -0.6180, -0.3101, -0.0767,  1.4930, -0.0342, -0.9817,  0.6823,  0.8172,\n","        -0.5187, -0.3150, -0.5581,  0.6642,  0.1961, -0.1349, -0.1148, -0.3034,\n","         0.4118, -2.2230, -1.0756, -1.0783, -0.3435,  0.3350,  1.9927, -0.0423,\n","        -0.6432,  0.7113,  0.4916,  0.1675,  0.3434, -0.2566, -0.8523,  0.1661,\n","         0.4010,  1.1685, -1.0137, -0.2158, -0.1515,  0.7832, -0.9124, -1.6106,\n","        -0.6443, -0.5104])"]},"metadata":{},"execution_count":10}],"source":["glove[\"king\"]"]},{"cell_type":"markdown","metadata":{"id":"IvwDUhbrsD_d"},"source":["## GloVe tokens (index)\n","\n","Importantly, we will need to obtain the index (or token) of a word in GloVe. This is because we will transform our sentence into integer token before passing it to the network, and these integer tokens will need to correspond to the index of the vectors for our embedding layer.\n","\n","There are two useful functions for us:\n","- string to integer (stoi)\n","- integer to string (itos)\n","\n","They do pretty much what is written on the tin:"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1670759404860,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"nxaePJLVqltU","outputId":"d97594b7-7fab-4681-cc9b-06b0c35947e6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["691"]},"metadata":{},"execution_count":11}],"source":["glove.stoi['king']"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1670759404861,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"Y22r86s8ssZ0","outputId":"98bbb602-9282-4429-9279-3d0437721228"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'king'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["glove.itos[691]"]},{"cell_type":"markdown","metadata":{"id":"CV9W4dOcr9pE"},"source":["## Feel free to play with vectors!\n","\n","Explore the embedding, and how each words are represented..."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1670759404862,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"3z2vvcmSq0FR","outputId":"a0f0146f-470c-4f59-ea49-163f4172eec7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.3785,  1.8233, -1.2648, -0.1043,  0.3583,  0.6003, -0.1754,  0.8377,\n","        -0.0568, -0.7580,  0.2268,  0.9859,  0.6059, -0.3142,  0.2888,  0.5601,\n","        -0.7746,  0.0714, -0.5741,  0.2134,  0.5767,  0.3868, -0.1257,  0.2801,\n","         0.2813, -1.8053, -1.0421, -0.1926, -0.5537, -0.0545,  1.5574,  0.3930,\n","        -0.2475,  0.3425,  0.4536,  0.1624,  0.5246, -0.0703, -0.8374, -1.0326,\n","         0.4595,  0.2530, -0.1784, -0.7340, -0.2002,  0.2347, -0.5609, -2.2839,\n","         0.0093, -0.6028])"]},"metadata":{},"execution_count":13}],"source":["glove['queen']"]},{"cell_type":"markdown","metadata":{"id":"vqIYe5PhtALK"},"source":["# Exercise 1: Text preprocessing\n","\n","Your first exercise will be to build a pre-processing pipeline. You can use the code in the lecture to help you do that. Also, I already created the functions signature below to help you understand what is needed. Each function is applied to a single review, not a batch of reviews (you will see this below).\n","\n","The transformations you need to do are the following:\n","1. Text needs to be all lower case (all words in GloVe are lower case only)\n","2. You need to remove numbers - they are not helpful for sentiment analysis\n","3. Remove punctuation (also not needed)\n","4. Transform the sentence into word tokens using NLTK. Now your sentence becomes a list of words.\n","5. Remove stopwords using NLTK\n","6. Get the index of the word in GloVe. If the word exists in the dictionary, keep it in the list. If not, just don't add it. This is a little bit more challenging, so consult the solution if you are unsure.\n","7. We also need to pad our sentence to MAX_LEN (the maximum length of the sentences). Notice below that I set this to be 100 words: we don't really need more than that, and we are getting good results (well, decent results) with 100 words. But all tensors need to be the same length, so pad_sentence is here to add zeros to those sentences that are shorted than 100 words. Again, consult the solution if you struggle with this one.\n","\n","I have left the last function in for you: transform_text calls each of the other functions one after another, and transforms the text.\n","\n","Make sure that you obtain a tensor of dimension 100, with all the right token, when you call transform_text on your example sentence from above. Then continue with the exercise.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1670759404863,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"5S3Ro7M3fC2C"},"outputs":[],"source":["MAX_LEN = 100"]},{"cell_type":"code","execution_count":15,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T14:59:53.673619Z","start_time":"2022-12-09T14:59:53.654730Z"},"executionInfo":{"elapsed":257,"status":"ok","timestamp":1670759405105,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"hArHwgpgYS8J"},"outputs":[],"source":["from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import string \n","from torch.nn.functional import pad \n","\n","def remove_numbers(txt):\n","    txt = ''.join(word for word in txt if not word.isdigit())\n","    return txt\n","\n","def remove_punctuation(txt):\n","    \n","    for punctuation in string.punctuation:\n","        txt = txt.replace(punctuation, '') \n","    \n","    return txt\n","\n","def tokenize(txt):\n","    word_tokens = word_tokenize(txt) \n","    return word_tokens\n","\n","def remove_stopwords(word_tokens):\n","    stop_words = set(stopwords.words('english')) \n","    word_tokens = [w for w in word_tokens if not w in stop_words] \n","    return word_tokens\n","\n","\n","def get_index(txt, vocab=glove):\n","    embedded_text = []\n","    \n","    for word in txt:\n","        try:\n","            embedded_text.append(glove.stoi[word])\n","        except:\n","            pass\n","    \n","    return embedded_text\n","\n","def pad_sentence(txt):\n","    if txt.shape[0]>=MAX_LEN:\n","        return txt[:MAX_LEN]\n","    else:\n","        return pad(txt, (0, MAX_LEN-txt.shape[0]), 'constant',0).long()\n","        \n","\n","def transform_text(txt):\n","    txt = txt.lower()\n","    txt = remove_numbers(txt)\n","    txt = remove_punctuation(txt)\n","    txt = tokenize(txt)\n","    txt = remove_stopwords(txt)\n","    txt = torch.tensor(get_index(txt)).long()\n","    return pad_sentence(txt)"]},{"cell_type":"code","execution_count":16,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T15:37:09.071016Z","start_time":"2022-12-09T15:37:09.059162Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670759405106,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"VdKb3KouYS8J","outputId":"747ac557-37b2-4d14-ff5f-c9e5a451b3ca"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([  862,  3830, 17940,  1506, 16469,  9667,   136,  4239,   773, 19827,\n","         8966, 14427,  1530,  1688,  9392,   300,  1872,  6922,  9274,  1005,\n","          175,   498,  9274,   369,   116,  2615,  1005,  1396,  5104,   853,\n","        14222,  2365,   161,  1005,  2892,  4490,  1797,  2054,   175,  8036,\n","          367,  9274, 19283,   538,   880,  1971,   814,   280,  9274,   346,\n","         4138,  2112,  2799,  1005,   112, 13001,  2818,   256,    69,   853,\n","         2745,  8631,   294,   255,   112,  1933,   445,   209,   714,  4068,\n","         1098,  2200,  1281,   142,   178,   503,   853,  2054, 10747,   419,\n","         9274,   622,    82,   170,   880,  5510,  1005,   899,   631,  8340,\n","         1378,   299,  9274,  2056,  1395,  1872,   372,   590,   149, 14875])"]},"metadata":{},"execution_count":16}],"source":["transform_text(example)"]},{"cell_type":"markdown","metadata":{"id":"aPlOVZYJvWG7"},"source":["# Preparing the dataset\n","\n","Now that you have written the helper functions, we can focus on preparing the dataset. First, I will create the target and the label for the train, test, and validation splits using a list comprehension. This will be a dataset not saved as a dataloader, and it will be useful to assess the loss and the accuracy of each one of our splits. But it won't be used as a batch.\n","\n","Run the code below. Beware, due to the size of the dataset this can be a bit of a lengthy cell to run!"]},{"cell_type":"code","execution_count":17,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T16:25:24.947957Z","start_time":"2022-12-09T16:25:23.971950Z"},"executionInfo":{"elapsed":81231,"status":"ok","timestamp":1670759486331,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"ugdic1aQYS8K"},"outputs":[],"source":["train_y = torch.tensor([item[0] for item in list(train_dataset)])-1\n","train_x = torch.stack([transform_text(item[1]) for item in list(train_dataset)])\n","\n","val_y = torch.tensor([item[0] for item in list(test_dataset)])-1\n","val_x = torch.stack([transform_text(item[1]) for item in list(test_dataset)])\n","\n","test_y = torch.tensor([item[0] for item in list(test_dataset)])-1\n","test_x = torch.stack([transform_text(item[1]) for item in list(test_dataset)])"]},{"cell_type":"markdown","metadata":{"id":"Vl9SIo5iv8f1"},"source":["\n","## Dataloader\n","\n","To be able to run batches on the GPU, we will need to save the dataset in a DataLoader class. We will also create a function called vectorize_batch, that will allow us to vectorise our batches on the go (i.e. batch by batch). Take note of how I wrote the train_loader: I use the dataset, define the batch size (256), and collate function (my vectorizer), and (very important!) I set shuffle to 'True'.\n","\n","Not setting shuffle to True results in the batch not being randomly shuffled, and a poor performance during training:"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":588,"status":"ok","timestamp":1670759486911,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"4Zm9TKDFf3ox"},"outputs":[],"source":["from torchtext.data import to_map_style_dataset\n","from torch.utils.data import DataLoader\n","\n","def vectorize_batch(batch):\n","    Y, X = list(zip(*batch))\n","    \n","    X_embedded = torch.stack([transform_text(txt) for txt in X])\n","    \n","    return X_embedded, torch.tensor(Y).long()-1 \n","\n","train_dataset=  to_map_style_dataset(train_dataset)\n","\n","train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=vectorize_batch, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"JR9Y3CXtwm8G"},"source":["Let's quickly check that the dataloader works. You should obtain a size of [256, 100] for X, and [256] for Y:"]},{"cell_type":"code","execution_count":19,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T16:25:32.264070Z","start_time":"2022-12-09T16:25:32.048861Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":482,"status":"ok","timestamp":1670759487390,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"eKSlEG9iYS8K","outputId":"df41a708-fc35-42a2-9216-8ee41365cb9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([256, 100]) torch.Size([256])\n"]}],"source":["for X, Y in train_loader:\n","    print(X.shape, Y.shape)\n","    break"]},{"cell_type":"markdown","metadata":{"id":"EInk3QjSwxmT"},"source":["# The embedding layer\n","\n","I have written a function for you that creates an embedding layer. We will pass to it the vectors of our GloVe object, and from this, we will now the number of embeddings (20000) and the embedding dimensions (50). \n","\n","We then pass the weights of the GloVe vocabulary to our embedding layer, and we choose to set the weight to 'not trainable'. This way, we won't try to relearn the correct weights for this task. But we can also choose to start with the GloVe weights, and then update them to suite our vocabulary.\n","\n","This is a classic example of transfer learning."]},{"cell_type":"code","execution_count":20,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T16:25:33.144702Z","start_time":"2022-12-09T16:25:33.137652Z"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1670759487390,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"pzHlnkI-YS8K"},"outputs":[],"source":["def create_emb_layer(weights_matrix, non_trainable=True):\n","    num_embeddings, embedding_dim = weights_matrix.size()\n","    emb_layer = nn.Embedding(num_embeddings, embedding_dim,padding_idx=0)\n","    emb_layer.load_state_dict({'weight': weights_matrix})\n","    if non_trainable:\n","        emb_layer.weight.requires_grad = False\n","\n","    return emb_layer, num_embeddings, embedding_dim"]},{"cell_type":"markdown","metadata":{"id":"dtJudwsSxSZf"},"source":["# Network architecture\n","\n","We will use an LSTM model, as you did on Friday. But this time, we will use the LSTM layer from nn.LSTM: yes, you do not need to write it from scratch!\n","\n","In fact, you don't need to write anything. I wrote the network for you, as this is very time consuming and we only have half a day for theory and practice today.\n","\n","Note the use of the embedding layer, as well as the fact that I use 2 LSTM layers."]},{"cell_type":"code","execution_count":21,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T17:00:30.539306Z","start_time":"2022-12-09T17:00:30.524277Z"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670759487390,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"vNdAV-m6YS8L"},"outputs":[],"source":["from torch import nn\n","\n","class LSTM(nn.Module):\n","    def __init__(self, hid_dim, output_dim):\n","        super(LSTM, self).__init__()\n","        \n","        self.hid_dim = hid_dim\n","        \n","        self.embedding, num_embeddings, embedding_dim = create_emb_layer(glove.vectors, False)\n","        \n","        n_layers = 2\n","\n","        self.lstm = nn.LSTM(embedding_dim, hid_dim, n_layers,dropout=0, batch_first=True)\n","        self.linear = nn.Linear(hid_dim,100)\n","        self.relu = nn.ReLU()\n","        self.fc = nn.Linear(100, output_dim)\n","        self.dropout = nn.Dropout(.5)\n","        \n","        self.reset_parameters()\n","        \n","    def reset_parameters(self):\n","        std= 1.0 / np.sqrt(self.hid_dim)\n","        \n","        for w in self.parameters():\n","            w.data.uniform_(-std, std)\n","        \n","\n","    def forward(self, text):\n","\n","        embedded = self.embedding(text)\n","\n","\n","        batch_size, seq_len,  _ = embedded.shape\n","        hid_dim = self.lstm.hidden_size\n","            \n","        outputs, (hidden, cell) = self.lstm(embedded)\n","\n","        outputs = outputs[:, -1]\n","        \n","        prediction = self.fc(self.dropout(self.relu(self.linear(outputs))))\n","\n","\n","        return prediction"]},{"cell_type":"markdown","metadata":{"id":"GNW_tIBZx0aD"},"source":["# Training and validation functions\n","\n","Below are my training and validation functions. Take a moment to study them, and then run the code."]},{"cell_type":"code","execution_count":22,"metadata":{"ExecuteTime":{"end_time":"2022-12-09T17:00:34.774166Z","start_time":"2022-12-09T17:00:34.753945Z"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670759487391,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"RmbC3MJcYS8M"},"outputs":[],"source":["from tqdm import tqdm\n","from sklearn.metrics import accuracy_score\n","import torch.nn.functional as F\n","import gc\n","\n","def CalcValLossAndAccuracy(model, loss_fn, val_X, val_Y):\n","    \n","    #print(f'Calculating Epoch Loss and Accuracy:')\n","    \n","    losses = []\n","    accuracies = []\n","    model.eval()\n","    with torch.no_grad():\n","        X, Y, title = (val_x, val_y,'Validation')\n","        X = val_X.to(device)\n","        Y = val_Y.to(device)\n","            \n","        outputs = model(X).squeeze()\n","        loss = loss_fn(outputs, Y.float())\n","            \n","        preds = [1 if p>=.5 else 0 for p in torch.sigmoid(outputs)]\n","        accuracy = accuracy_score(Y.detach().cpu().numpy().tolist(),preds)\n","            \n","        accuracies.append(accuracy)\n","        losses.append(loss)\n","\n","        \n","        print(f'{title} Loss : {loss:.3f}')\n","        print(f\"{title} Accuracy  : {accuracy:.3f}\")\n","    \n","    return losses, accuracies\n","\n","\n","def TrainModel(model, loss_fn, optimizer, train_loader, epochs=10):\n","    train_losses = []\n","    train_accuracy = []\n","    val_losses = []\n","    val_accuracy = []\n","    \n","    for i in range(1, epochs+1):\n","        \n","        print('-'*100)\n","        print(f'EPOCH {i}')\n","        print('-'*100)\n","        \n","        epoch_losses = []\n","\n","        model.train()\n","\n","        \n","        for X, Y in tqdm(train_loader, colour='BLUE'):\n","\n","            X = X.to(device)\n","            Y = Y.to(device)\n","            \n","            Y_preds = model(X).squeeze()\n","            loss = loss_fn(Y_preds, Y.float())\n","            \n","            epoch_losses.append(loss.item())\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","        print(\"Train Loss : {:.3f}\".format(torch.tensor(epoch_losses).mean()))\n","        \n","        losses, acc = CalcValLossAndAccuracy(model, loss_fn, val_x, val_y)\n","        train_losses.append(losses[0])\n","        train_accuracy.append(losses[0])\n","        val_losses.append(acc[0])\n","        val_accuracy.append(acc[0])\n","        \n","    return train_losses, val_losses, train_accuracy, val_accuracy"]},{"cell_type":"markdown","metadata":{"id":"prvDZBNox-6V"},"source":["# Training the model\n","\n","We will now train the model using the RMSprop optimizer, and 20 hidden units in our model. We will run for only 8 epochs for now:"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":242371,"status":"ok","timestamp":1670759729756,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"RaQv-ot5zWKj","outputId":"963b7166-4cb5-4d33-e7dc-7c0dca6f050e"},"outputs":[{"output_type":"stream","name":"stdout","text":["STARTING TRAINING\n","MODEL ARCHITECTURE:\n","LSTM(\n","  (embedding): Embedding(20000, 50, padding_idx=0)\n","  (lstm): LSTM(50, 20, num_layers=2, batch_first=True)\n","  (linear): Linear(in_features=20, out_features=100, bias=True)\n","  (relu): ReLU()\n","  (fc): Linear(in_features=100, out_features=1, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n"," \n","----------------------------------------------------------------------------------------------------\n","EPOCH 1\n","----------------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|\u001b[34m██████████\u001b[0m| 98/98 [00:29<00:00,  3.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss : 0.694\n","Validation Loss : 0.692\n","Validation Accuracy  : 0.502\n","----------------------------------------------------------------------------------------------------\n","EPOCH 2\n","----------------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|\u001b[34m██████████\u001b[0m| 98/98 [00:29<00:00,  3.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss : 0.638\n","Validation Loss : 0.559\n","Validation Accuracy  : 0.748\n","----------------------------------------------------------------------------------------------------\n","EPOCH 3\n","----------------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|\u001b[34m██████████\u001b[0m| 98/98 [00:28<00:00,  3.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss : 0.512\n","Validation Loss : 0.499\n","Validation Accuracy  : 0.787\n","----------------------------------------------------------------------------------------------------\n","EPOCH 4\n","----------------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|\u001b[34m██████████\u001b[0m| 98/98 [00:28<00:00,  3.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss : 0.403\n","Validation Loss : 0.481\n","Validation Accuracy  : 0.788\n","----------------------------------------------------------------------------------------------------\n","EPOCH 5\n","----------------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|\u001b[34m██████████\u001b[0m| 98/98 [00:28<00:00,  3.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss : 0.331\n","Validation Loss : 0.439\n","Validation Accuracy  : 0.817\n","----------------------------------------------------------------------------------------------------\n","EPOCH 6\n","----------------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|\u001b[34m██████████\u001b[0m| 98/98 [00:28<00:00,  3.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss : 0.289\n","Validation Loss : 0.421\n","Validation Accuracy  : 0.828\n","----------------------------------------------------------------------------------------------------\n","EPOCH 7\n","----------------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|\u001b[34m██████████\u001b[0m| 98/98 [00:28<00:00,  3.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss : 0.251\n","Validation Loss : 0.416\n","Validation Accuracy  : 0.827\n","----------------------------------------------------------------------------------------------------\n","EPOCH 8\n","----------------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["100%|\u001b[34m██████████\u001b[0m| 98/98 [00:28<00:00,  3.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss : 0.227\n","Validation Loss : 0.470\n","Validation Accuracy  : 0.820\n"]},{"output_type":"execute_result","data":{"text/plain":["([tensor(0.6922, device='cuda:0'),\n","  tensor(0.5587, device='cuda:0'),\n","  tensor(0.4986, device='cuda:0'),\n","  tensor(0.4810, device='cuda:0'),\n","  tensor(0.4394, device='cuda:0'),\n","  tensor(0.4208, device='cuda:0'),\n","  tensor(0.4159, device='cuda:0'),\n","  tensor(0.4702, device='cuda:0')],\n"," [0.50205, 0.7477, 0.78665, 0.78765, 0.8174, 0.82765, 0.82715, 0.81965],\n"," [tensor(0.6922, device='cuda:0'),\n","  tensor(0.5587, device='cuda:0'),\n","  tensor(0.4986, device='cuda:0'),\n","  tensor(0.4810, device='cuda:0'),\n","  tensor(0.4394, device='cuda:0'),\n","  tensor(0.4208, device='cuda:0'),\n","  tensor(0.4159, device='cuda:0'),\n","  tensor(0.4702, device='cuda:0')],\n"," [0.50205, 0.7477, 0.78665, 0.78765, 0.8174, 0.82765, 0.82715, 0.81965])"]},"metadata":{},"execution_count":23}],"source":["from torch.optim import RMSprop\n","\n","epochs = 8\n","learning_rate = 1e-3\n","\n","\n","loss_fn = nn.BCEWithLogitsLoss()\n","text_classifier = LSTM(20,1).to(device)\n","\n","optimizer = RMSprop(text_classifier.parameters(), lr=learning_rate)\n","\n","print(\"STARTING TRAINING\")\n","print(\"MODEL ARCHITECTURE:\")\n","print(text_classifier)\n","print(\" \")\n","\n","\n","TrainModel(text_classifier, loss_fn, optimizer, train_loader, epochs)"]},{"cell_type":"markdown","metadata":{"id":"8eMN9htT2BQa"},"source":["# Evaluating the model\n","\n","Let's look at the 3 first reviews in our test set, and the predictions from our model! Do these make sense?"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1670759729757,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"puFwhCMK2jrn","outputId":"4f4b12ac-e8c2-486b-f1a8-a44d69ca43d0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(2,\n","  'Focus is an engaging story told in urban, WWII-era setting. William Macy portrays everyman who is taken out of his personal circumstances and challenged with decisions testing his values affecting the community. Laura Dern, Macy and David Paymer give good performances, so also the good supporting ensemble.'),\n"," (2,\n","  'Director John Schlesinger\\'s tense and frantic film tells the true story of Christopher Boyce and Andrew Daulton Lee, two young men who sold United States government secrets to the Soviet Union in the early 1970\\'s.<br /><br />Timothy Hutton plays Christopher Boyce very competently. He is a young man very disillusioned by the CIA\\'s underhanded activities in allied Australia. Sean Penn, as the doped-up, drug running Andrew Daulton Lee, is outstanding.<br /><br />The competent and professional direction of Schlesinger, along with some very good acting, make \"The Falcon and the Snowman\" an espionage thriller not to be missed.<br /><br />Tuesday, February 4, 1992 - Video'),\n"," (2,\n","  \"I hope Robert Redford continues to make more films like this. Hillerman's books are wonderful, and as a young child raised in the Southwest his stories hit home! Adam Beach is a highly under rated and under used actor. Wake up Hollywood, not everyone thinks that your Mel Gibson's are cool! Many movie goer's today want to see films that make you think. I have seen all of the Redford/Hillerman series. They are thoughtful, scenic and have great plots. I'm hoping that if enough people write to Robert Redford he may decide to make a few more! Thank you Adam Beach and Tony Hillerman for great entertainment! If anyone get's a chance to read Tony Hillerman's latest book do so! It's great. I also recommend traveling through Arizona, New Mexico, Utah and Colorado. Stop at every view site and feel the setting of Hillerman's books. Amazing experience.\")]"]},"metadata":{},"execution_count":24}],"source":["test_dataset[:3]"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1670759729758,"user":{"displayName":"Lluis Guasch","userId":"08850588352998994016"},"user_tz":0},"id":"ae_XuEDFYS8N","outputId":"f89ba7fc-f1af-4f7e-98ff-0597ec1fb511","scrolled":false},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.9450],\n","        [0.9456],\n","        [0.9506]], device='cuda:0')\n"]}],"source":["text_classifier.eval()\n","with torch.no_grad():\n","    print(torch.sigmoid(text_classifier(test_x[:3].to(device))))"]},{"cell_type":"markdown","metadata":{"id":"zYeaNUKL24Om"},"source":["# Optional Exercise 2\n","\n","Here are a few things you can do if you want:\n","\n","1. Try to calculate the accuracy for the test set\n","2. The model is decent (about 80% accuracy). Can you improve on it?\n","\n","There are no given solutions for this exercise: it is up to you to play with the model if you want to.\n","\n","Hope you enjoyed your first try at NLP!!!!"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":0}