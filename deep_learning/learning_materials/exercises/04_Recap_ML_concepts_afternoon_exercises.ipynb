{"cells":[{"cell_type":"markdown","metadata":{"id":"9YehS8enAmDn"},"source":["<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n","\n","\n","---\n"]},{"cell_type":"markdown","source":["# **Recap of ML/DL basic concepts**\n","\n","\n","#### **Morning contents/agenda**\n","\n","1. Supervised VS unsupervised learning\n","\n","2. Parameters and hyperparameters of a network\n","\n","  2.1 Activation functions \n","\n","  2.2 Losses\n","\n","3. **Training is an optimisation problem**: gradient descent and backpropagation\n","\n","4. Batch, mini-batch, and stochastic gradient descent\n","\n","5. Bias and variance, and regularisers\n","\n","#### **Learning outcomes**\n","\n","1. Understand the difference between parameters of hyperparameters of a network\n","\n","2. Understand how networks are trained using gradient descent and backpropagation\n","\n","3. Understand how batch size works and the effect of regularisers in the training process\n","\n","\n","<br>\n","\n","#### **Afternoon contents/agenda**\n","\n","1. Half-moon classifier \n","\n","2. L2 regularisation on MNIST\n","\n","#### **Learning outcomes**\n","\n","1. Be able to build a simple classifier for simple datasets\n","\n","2. Understand the effect of explicit regularisation during training\n","\n","<br/>\n","\n","---\n","\n","<br/>"],"metadata":{"id":"koBaqXTQrby5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kYki018eTFJ"},"outputs":[],"source":["!pip install pycm livelossplot\n","%pylab inline\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from livelossplot import PlotLosses\n","from pycm import *\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","from torchsummary import summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LI8sNA9feT3H"},"outputs":[],"source":["def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"]},{"cell_type":"markdown","metadata":{"id":"EirUJEpBVvU3"},"source":["## 1. Half-moon classifier\n","\n","#### Write a network with 2 hidden layers to perform a binary classification task on the provided dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urlFMaVtY-Rg"},"outputs":[],"source":["from sklearn.datasets import make_moons\n","from sklearn.metrics import accuracy_score\n","\n","\n","def make_train_test(batch_size, batch_num, test_size, noise=0.01):\n","    \"\"\"\n","    Makes a two-moon train-test dataset with fixed batch size, number and noise level\n","    \"\"\"\n","    X_train, y_train = make_moons(n_samples=batch_size*batch_num, noise=noise)\n","    y_train = y_train.reshape(batch_num, batch_size, 1)\n","    X_train = X_train.reshape(batch_num, batch_size, 2)\n","\n","\n","    X_test, y_test = make_moons(noise=noise)\n","    y_test = y_test.reshape(test_size, 1)\n","    return X_train, y_train, X_test, y_test"]},{"cell_type":"markdown","source":["#### Define a simple FFN with two hidden layers and sigmoid activations:"],"metadata":{"id":"6iCp7Wb95fL8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3X5OAGRZT-4"},"outputs":[],"source":["class SingleHiddenLayerNetwork(nn.Module):\n","    def __init__(self, I, H1, H2, O):\n","        super(SingleHiddenLayerNetwork, self).__init__()\n","        ### your code here\n","        \n","    def forward(self, X):\n","        ### your code here\n","        return ###"]},{"cell_type":"markdown","source":["#### Define your own loss, as well as train and evaluate functions:"],"metadata":{"id":"ioSh8G3U5vte"}},{"cell_type":"code","source":["# define the loss (criterion)\n","def bce_loss(y, a3):\n","    return ### your code here\n","\n","# train function\n","def train(model, optimizer, data_loader):\n","    ### set your model to train\n","    for X, y in data_loader:\n","        # zero the gradients\n","        # forward pass\n","        # loss calculation\n","        # backprop\n","        # update the model\n","    \n","    y_pred = np.where(a3[:, 0].detach().numpy()>0.5, 1, 0) ### you can use this code\n","    accuracy = accuracy_score(y, y_pred)                   ### but try to understand what it does\n","    return loss, accuracy                                  ### and return the loss and the accuracy\n","\n","# evaluate function\n","def evaluate(model, data_loader):\n","    ### set your model to evaluate\n","    for X, y in data_loader:\n","        with torch.no_grad(): ### we don't need gradients now\n","            # forward pass\n","            # loss calculation\n","    y_pred = np.where(a3[:, 0].numpy()>0.5, 1, 0)  ### you can use this code\n","    accuracy = accuracy_score(y, y_pred)           ### but try to understand what it does\n","    return loss, accuracy                          ### and return the loss and the accuracy\n","    \n"],"metadata":{"id":"GArLqJ205uDH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Follow these steps:\n","\n","- define hyperparameters (use full-batch training)\n","- create train and test TensorDatasets\n","- create train and test DataLoaders"],"metadata":{"id":"8tLWPP8p54B6"}},{"cell_type":"code","source":["# Define hyperparameters\n","set_seed(42)             # random seed (keep it at 42 so that we can compare results)\n","epochs = 1000            # number of loops through whole dataset\n","batch_size = 1000        # size of a single batch\n","batch_num = 1            # use full batch training\n","test_size = 100          # examples in test set\n","lr = ### find a good one # learning rate\n","momentum = ### play with this value      # set momentum\n","\n","#Define the size of the input, hidden, and output layers\n","I, H1, H2, O = 2, 3, 5, 1  # you can use these values, but you can play around and change them too.\n","\n","#Use the function 'make_train_test' defined above to create two-moons + noise\n","X_train, y_train, X_test, y_test = ### call the function defined above\n","\n","\n","#### use the following lines of code, but try to understand what is X_train and y_train.\n","\n","#Define Train Set in Pytorch\n","X_train = torch.from_numpy(X_train).float()[0] #Convert to torch tensor, single batch\n","y_train = torch.from_numpy(y_train).float()[0] #Convert to torch tensor, single batch\n","\n","train_dataset = ### create a train TensorDataset\n","\n","#Define Test Set in Pytorch\n","X_test = ### repeat the steps above for the test set now to convert to torch tensor, already single batch\n","y_test = ### repeat the steps above for the test set now to convert to torch tensor, already single batch\n","\n","test_dataset = ### create a test TensorDataset\n","\n","#Use Pytorch's functionality to load data in batches. Here we use full-batch training again.\n","train_loader = ### create a train DataLoader\n","test_loader = ### create a test DataLoader\n","\n","\n","        "],"metadata":{"id":"5Dd6jnkT53dt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Instantiate your network and train it for 1000 epochs"],"metadata":{"id":"yEucAuH46LKm"}},{"cell_type":"code","source":["### use the code provided below to train the network\n","\n","network = SingleHiddenLayerNetwork(I, H1, H2, O)\n","optim = torch.optim.SGD(network.parameters(), lr=lr, momentum=momentum)\n","for i in range(1000):\n","    train_loss, train_accuracy = train(network, optim, train_loader)\n","    test_loss, test_accuracy = evaluate(network, test_loader)\n","    \n","    if i % 100 == 0:\n","        print(\"Training Loss in epoch \"+str(i)+\": %1.2f\" % train_loss.item())\n","        print(\"Training accuracy in epoch \"+str(i)+\": %1.2f\" % train_accuracy)\n","        print(\"Test Loss in epoch \"+str(i)+\": %1.2f\" % test_loss.item())\n","        print(\"Test accuracy in epoch \"+str(i)+\": %1.2f\" % test_accuracy, \"\\n\")\n","                "],"metadata":{"id":"iBkACnTB6KjO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use the code snippets provided to plot the training and set classified points of the half-moon."],"metadata":{"id":"pIFdPO9pA5CA"}},{"cell_type":"code","source":["network.eval()\n","with torch.no_grad():\n","    a_train = network(X_train)\n","    a_test = network(X_test)\n","print(\"Test set accuracy: \", accuracy_score(y_test, np.where(a_test[:, 0].numpy()>0.5, 1, 0)))\n","fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n","ax[0].scatter(X_train[:, 0], X_train[:, 1], c=np.where(a_train[:, 0].numpy()>0.5, 1, 0))\n","ax[1].scatter(X_test[:, 0], X_test[:, 1], c=np.where(a_test[:, 0].numpy()>0.5, 1, 0))\n","\n"],"metadata":{"id":"x_rpHldG7ZLy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cI6I6bPCZxdV"},"source":["<br>\n","\n","---\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"DWzJXSJSZroz"},"source":["## 2. L2 regularisation on MNIST\n","\n","Add L2 regularisation to this morning training with MNIST.\n","\n","The code for this morning implementation is below, use it and modify it as you see fit.\n","\n","\\[**HINT**: there is a way to do it that only involves changing one line of code, check the arguments of SGD [here](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\\]\n","\n","You can copy paste the code blocks you need from this morning session."]},{"cell_type":"code","source":["# create a simple network\n","class simpleFFN(nn.Module):\n","  def __init__(self):\n","    super(simpleFFN, self).__init__()\n","    self.hidden_1 = nn.Linear(784, 200, bias=True)\n","    self.hidden_2 = nn.Linear(200, 50, bias=True)\n","    self.hidden_3 = nn.Linear(50,200, bias=True)\n","    self.output = nn.Linear(200, 10, bias=False)\n","    self.activation = nn.Mish()  ## Debbie loves this one ;-)\n","    \n","  def forward(self, X):\n","    z1 = self.hidden_1(X)\n","    a1 = self.activation(z1)\n","    z2 = self.hidden_2(a1)\n","    a2 = self.activation(z2)\n","    z3 = self.hidden_3(a2)\n","    a3 = self.activation(z3)\n","    z4 = self.output(a3)\n","    a4 = self.activation(z4)\n","    return a4\n","\n","# test that it runs  \n","x = torch.randn((1, 1, 784))\n","model = simpleFFN()\n","y = model(x)\n","print(y)\n","print(model)"],"metadata":{"id":"WNlT1-zQ05ox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# download MNIST\n","mnist_train = MNIST(\"./\", download=True, train=True)\n","mnist_test = MNIST(\"./\", download=True, train=False)"],"metadata":{"id":"u9m0Kg3QEzxI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split the data\n","shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split(mnist_train.train_data, mnist_train.train_labels) \n","indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]\n","\n","# define an standardisation function\n","def apply_standardization(X): \n","  X /= 255.\n","  X -= 0.1307\n","  X /= 0.3081\n","  return X\n","\n","# standardise the data\n","X_train, y_train = apply_standardization(mnist_train.train_data[indices[0]].float()), mnist_train.train_labels[indices[0]]\n","X_val, y_val = apply_standardization(mnist_train.train_data[indices[1]].float()), mnist_train.train_labels[indices[1]]\n","X_test, y_test =  apply_standardization(mnist_test.test_data.float()), mnist_test.test_labels"],"metadata":{"id":"HYqjkRTPE4tO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the TensorDatasets containing mnist_train, mnist_validate, and mnist_test\n","mnist_train = TensorDataset(X_train, y_train.long())\n","mnist_validate = TensorDataset(X_val, y_val.long())\n","mnist_test = TensorDataset(X_test, y_test.long())"],"metadata":{"id":"l39A5Ml1FCft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# implement training and validation functions\n","\n","def train(model, optimizer, criterion, data_loader):\n","    model.train()                         # the model is in the training mode so the parameters(weights)to be optimised will be updated\n","    train_loss, train_accuracy = 0, 0     # initialise loss and accuracy to 0 for training\n","    for X, y in data_loader:              # iterate over the mini-batches defined in the data loader\n","        X, y = X.to(device), y.to(device) # send data to the device (GPU in our case)\n","        optimizer.zero_grad()             # resetting optimiser info\n","        a2 = model(X.view(-1, 28*28))     # forward pass\n","        loss = criterion(a2, y)           # compute loss\n","        loss.backward()                   # backpropagation to calculate the gradients\n","        train_loss += loss*X.size(0)      # # add it up for different mini-batches and undo loss normalisation\n","        y_pred = F.log_softmax(a2, dim=1).max(1)[1]  # get predictions\n","        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0) # compute accuracy\n","        optimizer.step()                  # perform a step of gradient descent\n","        \n","    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)  # here we can average over the whole dataset\n","\n","\n","def validate(model, criterion, data_loader):      # does not need optimiser\n","    model.eval()                                  # model is set to evaluation mode so no dropout or any other funny stuff here\n","    validation_loss, validation_accuracy = 0., 0. # initialise loss and accuracy to 0 for training\n","    for X, y in data_loader:                      # iterate over the mini-batches defined in the data loader\n","        with torch.no_grad():                     # deactivates autograd engine\n","            X, y = X.to(device), y.to(device)     # send data to the device (GPU in our case)\n","            a2 = model(X.view(-1, 28*28))         # forward pass\n","            loss = criterion(a2, y)               # evaluate loss\n","            validation_loss += loss*X.size(0)     # add it up for different mini-batches and undo loss normalisation\n","            y_pred = F.log_softmax(a2, dim=1).max(1)[1]  # get predictions\n","            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0) # compute accuracy\n","            \n","    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)  # here we can average over the whole dataset"],"metadata":{"id":"KbWyTwASFDPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set hyperparameters\n","\n","seed = 42\n","lr = 1e-2\n","momentum = 0.9\n","batch_size = 64  # here batch_size really refers to what we have described as mini-batch\n","test_batch_size = 1000\n","n_epochs = 30\n","\n","set_seed(seed)\n","model = simpleFFN().to(device)                                              # instantiate model and send it to the GPU\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)   # instantiate the optimizer\n","criterion = nn.CrossEntropyLoss() "],"metadata":{"id":"ovYwQ3ZBFOsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create DataLoaders\n","\n","train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0) ## num_workers=0 means that the main process will retrieve the data.\n","validation_loader = DataLoader(mnist_validate, batch_size=test_batch_size, shuffle=False, num_workers=0)\n","test_loader = DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False, num_workers=0)"],"metadata":{"id":"Db_8ocXSFaQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train for the number of epochs specified\n","\n","set_seed(seed)\n","liveloss = PlotLosses()    # plots evolution of loss and accuracy\n","for epoch in range(n_epochs):\n","    logs = {}\n","    train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","    logs['' + 'log loss'] = train_loss.item()\n","    logs['' + 'accuracy'] = train_accuracy.item()\n","    \n","    validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n","    logs['val_' + 'log loss'] = validation_loss.item()\n","    logs['val_' + 'accuracy'] = validation_accuracy.item()\n","    \n","    liveloss.update(logs)\n","    liveloss.draw()\n","    print(validation_loss.item())"],"metadata":{"id":"NPO7na2IFkNp"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjPUMZ1US0PF5z0nOmvSl5"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}