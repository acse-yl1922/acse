{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQORldhuYS7_"
   },
   "source": [
    "# NLP: Sentiment Analysis with GloVe\n",
    "\n",
    "### Exercise objectives:\n",
    "- Learn how to embed data with GloVe (and similar embedding like Word2Vec)\n",
    "- Build a data pipeline to prepare text data\n",
    "- Train a simple LSTM model for sentiment analysis\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "# The data\n",
    "\n",
    "Today we will use the IMDB movie review dataset. It is a classic data set that can be downloaded staight from Pytorch. It contains reviews of different movies, as well as a target: 1 for bad review, 2 for a good review. The goal is to train an LSTM model to predict whether a review is good, or bad.\n",
    "\n",
    "# Installing Dependencies\n",
    "\n",
    "But first, let's install a few libraries that you are unlikely to have if you are running this on your colab instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T10:56:06.628896Z",
     "start_time": "2022-12-09T10:48:31.760441Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6662,
     "status": "ok",
     "timestamp": 1670682724682,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "JET7pqXzYS8C",
    "outputId": "cd996b49-c07e-4de3-c683-607d0a6b5113"
   },
   "outputs": [],
   "source": [
    "!pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T11:05:42.134671Z",
     "start_time": "2022-12-09T11:05:39.702494Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4212,
     "status": "ok",
     "timestamp": 1670682728891,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "rzuGT6bFYS8D",
    "outputId": "4e6994d5-4641-41ae-eafb-534ce9492fd5"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmrW5-EblSWM"
   },
   "source": [
    "## Download NLTK files\n",
    "\n",
    "We also need to download a few key files for NLTK (this could take a minute or two o run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T12:33:40.374621Z",
     "start_time": "2022-12-09T12:33:40.366004Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2153,
     "status": "ok",
     "timestamp": 1670682731029,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "2tZGs_brYS8E",
    "outputId": "482dc890-6f8b-4b19-baff-11f62f383cf9"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdAbnwemljk3"
   },
   "source": [
    "# Importing torch \n",
    "\n",
    "Let's import torch and a few common dependencies, and make sure we set our device to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T10:48:09.786005Z",
     "start_time": "2022-12-09T10:48:09.444438Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670682731029,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "v1rF6Q_4YS8F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T11:12:27.293028Z",
     "start_time": "2022-12-09T11:12:27.285440Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3307,
     "status": "ok",
     "timestamp": 1670682734807,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "Yr4ItymxYS8F",
    "outputId": "c52ee5e5-cefb-44b4-be5b-805ab634f41f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYBrwQ1rlvI4"
   },
   "source": [
    "# Importing and visualizing the data\n",
    "\n",
    "Running the cell below will import that IMDB movie reviews in your notebook, and we can then visualize some results. We can import the reviews straight from datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T11:01:45.019715Z",
     "start_time": "2022-12-09T11:01:44.140864Z"
    },
    "executionInfo": {
     "elapsed": 1541,
     "status": "ok",
     "timestamp": 1670685919992,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "vLJ58FP0YS8G"
   },
   "outputs": [],
   "source": [
    "from torchtext import data,datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchdata\n",
    "\n",
    "train_dataset, test_dataset  = datasets.IMDB(root = '.data', split = ('train', 'test'))\n",
    "\n",
    "# Let's create a true test data and a true validation data as lists:\n",
    "test_dataset, val_dataset = train_test_split(list(test_dataset), train_size=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU1u79i0nYx2"
   },
   "source": [
    "## Let's see the 5 first reviews\n",
    "\n",
    "Note that they are tuples: 1 means a bad review, 2 means a good review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T11:02:05.364343Z",
     "start_time": "2022-12-09T11:02:05.299756Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670682757815,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "Axkzp6H7YS8H",
    "outputId": "977db2da-ad9c-4004-ea15-b3eb3ba5ca76"
   },
   "outputs": [],
   "source": [
    "val_dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcsmirLuoIa1"
   },
   "source": [
    "## Choosing one example\n",
    "\n",
    "We will need one example of text to prepare it. So, let's take the first review of your validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T12:28:07.180134Z",
     "start_time": "2022-12-09T12:28:07.171558Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670685996389,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "M8FDmQt2YS8I",
    "outputId": "bdc210e0-75ad-4e6f-97c1-0eadf99741a6"
   },
   "outputs": [],
   "source": [
    "example = val_dataset[0][1]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_viiDpMDoaSC"
   },
   "source": [
    "# Time to play with GloVe\n",
    "\n",
    "We want to embed our text with a pre-trained model. In the lecture, we talked about Word2Vec, which is one of the best embeddings. But Word2Vec is harder to implement in PyTorch because it is not offered as a pre-trained layer. Instead, we will play with GloVe, which is very similar to Word2Vec in concept and is readily available in PyTorch.\n",
    "\n",
    "## Download GloVe\n",
    "\n",
    "There are different versions of GloVe, depending on the size of the vector you want to use for your embedding, and the size of your vocabulary. Here, to make things as fast as possible, let's download the version with vectors embedded in 50 dimensions, and the smallest vocabulary possible. This is still a large file (862 Mb) so on my system this took almost 3 minutes to run: be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T14:59:52.269152Z",
     "start_time": "2022-12-09T14:59:51.781462Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175244,
     "status": "ok",
     "timestamp": 1670682944650,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "NNbz_Ki0YS8I",
    "outputId": "f630895f-ca4c-4cae-f911-c638b7c801d4"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "glove = GloVe(dim='50', name='6B', max_vectors=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGRUNm38qFCl"
   },
   "source": [
    "## Words to vectors\n",
    "\n",
    "Now that we have GloVe downloaded, we can see how it works. We can, for instance, very easily find the vector for any word. Here is how we can find the representation of the word 'king':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670686284458,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "vQvhqzFRqXsC",
    "outputId": "263c6925-6f59-4e4c-ed38-be85003218ab"
   },
   "outputs": [],
   "source": [
    "glove[\"king\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvwDUhbrsD_d"
   },
   "source": [
    "## GloVe tokens (index)\n",
    "\n",
    "Importantly, we will need to obtain the index (or token) of a word in GloVe. This is because we will transform our sentence into integer token before passing it to the network, and these integer tokens will need to correspond to the index of the vectors for our embedding layer.\n",
    "\n",
    "There are two useful functions for us:\n",
    "- string to integer (stoi)\n",
    "- integer to string (itos)\n",
    "\n",
    "They do pretty much what is written on the tin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 898,
     "status": "ok",
     "timestamp": 1670686878174,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "nxaePJLVqltU",
    "outputId": "7cb8df37-aba8-42e0-dfcb-0b7f33ce4c81"
   },
   "outputs": [],
   "source": [
    "glove.stoi['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1670686899331,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "Y22r86s8ssZ0",
    "outputId": "0acf2bdd-a958-4a6d-a590-6f0f8cdb93dc"
   },
   "outputs": [],
   "source": [
    "glove.itos[691]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV9W4dOcr9pE"
   },
   "source": [
    "## Feel free to play with vectors!\n",
    "\n",
    "Explore the embedding, and how each words are represented..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1670686617122,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "3z2vvcmSq0FR",
    "outputId": "c53eb21c-240a-4dc7-900f-e53ee15cc4df"
   },
   "outputs": [],
   "source": [
    "glove['queen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqIYe5PhtALK"
   },
   "source": [
    "# Exercise 1: Text preprocessing\n",
    "\n",
    "Your first exercise will be to build a pre-processing pipeline. You can use the code in the lecture to help you do that. Also, I already created the functions signature below to help you understand what is needed. Each function is applied to a single review, not a batch of reviews (you will see this below).\n",
    "\n",
    "The transformations you need to do are the following:\n",
    "1. Text needs to be all lower case (all words in GloVe are lower case only)\n",
    "2. You need to remove numbers - they are not helpful for sentiment analysis\n",
    "3. Remove punctuation (also not needed)\n",
    "4. Transform the sentence into word tokens using NLTK. Now your sentence becomes a list of words.\n",
    "5. Remove stopwords using NLTK\n",
    "6. Get the index of the word in GloVe. If the word exists in the dictionary, keep it in the list. If not, just don't add it. This is a little bit more challenging, so consult the solution if you are unsure.\n",
    "7. We also need to pad our sentence to MAX_LEN (the maximum length of the sentences). Notice below that I set this to be 100 words: we don't really need more than that, and we are getting good results (well, decent results) with 100 words. But all tensors need to be the same length, so pad_sentence is here to add zeros to those sentences that are shorted than 100 words. Again, consult the solution if you struggle with this one.\n",
    "\n",
    "I have left the last function in for you: transform_text calls each of the other functions one after another, and transforms the text.\n",
    "\n",
    "Make sure that you obtain a tensor of dimension 100, with all the right token, when you call transform_text on your example sentence from above. Then continue with the exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1670684767211,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "5S3Ro7M3fC2C"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T14:59:53.673619Z",
     "start_time": "2022-12-09T14:59:53.654730Z"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1670684769111,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "hArHwgpgYS8J"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string \n",
    "from torch.nn.functional import pad \n",
    "\n",
    "def remove_numbers(txt):\n",
    "    pass\n",
    "\n",
    "def remove_punctuation(txt):\n",
    "    pass\n",
    "\n",
    "def tokenize(txt):\n",
    "    pass\n",
    "\n",
    "def remove_stopwords(word_tokens):\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_index(txt, vocab=glove):\n",
    "    pass\n",
    "\n",
    "def pad_sentence(txt):\n",
    "    pass\n",
    "        \n",
    "\n",
    "def transform_text(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = remove_numbers(txt)\n",
    "    txt = remove_punctuation(txt)\n",
    "    txt = tokenize(txt)\n",
    "    txt = remove_stopwords(txt)\n",
    "    txt = torch.tensor(get_index(txt)).long()\n",
    "    return pad_sentence(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T15:37:09.071016Z",
     "start_time": "2022-12-09T15:37:09.059162Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1670684769112,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "VdKb3KouYS8J",
    "outputId": "2ff28f5d-680c-4c7e-8131-9861b8163c9c"
   },
   "outputs": [],
   "source": [
    "transform_text(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPlOVZYJvWG7"
   },
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "Now that you have written the helper functions, we can focus on preparing the dataset. First, I will create the target and the label for the train, test, and validation splits using a list comprehension. This will be a dataset not saved as a dataloader, and it will be useful to assess the loss and the accuracy of each one of our splits. But it won't be used as a batch.\n",
    "\n",
    "Run the code below. Beware, due to the size of the dataset this can be a bit of a lengthy cell to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T16:25:24.947957Z",
     "start_time": "2022-12-09T16:25:23.971950Z"
    },
    "executionInfo": {
     "elapsed": 67314,
     "status": "ok",
     "timestamp": 1670684836808,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "ugdic1aQYS8K"
   },
   "outputs": [],
   "source": [
    "train_y = torch.tensor([item[0] for item in list(train_dataset)])-1\n",
    "train_x = torch.stack([transform_text(item[1]) for item in list(train_dataset)])\n",
    "\n",
    "val_y = torch.tensor([item[0] for item in list(test_dataset)])-1\n",
    "val_x = torch.stack([transform_text(item[1]) for item in list(test_dataset)])\n",
    "\n",
    "test_y = torch.tensor([item[0] for item in list(test_dataset)])-1\n",
    "test_x = torch.stack([transform_text(item[1]) for item in list(test_dataset)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl9SIo5iv8f1"
   },
   "source": [
    "\n",
    "## Dataloader\n",
    "\n",
    "To be able to run batches on the GPU, we will need to save the dataset in a DataLoader class. We will also create a function called vectorize_batch, that will allow us to vectorise our batches on the go (i.e. batch by batch). Take note of how I wrote the train_loader: I use the dataset, define the batch size (256), and collate function (my vectorizer), and (very important!) I set shuffle to 'True'.\n",
    "\n",
    "Not setting shuffle to True results in the batch not being randomly shuffled, and a poor performance during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1064,
     "status": "ok",
     "timestamp": 1670684837869,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "4Zm9TKDFf3ox"
   },
   "outputs": [],
   "source": [
    "from torchtext.data import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def vectorize_batch(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    \n",
    "    X_embedded = torch.stack([transform_text(txt) for txt in X])\n",
    "    \n",
    "    return X_embedded, torch.tensor(Y).long()-1 \n",
    "\n",
    "train_dataset=  to_map_style_dataset(train_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=vectorize_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JR9Y3CXtwm8G"
   },
   "source": [
    "Let's quickly check that the dataloader works. You should obtain a size of [256, 100] for X, and [256] for Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T16:25:32.264070Z",
     "start_time": "2022-12-09T16:25:32.048861Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1670684837869,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "eKSlEG9iYS8K",
    "outputId": "9efe0607-9028-4464-8018-c803e75b5ce1"
   },
   "outputs": [],
   "source": [
    "for X, Y in train_loader:\n",
    "    print(X.shape, Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EInk3QjSwxmT"
   },
   "source": [
    "# The embedding layer\n",
    "\n",
    "I have written a function for you that creates an embedding layer. We will pass to it the vectors of our GloVe object, and from this, we will now the number of embeddings (20000) and the embedding dimensions (50). \n",
    "\n",
    "We then pass the weights of the GloVe vocabulary to our embedding layer, and we choose to set the weight to 'not trainable'. This way, we won't try to relearn the correct weights for this task. But we can also choose to start with the GloVe weights, and then update them to suite our vocabulary.\n",
    "\n",
    "This is a classic example of transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T16:25:33.144702Z",
     "start_time": "2022-12-09T16:25:33.137652Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1670684837869,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "pzHlnkI-YS8K"
   },
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=True):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim,padding_idx=0)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtJudwsSxSZf"
   },
   "source": [
    "# Network architecture\n",
    "\n",
    "We will use an LSTM model, as you did on Friday. But this time, we will use the LSTM layer from nn.LSTM: yes, you do not need to write it from scratch!\n",
    "\n",
    "In fact, you don't need to write anything. I wrote the network for you, as this is very time consuming and we only have half a day for theory and practice today.\n",
    "\n",
    "Note the use of the embedding layer, as well as the fact that I use 2 LSTM layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T17:00:30.539306Z",
     "start_time": "2022-12-09T17:00:30.524277Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1670688564448,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "vNdAV-m6YS8L"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hid_dim, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(glove.vectors, False)\n",
    "        \n",
    "        n_layers = 2\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hid_dim, n_layers,dropout=0, batch_first=True)\n",
    "        self.linear = nn.Linear(hid_dim,100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(100, output_dim)\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        std= 1.0 / np.sqrt(self.hid_dim)\n",
    "        \n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "        \n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "\n",
    "        batch_size, seq_len,  _ = embedded.shape\n",
    "        hid_dim = self.lstm.hidden_size\n",
    "            \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        outputs = outputs[:, -1]\n",
    "        \n",
    "        prediction = self.fc(self.dropout(self.relu(self.linear(outputs))))\n",
    "\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNW_tIBZx0aD"
   },
   "source": [
    "# Training and validation functions\n",
    "\n",
    "Below are my training and validation functions. Take a moment to study them, and then run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T17:00:34.774166Z",
     "start_time": "2022-12-09T17:00:34.753945Z"
    },
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1670688567056,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "RmbC3MJcYS8M"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "\n",
    "def CalcValLossAndAccuracy(model, loss_fn, val_X, val_Y):\n",
    "    \n",
    "    #print(f'Calculating Epoch Loss and Accuracy:')\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X, Y, title = (val_x, val_y,'Validation')\n",
    "        X = val_X.to(device)\n",
    "        Y = val_Y.to(device)\n",
    "            \n",
    "        outputs = model(X).squeeze()\n",
    "        loss = loss_fn(outputs, Y.float())\n",
    "            \n",
    "        preds = [1 if p>=.5 else 0 for p in torch.sigmoid(outputs)]\n",
    "        accuracy = accuracy_score(Y.detach().cpu().numpy().tolist(),preds)\n",
    "            \n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss)\n",
    "\n",
    "        \n",
    "        print(f'{title} Loss : {loss:.3f}')\n",
    "        print(f\"{title} Accuracy  : {accuracy:.3f}\")\n",
    "    \n",
    "    return losses, accuracies\n",
    "\n",
    "\n",
    "def TrainModel(model, loss_fn, optimizer, train_loader, epochs=10):\n",
    "    train_losses = []\n",
    "    train_accuracy = []\n",
    "    val_losses = []\n",
    "    val_accuracy = []\n",
    "    \n",
    "    for i in range(1, epochs+1):\n",
    "        \n",
    "        print('-'*100)\n",
    "        print(f'EPOCH {i}')\n",
    "        print('-'*100)\n",
    "        \n",
    "        epoch_losses = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        \n",
    "        for X, Y in tqdm(train_loader, colour='BLUE'):\n",
    "\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            Y_preds = model(X).squeeze()\n",
    "            loss = loss_fn(Y_preds, Y.float())\n",
    "            \n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Train Loss : {:.3f}\".format(torch.tensor(epoch_losses).mean()))\n",
    "        \n",
    "        losses, acc = CalcValLossAndAccuracy(model, loss_fn, val_x, val_y)\n",
    "        train_losses.append(losses[0])\n",
    "        train_accuracy.append(losses[0])\n",
    "        val_losses.append(acc[0])\n",
    "        val_accuracy.append(acc[0])\n",
    "        \n",
    "    return train_losses, val_losses, train_accuracy, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prvDZBNox-6V"
   },
   "source": [
    "# Training the model\n",
    "\n",
    "We will now train the model using the RMSprop optimizer, and 20 hidden units in our model. We will run for only 8 epochs for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 212541,
     "status": "ok",
     "timestamp": 1670689310397,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "RaQv-ot5zWKj",
    "outputId": "9c94772c-ce10-4c43-aa3d-0fe68a6c3e12"
   },
   "outputs": [],
   "source": [
    "from torch.optim import RMSprop\n",
    "\n",
    "epochs = 8\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "text_classifier = LSTM(20,1).to(device)\n",
    "\n",
    "optimizer = RMSprop(text_classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"MODEL ARCHITECTURE:\")\n",
    "print(text_classifier)\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "TrainModel(text_classifier, loss_fn, optimizer, train_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eMN9htT2BQa"
   },
   "source": [
    "# Evaluating the model\n",
    "\n",
    "Let's look at the 3 first reviews in our test set, and the predictions from our model! Do these make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1670689521090,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "puFwhCMK2jrn",
    "outputId": "cfc29111-aab8-40af-b929-7974d79e198e"
   },
   "outputs": [],
   "source": [
    "test_dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 965,
     "status": "ok",
     "timestamp": 1670689461425,
     "user": {
      "displayName": "Cédric John",
      "userId": "08599675448238365153"
     },
     "user_tz": 0
    },
    "id": "ae_XuEDFYS8N",
    "outputId": "ee9e0db8-43a8-4728-ef56-c732370c6c55",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text_classifier.eval()\n",
    "with torch.no_grad():\n",
    "    print(torch.sigmoid(text_classifier(test_x[:3].to(device))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYeaNUKL24Om"
   },
   "source": [
    "# Optional Exercise 2\n",
    "\n",
    "Here are a few things you can do if you want:\n",
    "\n",
    "1. Try to calculate the accuracy for the test set\n",
    "2. The model is decent (about 80% accuracy). Can you improve on it?\n",
    "\n",
    "There are no given solutions for this exercise: it is up to you to play with the model if you want to.\n",
    "\n",
    "Hope you enjoyed your first try at NLP!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to play around..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
